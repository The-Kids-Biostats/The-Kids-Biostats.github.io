---
title: "Small-sample analysis"
format:
  html:
    code-fold: true
    toc: true
    toc-location: left
    other-links:
      - text: The Jackson Laboratory
        href: https://www.jax.org/jax-mice-and-services/strain-data-sheet-pages/body-weight-chart-000664   
date: ""    
author: "Bethy McKinnon"
draft: true
---

## 

```{r setup, include=F, message = F}
knitr::opts_chunk$set(echo = F, warning = F,include=F)

library(tidyverse)
#library(simstudy); library(data.table)
#library(lme4)
#library(jtools); library(gtsummary)
library(ggpubr)
library(cowplot)
library(pwr)
library(DescTools)
my_compact_theme <-
  list(
    # for gt in render
    "as_gt-lst:addl_cmds" = list(tab_spanner = rlang::expr(gt::tab_options(table.font.size = 14,
                                                                           data_row.padding = 4)))
  )

#set_gtsummary_theme(my_compact_theme)

# Theme elements sized here for display in a 2-column configuration
theme_clean <- function() {
  theme_minimal() + 
    theme(panel.grid.minor = element_blank(),
          plot.title = element_text(size=rel(2.4)),
          axis.title = element_text(size=rel(1.5)),
          axis.text.x=element_text(size=rel(1.5)),
          axis.text.y=element_text(size=rel(1.2)),
          strip.text = element_text(size = rel(1), hjust = 0),
          strip.background = element_rect(fill = "grey80", color = NA))
}
```

# What is small?

Sample size is often limited by practical constraints eg cost of equipment, logistics of animal breeding and management, prevalence of patients with a particular condition. While small can mean different things to different researchers, group sizes of 3-10 are typical of many animal studies and laboratory experiments.

# Sample statistics and sampling variation

To study a population of interest, data is gathered from a representative sample and characterised by summaries such as the sample median, mean, standard deviation or a proportion. These sample statistics vary about the population parameter for which they provide an estimate, and confidence intervals give a plausible range for this population value.

The smaller the sample size, the larger the distance between the statistic and the true value (ie the sampling error) is likely to be, and the wider the range of the confidence interval.

```{r}
# Default parameters have been set to planned mouse weight example
do.dist.plot<-function(dat,x="x",xlab="Weight (g)",title="",
                       bwidth=0.5,xintcpt=23,
                       xlims=c(15,31),xbreaks=seq(from=16,to=32,by=2))
{
  dat$x<-dat[,x]
 Plot<- ggplot(dat, aes(x = x)) + 
  geom_histogram(aes(y = after_stat(density)),center=xintcpt,binwidth=bwidth,
                 colour = 1, fill = "white") +
  geom_density(lwd = 1, colour = 4,
               fill = 4, alpha = 0.25)+
  scale_x_continuous(limits=xlims,breaks=xbreaks)+
  labs(x=xlab,title=title,y="Density")+
  geom_vline(aes(xintercept=xintcpt),colour="red")+
  theme_clean()
 return(Plot)
}

```

Consider the following example that demonstrates how sampling variation increases with decreasing sample sizes.

::: {layout-ncol="2"}
We will simulate data to represent a "population" of healthy female C57BL/6J mice who are expected to have a mean weight of 23 grams, with a standard deviation of 2 grams, at 14 weeks of age. (The Jackson Laboratory)

```{r}
set.seed(5)
fem.mice<-data.frame(x=rnorm(50000,mean=23,sd=2))
p.pop<-do.dist.plot(fem.mice,xlab="Weight (g)",title="Population distribution",bwidth=0.5)

```

```{r,include=T}
p.pop
```
:::

In practice researchers would only have data on a sample (subset) of a (usually hypothetical) population, from which to calculate summary statistics as estimates of general population characteristics. As may be expected, for larger datasets the calculated summary statistic is more likely to be closer to the value of the population from which the sample was drawn. Let's see how the calculated mean might be expected to vary from sample to sample for our mouse weight example.

We'll start by randomly 500 drawing samples that are each of size 50.

```{r}
get.sam<-function(sam.no,sam.size,popdat,x)
{
  out<-data.frame(i=sam.no,n=sam.size,x=sample(popdat[,x],size=sam.size))
  return(out)
}
get.mean<-function(dat)
{
  n<-nrow(dat)
  SE<-sd(dat$x)/sqrt(n)
  Mean<-mean(dat$x)
  lwr<-Mean-qt(0.975,df=n-1)*SE
  upr<-Mean+qt(0.975,df=n-1)*SE
  return(cbind(dat[1,c("i","n")],mean=Mean,se=SE,lwr=lwr,upr=upr))
}

set.seed(23)
samdat50<-do.call("rbind",lapply(1:500,get.sam,sam.size=50,popdat=fem.mice))
meandat50<-do.call("rbind",lapply(split(samdat50,samdat50$i),get.mean))

do.samplot<-function(dat,mdat,title="",xintcpt=23,add.ci=F)
{
  dat<-subset(dat,dat$i<6)
  mdat<-subset(mdat,mdat$i<6)
  mdat$x<-mdat[,"mean"]
  dat$y<-11-dat$i
  mdat$y<-11-mdat$i
Splot<-  ggplot(data=dat,aes(x=x,y=y+0.02,fill=factor(i)))+
  geom_vline(aes(xintercept=xintcpt),colour="red",linetype=2)+
  geom_point(alpha=0.5,size=rel(3.5),shape=25)+
  geom_point(data=mdat,aes(x=mean,y=y-0.3),size=rel(3.5),shape=17)+
  labs(x="Weight (g)",y="Sample",title=title)+
  scale_y_continuous(limits=c(4.5,10.2),breaks=6:10,minor_breaks=NULL,labels=rev(paste0("S-",1:5)))+
  scale_x_continuous(expand=c(0,0),breaks=seq(from=16,to=32,by=2),minor_breaks=NULL)+
  #scale_shape_manual(values=21:25)+
  annotate(geom="segment",x = 14.5,y = 4.5, xend = 14.5, yend = 5.6,
           col = "grey40", linewidth=1.5,linetype=3) +
  coord_cartesian(xlim = c(15,31),clip="off") +
  theme_clean()+theme(legend.position="none",
                      plot.title=element_text(size=rel(2)))
if(add.ci)
  Splot<-Splot+geom_errorbarh(data=mdat,aes(xmin=lwr,xmax=upr,y=y-0.3),height=0)
return(Splot)
}

do.meanplot<-function(mdat,title="Distribution of sample mean",bwidth=0.5,xintcpt=23)
{
 Mplot<- ggplot(mdat, aes(x = mean)) + 
  geom_histogram(aes(y = after_stat(density)),center=23,binwidth=bwidth,
                 colour = 1, fill = "white") +
  geom_density(lwd = 1, colour = 4,
               fill = 4, alpha = 0.25)+
  scale_x_continuous(expand=c(0,0),breaks=seq(from=16,to=32,by=2))+
  coord_cartesian(xlim = c(15,31),clip="off")+
  labs(x="Mean weight (g)",title=title,y="Density")+
  geom_vline(aes(xintercept=xintcpt),colour="red")+
 theme_clean()+theme(legend.position="none",
                     plot.title=element_text(size=rel(1.5)))
return(Mplot)
}

splot50<-do.samplot(dat=samdat50,mdat=meandat50,
                    title="Random draws of size n=50")
mplot50<-do.meanplot(mdat=meandat50,xintcpt=mean(meandat50$mean),bwidth=0.2)
```

::: {layout-ncol="2"}
Note that the mean of the sample means takes a value of `r format(round(mean(meandat50$mean),2),nsmall=2)`, the same value, to 2 decimal places, as that in the parent population, but the spread is much reduced (Fig 2b). In fact, the standard deviation of the sample means (ie standard error of the mean) is `r round(sd(meandat50$mean),2)`, in line with the expected reduction of the population standard deviation of 2 by a factor of $1/\sqrt{n}$.

```{r,include=T,fig.height=7}
#ggarrange(splot50,mplot50,ncol=1,heights=c(2,1.8))
plot_grid(splot50,mplot50,nrow=2,ncol=1,align="v",rel_heights=c(2,1.5))
```
:::

::: column-margin
**Figure 2**\
(a) The first 5 samples of 500 draws having a sample size of 50, from a simulated population with mean=23 and std dev=2.\
<br> (b) The distribution of the sample means calculated from each of the 500 draws.
:::

Now let's see what happens as the sample size decreases.

```{r}

set.seed(19)
set.seed(21)
samdat20<-do.call("rbind",lapply(1:500,get.sam,sam.size=20,popdat=fem.mice))
meandat20<-do.call("rbind",lapply(split(samdat20,samdat20$i),get.mean))


set.seed(19)
samdat5<-do.call("rbind",lapply(1:500,get.sam,sam.size=5,popdat=fem.mice))
meandat5<-do.call("rbind",lapply(split(samdat5,samdat5$i),get.mean))

splot20<-do.samplot(dat=samdat20,mdat=meandat20,
                    title="Random draws of size n=20")
mplot20<-do.meanplot(mdat=meandat20,xintcpt=mean(meandat20$mean),bwidth=0.4)
splot5<-do.samplot(dat=samdat5,mdat=meandat5,
                    title="Random draws of size n=5")
mplot5<-do.meanplot(mdat=meandat5,xintcpt=mean(meandat5$mean),bwidth=0.5)


```

::: {layout-ncol="2"}
```{r,include=T,fig.height=7}
#ggarrange(splot50,mplot50,ncol=1,heights=c(2,1.8))
plot_grid(splot20,mplot20,nrow=2,ncol=1,align="v",rel_heights=c(2,1.5))
```

```{r,include=T,fig.height=7}
#ggarrange(splot50,mplot50,ncol=1,heights=c(2,1.8))
plot_grid(splot5,mplot5,nrow=2,ncol=1,align="v",rel_heights=c(2,1.5))
```
:::

::: column-margin
**Figure 3**\
The first 5 samples of 500 draws having a sample size of 50, and those having a sample size of 5, together with the respective distributions of their sample means.
:::

::: {layout-ncol="2"}
It is clear that as the sample size decreases the estimates of the population mean become progressively more variable. This increased variability is taken into account in the formula for calculating the 95% confidence interval which become accordingly wider as precision decreases:

```{r,include=T,fig.height=9}
splot50.ci<-do.samplot(dat=samdat50,mdat=meandat50,add.ci=T,
                    title="Sample size: n=50")
splot20.ci<-do.samplot(dat=samdat20,mdat=meandat20,add.ci=T,
                    title="Sample size: n=20")
splot5.ci<-do.samplot(dat=samdat5,mdat=meandat5,add.ci=T,
                    title="Sample size: n=5")
ggarrange(splot50.ci,splot20.ci,splot5.ci,ncol=1)
```
:::

::: column-margin
**Figure 4**\
Random samples of increasing size, annotated with means and 95% confidence intervals.
:::

# Comparative studies with small group sizes

A major limitation of small-sample studies is that they lack power to detect anything other than large differences or large â€œeffects", as the sampling distribution of the test statistic needs to take account of increased variability of the sample statistics on which the test procedure is based.

For comparisons of mean values (eg a t-test) a large effect translates to a large difference between groups relative to the within-group variation.

```{r}
eff.size<-seq(from=0,to=3,by=0.2)
es.dat.80<-data.frame(n=c(5,10,15),
                      eff.size=c(pwr.t.test(n=5,power=0.8)$d,
                                 pwr.t.test(n=10,power=0.8)$d,
                                 pwr.t.test(n=15,power=0.8)$d),
                      power=0.8)
es.dat<-rbind(es.dat.80,
              data.frame(n=5,eff.size,power=pwr.t.test(n=5,d=eff.size)$power),
              data.frame(n=10,eff.size,power=pwr.t.test(n=10,d=eff.size)$power),
              data.frame(n=15,eff.size,power=pwr.t.test(n=15,d=eff.size)$power))
es.dat<-es.dat[order(es.dat$n,es.dat$power),]
#es.dat$N<-factor(es.dat$n,label=c(expression(n_1=n_2=5),expression(n_1=n_2=10),expression(n_1=n_2=15)))
#es.dat.80$N<-factor(es.dat.80$n,label=c(expression(n_1=n_2=5),expression(n_1=n_2=10),expression(n_1=n_2=15)))
es.dat.80$N<-factor(es.dat.80$n,labels=paste0("n=",c(5,10,15)))
es.dat$N<-factor(es.dat$n,labels=paste0("n=",c(5,10,15)))
power.plot<-ggplot(data=es.dat,aes(x=eff.size,y=power,colour=N))+
  geom_line(linewidth=1)+
  geom_point(data=es.dat.80,aes(colour=N),shape=19,size=rel(3))+
  geom_segment(data=es.dat.80,aes(yend=rep(0,3),colour=N),linetype=2)+
  geom_segment(data=es.dat.80,aes(xend=rep(0,3),colour=N),linetype=2)+
  labs(x="Effect size",y="Power to conclude a difference \nbetween the means of 2 groups",
       title="Power curves for t-test",legend="Sample size")+
  theme_clean()+theme(legend.position="right",
                      legend.text=element_text(size=rel(1.2)),
                      legend.title=element_text(size=rel(1.2)))

```

::: {layout-ncol="2"}
If n=5 for both groups, the minimum difference a 2-sample t-test has at least 80% power to detect is just over 2 standard deviations. In line with the example above, if we were comparing weights of 2 treatment groups of mice, and it's reasonable to assume a common SD of 2g, this would correspond to an absolute difference of 4g in the mean weights, quite a large treatment effect.

```{r,include=T}
power.plot
```
:::

::: column-margin
**Figure 4**\
Power plots for a 2-sample t-test, with alpha=0.05 and equal sample sizes, assuming equal variances.
:::

At the design stage of an experiment, determine the minimum clinically meaningful difference/effect size you want to be able to detect and power accordingly. Consider increasing power by extending the simple 2-sample approach to factorial designs where the between treatment group differences are simultaneously examined across one or more factors (eg sex, age group).

# Using confidence intervals to guide decision-making

Confidence intervals can be a little tricky to get your head around. They don't provide a range in which X% of the data values lie, but rather relate to our belief regarding where the population parameter of interest (eg true mean or proportion) lies. The "confidence" comes from sampling theory: Suppose we sample the same population many many times, and for each sample calculate the 95% confidence interval around the sample mean using the known formula, we would expect that 95% of the time the constructed interval will capture the true population mean. Sometimes, just by chance, the whole sample may lie towards the tail of the population distribution (see Sample 5 for n=5 in the example above) so that the 95% CI happens to "miss" the true value. We don't actually know if this has happened of course, since in practice we only have a single sample and don't know the true parameter value for the population from which the sample was drawn; hence the level of confidence simply reflects a rather nominal degree of plausibility.

Even though we know that for small sample sizes the calculated confidence intervals will be wide, particularly for proportions, the upper and/or lower limits can still offer useful bounds to aid with decision making.

```{r}
get.limits<-function(ntot,dd,k)
{
  n.resp<-0:ntot
  ucb<-data.frame(type="upr",n.resp=n.resp,n=ntot,prop=n.resp/ntot,
                     y=BinomCI(x=n.resp,n=ntot,conf.level=0.9,sides="right",method="clopper")[,3])
  lcb<-data.frame(type="lwr",n.resp=n.resp,n=ntot,prop=n.resp/ntot,
                     y=BinomCI(x=n.resp,n=ntot,conf.level=0.9,sides="left",method="clopper")[,2])
  lims<-rbind(ucb,lcb)
  lims$ymin<-ifelse(lims$type=="upr",0,lims$y)
  lims$ymax<-ifelse(lims$type=="upr",lims$y,1)
  lims$x<-ifelse(lims$type=="upr",lims$n.resp-dd,lims$n.resp+dd)
  lims$xmin<-ifelse(lims$type=="upr",lims$n.resp-k*dd,lims$x)
  lims$xmax<-ifelse(lims$type=="upr",lims$x,lims$n.resp+k*dd)
  return(lims)
}
plot.limits<-function(ntot,DLT=0.4,Resp=0.5)
{
lim.vals<-get.limits(ntot=ntot,0.1,3)
lim.vals$Type<-factor(lim.vals$type,labels=c("80% LCB for response","90% UCB for toxicity"))
my.col<-c("#C4961A","#00AFBB")
lims.plot<-ggplot(data=lim.vals,aes(x=x,y=y,colour=Type))+
  geom_linerange(aes(ymin=ymin,ymax=ymax,x=x),linewidth=1)+
  geom_linerange(aes(xmin=xmin,xmax=xmax,y=y),linewidth=1)+
  scale_x_continuous(expand=c(0.2,0.2),breaks=0:ntot,name="Number of DLTs",
                     sec.axis=dup_axis(name="Number of responses"))+
  scale_y_continuous(expand=c(0,0),name="Probability of toxicity",
                     breaks=seq(from=0,to=1,by=0.2),minor_breaks=NULL,
                     sec.axis = dup_axis(
    name = "Probability of response",
    breaks=seq(from=0,to=1,by=0.2))) +
scale_color_manual(values=my.col,name="") +
    coord_cartesian(xlim = c(0,ntot),clip="off") +
  geom_hline(yintercept=DLT,color=my.col[2],linetype=2)+  annotate(geom="text",x=-1,y=DLT,label="DLT \nthreshold", color=my.col[2], size=rel(3.5),fontface=3)+
    geom_hline(yintercept=Resp,color=my.col[1],linetype=3)+
annotate(geom="text",x=11,y=Resp,label="Efficacy \nthreshold", color=my.col[1],  size=rel(4),fontface=3)+ 
  labs(title=paste("Planned cohort size of ",ntot))+

  theme_clean()+theme(legend.position="bottom",
                      legend.text=element_text(size=rel(1)),
    axis.title.y = element_text(margin=margin(r=15,l=25)))
##,size=rel(1.5)), plot.title=element_text(size=rel(2))
  return(lims.plot)
}
lims.15<-plot.limits(ntot=15)
lims.10<-plot.limits(ntot=10)

```

For example, patient recruitment to an early-phase clinical trial of a new drug under development may cease early if there is sufficient evidence of high toxicity or provisional efficacy according to pre-defined criteria. Stopping rules based on one-sided binomial confidence intervals can inform trial planning and implementation.

::: {layout-ncol="2"}
Say enrolment into a dose expansion cohort is planned, with dose limiting toxicity set at a rate of 40% and demonstration of provisional efficacy if treatment response rates are at least 50%. Assuming the planned cohort size is 10, Figure 4 presents 80% lower confidence bounds (LCBs) as a reference for response counts, and more conservative 90% upper confidence bounds (UCBs) for toxicity counts .

```{r,include=T}
lims.10

```
:::

```{r,include=T,fig.height=3,fig.width=7,eval=F}
#plot_grid(lims.10,lims.15,nrow=2,align="h")
lims.10
```

::: column-margin
**Figure 4** 90% upper confidence bounds (UCBs, teal) for dose-limiting toxicity (DLT) and 80% lower confidence bounds (LCBs, ochre) for treatment response, for a sample size of 10. Bounds were constructed from one-sided binomial confidence intervals, using the Clopper-Pearson method.
:::

The drug would be deemed to have an acceptable toxicity profile if no more than 1 subject was impacted by a dose-limiting toxicity (DLT). Given the small sample size, if there are 2 or more impacted subjects then the confidence interval for the true toxicity rate will be in excess of the 40% threshold, even though the observed rate may be no more than 20%. The stopping rule for unacceptable toxicity would be invoked if a second DLT occurs prior to the final enrolment. Similarly, if treatment elicits a defined response in at least 8 patients, there is reasonable confidence that the treatment has demonstrated sufficient efficacy to warrant continued investigation, and should this be established prior to enrolment of the complete cohort the trial may be stopped early.

Confidence intervals for proportions are notoriously wide, and typically large sample sizes are sought to obtain a required level of precision for the rate that is being estimated. If available resources are limited, it may be that a more efficient experimental could be considered.

# Further thoughts

For small-sample studies, as with those based on larger samples, consider if the results can be appropriately generalised to fit with the aim of the study.

**Population sampling:** Consider if there are subtle biases resulting from recruitment strategies that may impact how representative the sample is of the population being characterized.

**Laboratory experiments or animal studies:** Ensure that environmental factors have been adequately controlled and appropriate randomization undertaken so that differences between groups can be confidently attributed to the condition/treatment under study. Small-sample experiments require tight control of between-individual variation to achieve maximum power and appropriate randomization must be undertaken for results to be valid.

## Reproducibility Information

To access the .qmd (Quarto markdown) files as well as any R scripts or data that was used in this post, [please visit our GitHub](https://github.com/The-Kids-Biostats/The-Kids-Biostats.github.io/tree/main/posts/pre_post):

The session information can also be seen below.

```{r}
sessionInfo()
```
