{
  "hash": "5b17f43cd3da47ac2e1bcd64c6e287f3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Parallel Computing in R\"\nauthor: \"Zac Dempsey\"\nformat:\n  html:\n    code-fold: true\n    toc: true\n    toc-location: left\ndate: \"2024-06-18\"\ncategories:\n  - R\n  - Parallel\ndraft: true\n---\n\n\n\n::: {.cell}\n\n:::\n\n\n# Overview\n\nWhile most of the datasets we work with are of a manageable size here at Telethon Kids, occasionally we are presented with data containing *millions* of rows and/or *thousands* of columns. Performing almost any operation on these large datasets can take hours to execute -- a real test of patience for exploratory work!\n\nParallel computing is one relatively simple way to reduce computation time on large datasets – leveraging our computer's processing capabilities by simultaneously distributing tasks \"in parallel\" across multiple processors (without too much extra code). This is particularly useful for loop-based computations/simulations, where multiple parameters/conditions must be swept over whether that as part of variable creation/curation, model execution/tuning, or some other task.\n\nThat being said, while parallel computing is not a \"magical general solution\" to all computationally intensive tasks – it is worth investing some time to understand how it works, and become familiar with the packages and/or functions used within a parallel framework. There are still circumstances where the always handy `purrr::map` function, or basic for-loops can be equally or more efficient than initialising parallel computing.\n\n\n::: {.cell}\n\n:::\n\n\n<p>\n\n# What is Parallel Computing?\n\nParallel computing refers to executing multiple operations in \"parallel\" (i.e. at the same time) across a machine's multiple cores/processors. Most modern computers have at least four cores, and often many more. By default, R uses only one core, running iterative operations sequentially; not starting the next until the previous is complete. However, under a parallel framework, multiple iterations/calculations can be allocated across multiple cores so they can be executed at the same time, thus saving time. These time savings accumulate particuarly with large underlying tasks (as opposed to running many small tasks in parallel — something that often ends up slower!).\n\nIn this post, we will have a look at a couple of examples using `R`. We provide some simple examples and use-cases, in addition to some further resources if you would like to learn more.\n\nThe [`doParallel`](#0) and [`foreach`](#0) packages of `R` are one set of packages, intended to be used together, to facilitate a syntactically simple parallel computing environment in `R` – by use of the `%dopar%` operator. Additionally the base `parallel` package is used for some basic set-up and detection.\n\nThese examples are run using R version 4.3.2 (2023-10-31 ucrt) on Windows 10 x64.\n\n# When should we consider Parallel Computing?\n\nIt is potentially easier to think about when we should *not* consider parallel computing. For parallel computing to be workable, the overall job needs to be something that can be broken down into [independent]{.underline} tasks, that is, into tasks that can each be completed with [no dependence on the input or output from another task]{.underline}. So, any job where the tasks must completed in sequential order can not be executed (or sped up) with parallel computing— this might be sequential (time dependent) processing of data (e.g., need to standardise variables *before* extreme values are identified and tagged) or computation (matrix algebra within a regression model).\n\nA quick analogy: you can not complete the job of making a pizza for a customer in a parallel way *\\[you can not cook the pizza at the same time as you're preparing the dough at the same time as you're putting the toppings on the pizza\\]* but you can complete the job of 'preparing all the pizza dough for tonight' in parallel *\\[by getting 10 people with 10 bowls and 10 bags of flour to all mix at the same time\\]*.\n\nAs it relates to data analysis, parallel computing requires us to divvy up tasks in a standalone way, and (generally) assemble the output from those tasks in a sensible way. An example:\n\n-   Six months of minutely data for 100 patients, where you need to calculate a range of summary statistics (including from some advanced modeling) for each patient. The data can be subset into '100' chunks as the summary statistics for each 'chunk' do not require access to (or knowledge of) the raw data or output (summary statistics) from any other chunk. The resulting summary statistics can then be compiled into a table or list for later use.\n\nOf course, there is some overhead to the allocating out of tasks to different cores and the compiling of results which one may want to consider. It is generally worthwhile to pilot your code on a subset of tasks to understand the potential overhead relative to the potential decrease in overall computation time that may be achieved.\n\n<br>\n\n# Setup\n\n-   Let's check the number of cores available on our machine.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparallel::detectCores()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 8\n```\n\n\n:::\n:::\n\n\n<p>\n\n-   We should not allocate all of our machine's available cores to parallel computations – this will consume all of the available resources!\n-   Let's use 6 cores for our examples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::registerDoParallel(6)\n```\n:::\n\n\n-   Below is a screenshot from the Windows task manager. R \"front-end\" instances are initialised for each of the 6 initialised cores.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](parallel_comps.png){fig-align='center' width=700px}\n:::\n:::\n\n\n-   When we no longer require a parallel environment, we must shut down and de-register the parallel cluster – otherwise they will remain active.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::stopImplicitCluster()\n```\n:::\n\n\n-   Let's also set a random seed for sample reproducibility\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2024)\n```\n:::\n\n\n<br>\n\n# Example 1: Bootstrapping\n\nBootstrapping is a relatively simple method that uses random sampling with replacement to mimic random sampling of a population. This allows us to estimate the sampling distribution for a statistic (mean, variance, etc.) and assess its stability/reliability, without making strong assumptions about the underlying population. This can be useful when the data are problematic in some fundamental way (small sample sizes, insufficient variation, violation of model assumptions).\n\nIn `R`, for-loops can be used to derive a set of bootstrap samples. As we will see, parallel environments can much more efficiently handle repeated independent operations like these.\n\n-   Let's use the `mtcars` data set to run a basic linear regression.\n    -   We would like to explore the association between a vehicle's fuel efficiency (`mpg`) and horsepower (`hp`), weight (`wt`) and transmission type (automatic/manual, `am`).\n-   We suspect some of the assumptions of standard linear regression may be violated (normality of errors) because of the small sample size. By bootstrapping, we can explore the stability of the regression coefficients by calculating a confidence interval about the bootstrapped coefficient estimates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Let's set up the data and number of bootstrap samples\ndata(mtcars)\nmtcars <- as_tibble(mtcars)\n\nhead(mtcars)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 11\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1  21       6   160   110  3.9   2.62  16.5     0     1     4     4\n2  21       6   160   110  3.9   2.88  17.0     0     1     4     4\n3  22.8     4   108    93  3.85  2.32  18.6     1     1     4     1\n4  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1\n5  18.7     8   360   175  3.15  3.44  17.0     0     0     3     2\n6  18.1     6   225   105  2.76  3.46  20.2     1     0     3     1\n```\n\n\n:::\n:::\n\n\n<p>\n\nWe will present 3 ways to bootstrap data – two using for-loop structures, and one using a parallel computing environment. In each instance, we will compare how the output is structured and results returned.\n\n<p>\n\nNow, let's set the number of bootstrap trials.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrials <- 10000 # Number of bootstrap samples\n```\n:::\n\n\n<p>\n\n## For-loop\n\n-   Let's begin by using the \"tried and true\" for-loop. For each of the 10^{4} bootstrap samples, we:\n    -   Initialise a bootstrap sample, `ind`.\n    -   Run our linear regression on the bootstrap sample, `results`\n    -   Extract the coefficients from `results`, and append this to an overall coefficient matrix `bootstrap_coefs`.\n-   Subsequently, we can calculate a 95% confidence interval for each of our parameter estimates by taking the 2.5th and 97.5th percentiles from the bootstrap distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart <- proc.time() # Start our timer!\n\n# Initialise a matrix to store the coefficients from each bootstrap sample\nbootstrap_coefs <- matrix(NA, nrow = trials, ncol = 4)\ncolnames(bootstrap_coefs) <- names(coef(lm(mpg ~ hp + wt + am, data = mtcars)))\n\nfor (i in 1:trials){\n  ind <- mtcars[sample(nrow(mtcars), replace = TRUE),]\n  result <- lm(mpg ~ hp + wt + as_factor(am), data = ind)\n  \n  bootstrap_coefs[i, ] <- coef(result)\n}\n\n# Calculate the 2.5th and 97.5th percentile for each variable's coefficient estimates from the bootstrap distribution.\nbootstrap_cis <- apply(bootstrap_coefs, \n                       2, \n                       function(coefs){quantile(coefs, probs = c(0.025, 0.975))})\n\nend <- proc.time() # End our timer!\ntime1 <- end-start\nprint(time1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n   9.53    0.19    9.97 \n```\n\n\n:::\n:::\n\n\n<p>\n\nLet's visualise the first few lines of the bootstrapped results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(bootstrap_coefs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     (Intercept)          hp        wt         am\n[1,]    34.04596 -0.02741585 -3.158154  0.6330600\n[2,]    31.46670 -0.03097903 -2.375727  5.8142235\n[3,]    35.98084 -0.02535775 -3.763464 -0.2485866\n[4,]    33.47330 -0.04410244 -2.343210  2.6890689\n[5,]    32.21798 -0.04138935 -2.222471  1.2289610\n[6,]    32.78747 -0.02758182 -2.989311  1.1744731\n```\n\n\n:::\n:::\n\n\nand associated 95% confidence interval\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbootstrap_cis\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      (Intercept)          hp        wt         am\n2.5%     29.07843 -0.05539954 -5.134682 -0.7627477\n97.5%    40.74463 -0.02161861 -1.057830  4.9428501\n```\n\n\n:::\n:::\n\n\n<p>\n\n## `%do%` loop (not parallel)\n\n-   As an alternative, let's also use the `%do%` operator from the `foreach` package. Similar to a for-loop, the loop is sequentially executed.\n-   Similar in execution time to the for-loop operation..\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart <- proc.time()\n\nbootstrap_coefs <- foreach::foreach(i = 1:trials, .combine = rbind) %do% {\n  ind <- mtcars[sample(nrow(mtcars), replace = TRUE), ]\n  result <- lm(mpg ~ hp + wt + am, data = ind)\n  coef(result)\n}\n\n# Calculate the 2.5th and 97.5th percentile for each variable's coefficient estimates from the bootstrap distribution.\nbootstrap_cis <- apply(bootstrap_coefs, \n                       2, \n                       function(coefs) {quantile(coefs, probs = c(0.025, 0.975))})\n\nend <- proc.time()\ntime2 <- end-start\nprint(time2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n   8.48    0.11    8.76 \n```\n\n\n:::\n:::\n\n\n<p>\n\nSimilarly, let's visualise the first few lines of the bootstrapped results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(bootstrap_coefs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         (Intercept)          hp        wt       am\nresult.1    32.28476 -0.02807558 -2.989010 2.404865\nresult.2    38.19523 -0.02740136 -4.576854 1.082726\nresult.3    32.87519 -0.06693216 -1.172027 3.879952\nresult.4    29.54068 -0.03806135 -1.468158 1.933494\nresult.5    31.54392 -0.02793433 -2.756060 1.347816\nresult.6    34.42623 -0.03900395 -2.865725 1.167583\n```\n\n\n:::\n:::\n\n\nand associated 95% confidence interval\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbootstrap_cis\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      (Intercept)          hp        wt         am\n2.5%     29.03455 -0.05615444 -5.286909 -0.8228096\n97.5%    41.09610 -0.02124169 -1.055386  4.9293286\n```\n\n\n:::\n:::\n\n\n<p>\n\n## `%dopar%` parallelisation\n\n-   Now, let's run this in parallel across 6 cores.\n-   The execution time is significantly reduced relative to the above two cases.\n-   The `%dopar%` operator defines the for-loop in the parallel environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::registerDoParallel(cores = 6)\n\nstart <- proc.time()\nbootstrap_coefs <- foreach(i = 1:trials, .combine = rbind, .packages = 'stats') %dopar% {\n  ind <- mtcars[sample(nrow(mtcars), replace = TRUE), ]\n  result <- lm(mpg ~ hp + wt + am, data = ind)\n  coef(result)\n}\n\n# Calculate the 2.5th and 97.5th percentile for each variable's coefficient estimates from the bootstrap distribution.\nbootstrap_cis <- apply(bootstrap_coefs, \n                       2, \n                       function(coefs) {quantile(coefs, probs = c(0.025, 0.975))})\n\nend <- proc.time()\ntime3 <- end-start\nprint(time3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n   2.39    0.56    4.36 \n```\n\n\n:::\n\n```{.r .cell-code}\ndoParallel::stopImplicitCluster()\n```\n:::\n\n\nSimilarly, let's visualise the first few lines of the bootstrapped results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(bootstrap_coefs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         (Intercept)          hp         wt         am\nresult.1    29.77147 -0.06252019 -0.6651191  5.4466403\nresult.2    34.40579 -0.02313541 -3.6991414  0.7112667\nresult.3    33.37915 -0.05173181 -2.2064401  2.7886492\nresult.4    42.00942 -0.02220918 -5.8175000 -0.8642204\nresult.5    37.03595 -0.03170598 -4.1306537  1.6932277\nresult.6    34.76352 -0.03045483 -3.6501972  1.8373947\n```\n\n\n:::\n:::\n\n\nand associated 95% confidence interval\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbootstrap_cis\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      (Intercept)          hp        wt         am\n2.5%     28.99181 -0.05547634 -5.179310 -0.8329246\n97.5%    40.77477 -0.02170341 -1.047131  4.9953484\n```\n\n\n:::\n:::\n\n\n<p>\n\n## Discussion\n\nWe have presented three ways to generate bootstrapped confidence intervals for coefficient estimates of a multiple linear regression.\n\n1\\) Traditional for-loop\\\n2) Non-parallelised loop using the `foreach` package\\\n3) Parallelised computation\n\nImmediately, the syntax of the alternative for-loop structures are no less readable or more difficult to construct than the traditional for-loop. The output is structured similarly and combines easily into a list using the `foreach::foreach`.\n\nComputation time, however, is sped up significantly – particularly in the parallel environment. In fact, the parallel instance is approximately 56% faster than the traditional for-loop. Across multiple analyses and data sets, these time savings certainly add up!\n\n<br>\n\n# Example 2: Random forest model\n\n-   Let's say we would like a model that predicts the distribution of a plant species based on various environmental factors (temperature, precipitation, elevation, vegetation type).\n\n-   We would like to use a random forest (using the `randomForest` package of `R`) model for this – a popular model for classification based on combining the output of multiple decision trees.\n\n-   This example is inspired by the example from the [vignette of the `foreach` package](https://cran.r-project.org/web/packages/foreach/vignettes/foreach.html).\n\n## Data\n\n-   First, let's simulate our data set and set some parameters\n\n-   Data specification\n\n    -   100,000 observations.\n\n    -   Independent variables temperature, precipitation and elevation sampled from a random normal distribution and vegetation type (categorical factor) randomly prescribed.\n\n    -   Categorical factor variable randomly prescribed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 10000 # Sample size\n\ndata <- data.frame(temperature = rnorm(n, \n                                       mean = 15, \n                                       sd   = 40),\n                   precipitation = rnorm(n, \n                                         mean = 1000, \n                                         sd   = 300),\n                   elevation = rnorm(n, \n                                     mean = 500, \n                                     sd   = 200),\n                   vegetation_type = as_factor(sample(c(\"forest\", \n                                                        \"grassland\", \n                                                        \"wetland\", \n                                                        \"desert\"), \n                                                      n, \n                                                      replace = T)),\n                   species_presence = as_factor(sample(c(\"present\", \n                                                         \"absent\"), \n                                                       n, \n                                                       replace = T)))\n\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  temperature precipitation elevation vegetation_type species_presence\n1   12.994690      909.1494  706.1019         wetland           absent\n2    8.897814      870.6431  342.1537       grassland          present\n3  -11.155267      681.5987  353.4205          forest          present\n4   53.423857      791.8671  657.7516         wetland           absent\n5   82.254464      610.9436  386.1190          forest          present\n6   35.041577      876.3924  409.8187          desert          present\n```\n\n\n:::\n:::\n\n\n-   Now, let's assign 70% of the data to our training set, and the remaining 30% to our test data set.\n\n-   Initialise a random forest model with 1000 trees.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_index <- sample(1:n, 0.8*n)\ntrain_data <- data[train_index, ]\ntest_data <- data[-train_index, ]\nrm(train_index)\n\nnum_trees <- 1000\n```\n:::\n\n\n<p>\n\n## For-loop\n\n-   Let's split up the computation of a 1000 tree random forest model by sequentially 4 random forest models comprising 250 trees.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart <- proc.time()\nrf <- list()\nfor (i in 1:(num_trees/250)){\n  sampled_data <- train_data[sample(nrow(train_data), replace = TRUE),]\n\n  rf[[i]] <- randomForest::randomForest(species_presence ~ ., \n                                        data = sampled_data, \n                                        ntree = num_trees/4)\n}\n\ncombined_output <- do.call(randomForest::combine, rf)\n\npredictions <- predict(combined_output, test_data %>% select(-species_presence))\n\nend <- proc.time()\ntime1 <- end - start\nprint(time1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n   6.76    0.29    7.18 \n```\n\n\n:::\n:::\n\n\nLet's print the confusion matrix of this output. Because this data was relatively simply simulated, we don't expect the predictive power to be too great.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(predictions, test_data$species_presence)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           \npredictions absent present\n    absent     487     496\n    present    515     502\n```\n\n\n:::\n:::\n\n\n## %do% loop (not parallel)\n\n-   Similar to the traditional for-loop, we can sequentially execute this code using the `%do%` operator.\n\n-   As expected, the run-time is somewhat similar to before.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart <- proc.time()\nrf <- foreach::foreach(ntree = rep(num_trees/4, 4), \n                       .combine = randomForest::combine, \n                       .packages = 'randomForest') %do%\n  randomForest::randomForest(species_presence ~ ., \n                             data = train_data,\n                             ntree = ntree)\n\npredictions <- predict(rf, test_data %>% select(-species_presence))\n\nend <- proc.time()\ntime2 <- end - start\nprint(time2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n   7.11    0.62    7.83 \n```\n\n\n:::\n:::\n\n\nLet's print the confusion matrix of this output. Because this data was relatively simply simulated, we don't expect the predictive power to be too great.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(predictions, test_data$species_presence)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           \npredictions absent present\n    absent     482     498\n    present    520     500\n```\n\n\n:::\n:::\n\n\n<p>\n\n## %dopar% parallelisation\n\n-   Let's now run this model in parallel.\n\n-   For simplicity, let's allocate 4 cores to the computation so one core processes each of the 4 random forest models comprising 250 trees simultaneously.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::registerDoParallel(cores = 4)\n\nstart <- proc.time()\nrf <- foreach::foreach(ntree = rep(num_trees/4, 4), \n                       .combine = randomForest::combine, \n                       .packages = 'randomForest') %dopar%\n  randomForest::randomForest(species_presence ~ ., \n                             data = train_data,\n                             ntree = ntree)\n\npredictions <- predict(rf, test_data %>% select(-species_presence))\n\nend <- proc.time()\ntime3 <- end-start\nprint(time3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n   1.52    0.42    4.60 \n```\n\n\n:::\n\n```{.r .cell-code}\ndoParallel::stopImplicitCluster()\n```\n:::\n\n\nLet's print the confusion matrix of this output. Because this data was relatively simply simulated, we don't expect the predictive power to be too great.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(predictions, test_data$species_presence)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           \npredictions absent present\n    absent     475     481\n    present    527     517\n```\n\n\n:::\n:::\n\n\n<p>\n\n## Discussion\n\nUsing [easily adaptable and readable syntax]{.underline}, again we can leverage a parallel environment to significantly hasten the computation time of our large model. Relative to a standard for-loop, the parallelised computation is approximately 36% faster.\n\n<br>\n\n# Example 3: Parallel regressions with different outcomes\n\nAs hinted in the overview to this blog, parallel computing is not the general solution to running loop-based code more quickly. Instances which are intrinsically \"fast\" to run will not benefit from a parallel environment and may in fact run *slower* in parallel, after accounting for the overhead in setting up the parallel processing.\\\n\\\nThere are other packages which may be useful in these circumstances, such as the always handy `purrr:map` or `furrr::future_map`.\n\n## Data\n\n-   Now let's apply this to a different situation – where we have different model specifications we would like to run on the same \"large\" data set.\n-   Let's use `nycflights13::flights` dataset – a set of over 300,000 flight records that departed from all NYC airports in 2013.\n-   We would like to explore how arrival delay (as a continuous and dichotomous (delayed = 1, not delayed = 0) outcome variable) may be influenced by a set of independent variables. We would like to stratify this by month.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights <- nycflights13::flights\nflights <- flights %>%\n  select(year, month, day, dep_delay, arr_delay, air_time, distance) %>%\n  mutate(arr_delay_bin = as.factor(case_when(arr_delay >  15 ~ 1, TRUE ~ 0)),\n         month = month.name[month])\n\nflights\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 336,776 × 8\n    year month     day dep_delay arr_delay air_time distance arr_delay_bin\n   <int> <chr>   <int>     <dbl>     <dbl>    <dbl>    <dbl> <fct>        \n 1  2013 January     1         2        11      227     1400 0            \n 2  2013 January     1         4        20      227     1416 1            \n 3  2013 January     1         2        33      160     1089 1            \n 4  2013 January     1        -1       -18      183     1576 0            \n 5  2013 January     1        -6       -25      116      762 0            \n 6  2013 January     1        -4        12      150      719 0            \n 7  2013 January     1        -5        19      158     1065 1            \n 8  2013 January     1        -3       -14       53      229 0            \n 9  2013 January     1        -3        -8      140      944 0            \n10  2013 January     1        -2         8      138      733 0            \n# ℹ 336,766 more rows\n```\n\n\n:::\n:::\n\n\n<p>\n\nWe would like to specify a set of models to explore flight delay\n\n-   Outcome variables\n\n    -   Arrival delay (continuous)\n\n    -   Arrival delayed (dichotomous)\n\n-   Independent variables\n\n    -   Flight distance (`distance`)\n\n    -   Air time (`air_time`)\n\n    -   Departure delay (`dep_delay`)\n\n-   Stratification variable\n\n    -   Month (`month`)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nindep_vars <- c(\"distance\", \"air_time\", \"dep_delay\")\noutcome_vars <- c(\"arr_delay\", \"arr_delay_bin\")\n```\n:::\n\n\n<p>\n\n## For-loop\n\n-   For each \"month\", we extract the subset of data\n\n-   For each outcome variable, we run a model. If the outcome variable is continuous, we run a simple linear model; otherwise we run a basic logistic regression.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart <- proc.time()\nmodels <- list()\nfor (i in unique(flights$month)){\n  month_dat <- flights %>% filter(month == i)\n  sub_models <- list()\n  for (j in outcome_vars){\n    if (j == \"arr_delay\"){\n      model <- lm(as.formula(paste(j,\"~\",paste(indep_vars, collapse = \"+\"))),\n                  data = month_dat)\n      \n    } else if (j == \"arr_delay_bin\"){\n      model <- glm(as.formula(paste(j,\"~\",paste(indep_vars, collapse = \"+\"))),\n                   family = binomial,\n                   data = month_dat)\n    }\n    sub_models[[j]] <- summary(model)\n  }\n  models[[i]] <- sub_models\n}\nnames(models) <- unique(flights$month)\n\nend <- proc.time()\ntime1 <- end-start\nprint(time1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n   1.33    0.34    1.71 \n```\n\n\n:::\n:::\n\n\n<p>\n\n<p>\n\n## `purrr:map`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart <- proc.time()\nmodels <- map(unique(flights$month), ~{\n  month_dat <- flights %>% filter(month == .x)\n  \n  models <- map(outcome_vars, \n      ~ if (.x == \"arr_delay\") {\n        lm(as.formula(paste(\"arr_delay ~ \", \n                            paste(indep_vars, collapse = \" + \"))),\n           data = month_dat) %>%\n          summary()\n        \n        } else if (.x == \"arr_delay_bin\"){\n          glm(as.formula(paste(\"arr_delay_bin ~\",\n                               paste(indep_vars, collapse = \" + \"))),\n              family = binomial,\n              data = month_dat) %>%\n            summary()\n        })\n  names(models) <- outcome_vars\n  models\n})\n\nnames(models) <- unique(flights$month)\n\nend <- proc.time()\ntime1 <- end-start\nprint(time1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n   1.36    0.30    1.67 \n```\n\n\n:::\n:::\n\n\n<p>\n\n## `furrr::future_map`\n\n-   <span style = \"color:red\">Preamble about what this function actually does!</span>\n\nThe syntax is identical to `purrr::map`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(furrr)\n\nplan(multisession) # Initialise parallel environment using furrr\n\nstart <- proc.time()\nmodels <- furrr::future_map(unique(flights$month), ~{\n  month_dat <- flights %>% filter(month == .x)\n  \n  models <- furrr::future_map(outcome_vars, \n      ~ if (.x == \"arr_delay\") {\n        lm(as.formula(paste(\"arr_delay ~ \", \n                            paste(indep_vars, collapse = \" + \"))),\n           data = month_dat) %>%\n          summary()\n        \n        } else if (.x == \"arr_delay_bin\"){\n          glm(as.formula(paste(\"arr_delay_bin ~\", \n                               paste(indep_vars, collapse = \" + \"))),\n              family = binomial,\n              data = month_dat) %>%\n            summary()\n        })\n  names(models) <- outcome_vars\n  models\n})\nnames(models) <- unique(flights$month)\n\nend <- proc.time()\ntime1 <- end-start\n\nplan(sequential) # Revert to sequential processing\nprint(time1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n   0.91    0.27    7.09 \n```\n\n\n:::\n:::\n\n\n## `%dopar%` parallelisation\n\n-   We use the `%:%` operator from the `foreach` package to nest a for-loop within a parallel environment.\n\n-   The syntax does not differ too dramatically.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#doParallel::registerDoParallel(cores = 6)\n#\n#start <- proc.time()\n#results <- foreach(i = unique(flights$month), \n#                   .packages = \"stats\", \n#                   .combine = \"list\") %dopar% {\n#  month_results <- foreach(j = outcome_vars, .combine = \"list\", .multicombine = #TRUE) %dopar% {\n#    month <- i\n#    month_dat <- subset(flights, month = month)\n#    \n#    if (j == \"arr_delay\"){\n#      model <- lm(as.formula(paste(j,\"~\",paste(indep_vars, collapse = \"+\"))),\n#                  data = month_dat)\n#      list(j = summary(model))\n#    } else if (j == \"arr_delay_bin\"){\n#      model <- glm(as.formula(paste(j,\"~\",paste(indep_vars, collapse = \"+\"))),\n#                   family = binomial,\n#                   data = month_dat)\n#      list(j = summary(model))\n#    }\n#  }\n#  names(month_results) <- outcome_vars\n#  list(i = month_results)\n#  }\n#\n#end <- proc.time()\n#time2 <- end - start\n#print(time2)\n#\n#doParallel::stopImplicitCluster()\n```\n:::\n\n\nWe now see that the parallel environment is Again, this simple example represents a -10% time saving relative to a standard for-loop implementation.\n\n<br>\n\n# Conclusion\n\nUsing just a few relatively simple examples, we have demonstrated that a parallel computing environment, can be a relatively easy and quick way to improve the runtime of iteration/loop-based operations - with little modification to your code!\n\nWhile not appropriate in all circumstances (depending on the exact use-case), it is worth being across the relevant packages and commands used in `R` for parallel computing, particularly as datasets grow increasingly large and analytical methodologies increasingly complex.\n\n<br>\n\n## Acknowledgements\n\nThanks to Wesley Billingham, Matt Cooper and Elizabeth McKinnon for providing feedback on and reviewing this post.\n\nYou can look forward to seeing posts from these other team members here in the coming weeks and months.\n\n<br>\n\n# Further Resources\n\n-   Vignettes\n    -   <https://cran.r-project.org/web/packages/doParallel/vignettes/gettingstartedParallel.pdf>\n-   Tutorials\n    -   <https://unc-libraries-data.github.io/R-Open-Labs/Extras/Parallel/foreach.html>\n\n<br>\n\n## Reproducibility Information\n\nTo access the .qmd (Quarto markdown) files as well as any R scripts or data that was used in this post, please visit our GitHub:\n\n<https://github.com/The-Kids-Biostats/The-Kids-Biostats.github.io/tree/main/posts/parallel>\n\nThe session information can also be seen below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.3.2 (2023-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_Australia.utf8  LC_CTYPE=English_Australia.utf8   \n[3] LC_MONETARY=English_Australia.utf8 LC_NUMERIC=C                      \n[5] LC_TIME=English_Australia.utf8    \n\ntime zone: Australia/Perth\ntzcode source: internal\n\nattached base packages:\n[1] parallel  stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] furrr_0.3.1          future_1.33.2        randomForest_4.7-1.1\n [4] doParallel_1.0.17    iterators_1.0.14     foreach_1.5.2       \n [7] lubridate_1.9.3      forcats_1.0.0        stringr_1.5.1       \n[10] dplyr_1.1.4          purrr_1.0.2          readr_2.1.5         \n[13] tidyr_1.3.1          tibble_3.2.1         ggplot2_3.5.1       \n[16] tidyverse_2.0.0     \n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.4         generics_0.1.3     stringi_1.8.3      listenv_0.9.1     \n [5] hms_1.1.3          digest_0.6.35      magrittr_2.0.3     evaluate_0.23     \n [9] grid_4.3.2         timechange_0.3.0   fastmap_1.1.1      jsonlite_1.8.8    \n[13] fansi_1.0.6        scales_1.3.0       codetools_0.2-20   cli_3.6.2         \n[17] rlang_1.1.3        parallelly_1.37.1  munsell_0.5.1      withr_3.0.0       \n[21] yaml_2.3.8         tools_4.3.2        tzdb_0.4.0         colorspace_2.1-0  \n[25] globals_0.16.3     vctrs_0.6.5        R6_2.5.1           lifecycle_1.0.4   \n[29] htmlwidgets_1.6.4  pkgconfig_2.0.3    pillar_1.9.0       gtable_0.3.5      \n[33] glue_1.7.0         xfun_0.43          tidyselect_1.2.1   rstudioapi_0.16.0 \n[37] knitr_1.46         htmltools_0.5.8.1  rmarkdown_2.26     nycflights13_1.0.2\n[41] compiler_4.3.2    \n```\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in rm(trials, results, i, j, model, outcome_vars, input_vars, start, :\nobject 'results' not found\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in rm(trials, results, i, j, model, outcome_vars, input_vars, start, :\nobject 'input_vars' not found\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in rm(trials, results, i, j, model, outcome_vars, input_vars, start, :\nobject 'bootstrap_results' not found\n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}