{
  "hash": "5400edecd6d3e239c51bba67f78f0b58",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Parallel Computing in R\"\noutput: html_document\nauthor: \"Zac Dempsey\"\ndate: \"2024-06-14\"\ncategories:\n  - R\n  - Parallel\ndraft: true\n---\n\n\n\n::: {.cell}\n\n:::\n\n\n<br>\n\n## Overview\n\nWhile most of the datasets we work wtih are of a manageable size here at Telethon Kids Institute, occasionally we are suddenly presented with data containing *millions* of rows and *thousands* of columns. Performing almost any operation on these large datasets can take hours to execute -- a real test of patience for exploratory work!\n\nParallel computing is one relatively simple way to reduce computation time on large datasets – leveraging computer's processing capabilities and simultaneously distributing tasks \"in parallel\" across multiple processors (without too much extra code). This is particularly useful for loop-based computations/simulations, where multiple parameters/conditions must be swept over and explored to appropriately tune a model or output.\n\nThat being said, parallel computing is not the \"magical general solution\" to all computationally intensive tasks – it is still worth investing the time to make sure the packages and/or functions being used are compatible in a parallel workflow. There are still circumstances where the always handy `purrr::map` functinon, or basic for-loops can be equally or more efficient than initialising a parallel environment.\n\n\n::: {.cell}\n\n:::\n\n\n<p>\n\n## What is Parallel Computing?\n\nParallel computing refers to allocating multiple operations in \"parallel\" across a machine's multiple cores/processors. Most modern computers have at least four cores, and often vastly more. By default, R uses only one core, running iterative operations sequentially; not starting the next until the previous is complete. Therefore, under a parallel framework, multiple iterations/calculations can be allocated across multiple cores and completed at the same time -- saving time.\n\nIn this blog, we will have a look at a couple of examples using `R`. We provide some simple examples and use cases, in addition to some further resources if you would like to learn more.\n\nThe [`doParallel`](#0) and [`foreach`](#0) packages of `R` are one set of packages, aimed to be used together, to facilitate a syntactically simple parallel computing environment in `R` – by use of the `%dopar` operator. Additionally the base `parallel` package is used for some basic set-up and detection.\n\nThese examples are run using R version 4.3.2 (2023-10-31 ucrt) on Windows 10 x64.\n\n<br>\n\n### Setup\n\n-   Let's check the number of cores available on our machine.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparallel::detectCores()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 8\n```\n\n\n:::\n:::\n\n\n<p>\n\n-   We should not allocate all of our machine's available cores to parallel computations – this will consume all of the available resources!\n-   Let's use 6 cores for our examples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::registerDoParallel(6)\n```\n:::\n\n\n-   Below is a screenshot from the Windows task manager. R \"front-end\" instances are initialised for each of the 6 initialised cores.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](parallel_comps.png){fig-align='center' width=400px}\n:::\n:::\n\n\n-   When we no longer require a parallel environment, we must shut down and de-register the parallel cluster – otherwise they will remain active.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::stopImplicitCluster()\n```\n:::\n\n\n-   Let's also set a random seed for sample reproducibility\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2024)\n```\n:::\n\n\n<br>\n\n## Example 1: Bootstrapping\n\nBootstrapping is a simple metric that uses random sampling with replacement to mimic the sampling process. This allows us to estimate the sampling distribution for a statistic (mean, variance, etc.) and assess its stability/reliability, without making strong assumptions about the underlying population. What is more, when we are constrained by the data (small sample sizes, insufficient variation, violation of model assumptions) we can use bootstrapping to assess stability and validity.\n\nIn `R`, for-loops can be used to derive a set of bootstrap samples. As we will see, parallel environments can much more efficiently handle operations like these of a repeated nature.\n\n-   Let's use the `mtcars` data set to run a basic linear regression.\n    -   We would like to explore the association between a vehicle's fuel efficiency (`mpg`) and horsepower (`hp`), weight (`wt`) and transmission type (automatic/manual, `am`).\n-   We suspect some of the assumptions of standard linear regression may be violated (normality of errors) because of the small sample size. By bootstrapping, we can explore the stability of the regression coefficients by calculating a confidence interval about the bootstrapped coefficient estimates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Let's set up the data and number of bootstrap samples\ndata(mtcars)\nmtcars <- as_tibble(mtcars)\n\nhead(mtcars)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 11\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1  21       6   160   110  3.9   2.62  16.5     0     1     4     4\n2  21       6   160   110  3.9   2.88  17.0     0     1     4     4\n3  22.8     4   108    93  3.85  2.32  18.6     1     1     4     1\n4  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1\n5  18.7     8   360   175  3.15  3.44  17.0     0     0     3     2\n6  18.1     6   225   105  2.76  3.46  20.2     1     0     3     1\n```\n\n\n:::\n:::\n\n\n<p>\n\n-   Let's set the number of bootstrap trials.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrials <- 10000 # Number of bootstrap samples\n```\n:::\n\n\n<p>\n\n### For-loop\n\n-   Let's begin by using the \"tried and true\" for-loop. For each of the 10^{4} bootstrap samples, we:\n    -   Initialise a bootstrap sample, `ind`.\n    -   Run our linear regression on the bootstrap sample, `results`\n    -   Extract the coefficients from `results`, and append this to an overall coefficient matrix `bootstrap_coefs`.\n-   Subsequently, we can calculate a 95% confidence interval for each of our parameter estimates by taking the 2.5th and 97.5th percentiles from the bootstrap distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart <- proc.time() # Start our timer!\n\n# Initialise a matrix to store the coefficients from each bootstrap sample\nbootstrap_coefs <- matrix(NA, nrow = trials, ncol = 4)\ncolnames(bootstrap_coefs) <- names(coef(lm(mpg ~ hp + wt + am, data = mtcars)))\n\nfor (i in 1:trials){\n  ind <- mtcars[sample(nrow(mtcars), replace = TRUE),]\n  result <- lm(mpg ~ hp + wt + as_factor(am), data = ind)\n  \n  bootstrap_coefs[i, ] <- coef(result)\n}\n\n# Calculate the 2.5th and 97.5th percentile for each variable's coefficient estimates from the bootstrap distribution.\nbootstrap_cis <- apply(bootstrap_coefs, 2, function(coefs){quantile(coefs, probs = c(0.025, 0.975))})\n\nend <- proc.time() # End our timer!\ntime1 <- end-start\nprint(time1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n   8.97    0.10    9.11 \n```\n\n\n:::\n:::\n\n\n<p>\n\n### `%do%` loop (not parallel)\n\n-   As an alternative, let's also use the `%do%` operator from the `foreach` package. Similar to a for-loop, the loop is sequentially executed.\n-   This is similar in execution time to the for-loop operation..\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart <- proc.time()\n\nbootstrap_results <- foreach::foreach(i = 1:trials, .combine = rbind) %do% {\n  ind <- mtcars[sample(nrow(mtcars), replace = TRUE), ]\n  result <- lm(mpg ~ hp + wt + am, data = ind)\n  coef(result)\n}\n\n# Calculate the 2.5th and 97.5th percentile for each variable's coefficient estimates from the bootstrap distribution.\nbootstrap_cis <- apply(bootstrap_results, 2, function(coefs) {quantile(coefs, probs = c(0.025, 0.975))})\n\nend <- proc.time()\ntime2 <- end-start\nprint(time2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n   7.38    0.02    7.41 \n```\n\n\n:::\n:::\n\n\n<p>\n\n### `%dopar%` parallelisation\n\n-   Now, let's run this in parallel across 6 cores.\n-   The execution time is significantly reduced relative to the above two cases.\n-   The `%dopar%` operator defines the for-loop in the parallel environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::registerDoParallel(cores = 6)\n\nstart <- proc.time()\nbootstrap_results <- foreach(i = 1:trials, .combine = rbind, .packages = 'stats') %dopar% {\n  ind <- mtcars[sample(nrow(mtcars), replace = TRUE), ]\n  result <- lm(mpg ~ hp + wt + am, data = ind)\n  coef(result)\n}\n\n# Calculate the 2.5th and 97.5th percentile for each variable's coefficient estimates from the bootstrap distribution.\nbootstrap_cis <- apply(bootstrap_results, 2, function(coefs) {quantile(coefs, probs = c(0.025, 0.975))})\n\nend <- proc.time()\ntime3 <- end-start\nprint(time3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n   2.23    0.30    3.63 \n```\n\n\n:::\n\n```{.r .cell-code}\ndoParallel::stopImplicitCluster()\n```\n:::\n\n\n<br>\n\n## Example 2: Parallel regressions with different outcomes\n\n-   Now let's apply this to a different situation – where we have different model specifications we would like to run on the same \"large\" data set.\n-   Let's use `nycflights13::flights` dataset – a set of over 300,000 flight records that departed from all NYC airports in 2013.\n-   We would like to explore how arrival delay (as a continuous and dichotomous outcome variable) may be influenced by a set of independent variables. We would like to stratify this by month.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights <- nycflights13::flights\nflights <- flights %>%\n  select(year, month, day, dep_delay, arr_delay, air_time, distance) %>%\n  mutate(arr_delay_bin = as.factor(case_when(arr_delay >  15 ~ 1, TRUE ~ 0)))\nflights\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 336,776 × 8\n    year month   day dep_delay arr_delay air_time distance arr_delay_bin\n   <int> <int> <int>     <dbl>     <dbl>    <dbl>    <dbl> <fct>        \n 1  2013     1     1         2        11      227     1400 0            \n 2  2013     1     1         4        20      227     1416 1            \n 3  2013     1     1         2        33      160     1089 1            \n 4  2013     1     1        -1       -18      183     1576 0            \n 5  2013     1     1        -6       -25      116      762 0            \n 6  2013     1     1        -4        12      150      719 0            \n 7  2013     1     1        -5        19      158     1065 1            \n 8  2013     1     1        -3       -14       53      229 0            \n 9  2013     1     1        -3        -8      140      944 0            \n10  2013     1     1        -2         8      138      733 0            \n# ℹ 336,766 more rows\n```\n\n\n:::\n:::\n\n\n<p>\n\nWe would like to specify a set of models to explore flight delay\n\n-   Outcome variables\n\n    -   Arrival delay (continuous)\n\n    -   Arrival delayed (dichotomous)\n\n-   Independent variables\n\n    -   Flight distance (`distance`)\n\n    -   Air time (`air_time`)\n\n    -   Departure delay (`dep_delay`)\n\n-   Stratification variable\n\n    -   Month (`month`)\n\n\n::: {.cell}\n\n```{.r .cell-code}\noutcome_vars <- c(\"distance\", \"air_time\", \"dep_delay\")\ninput_vars <- c(\"arr_delay\", \"arr_delay_bin\")\n\n#formulas <- map(output_vars, ~paste(.x, \"~\", paste(input_vars, collapse = \" + \")))\n```\n:::\n\n\n<p>\n\n### For-loop\n\n-   For each \"month\", we extract the subset of data\n\n-   For each outcome variable, we run a model. If the outcome variable is continuous, we run a simple linear model; otherwise we run a basic logistic regression.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart <- proc.time()\nfor (i in unique(flights$month)){\n  month_dat <- flights %>% filter(month == i)\n  for (j in input_vars){\n    if (j == \"arr_delay\"){\n      model <- lm(as.formula(paste(j,\"~\",paste(outcome_vars, collapse = \"+\"))),\n                  data = month_dat)\n    } else if (j == \"arr_delay_bin\"){\n      model <- glm(as.formula(paste(j,\"~\",paste(outcome_vars, collapse = \"+\"))),\n                   family = binomial,\n                   data = month_dat)\n    }\n  }\n}\nend <- proc.time()\nprint(end - start)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n   1.26    0.22    1.49 \n```\n\n\n:::\n:::\n\n\n<p>\n\n### `%dopar%` parallelisation\n\n-   Similar situation to before. We use the `%:%` operator from the `foreach` package to nest a for-loop within a parallel environment.\n\n-   The syntax does not differ too dramatically.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::registerDoParallel(cores = 6)\n\nstart <- proc.time()\nresults <- foreach(i = unique(flights$month), .packages = \"stats\") %:%\n  foreach(j = outcome_vars) %dopar% {\n    month_dat <- subset(flights, month = i)\n    if (j == \"arr_delay\"){\n      model <- lm(as.formula(paste(j,\"~\",paste(outcome_vars, collapse = \"+\"))),\n                  data = month_dat)\n    } else if (j == \"arr_delay_bin\"){\n      model <- glm(as.formula(paste(j,\"~\",paste(outcome_vars, collapse = \"+\"))),\n                   family = binomial,\n                   data = month_dat)\n    }\n  }\nend <- proc.time()\nprint(end - start)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n   0.18    0.07    0.98 \n```\n\n\n:::\n\n```{.r .cell-code}\ndoParallel::stopImplicitCluster()\n```\n:::\n\n\n<br>\n\n## Conclusion\n\nUsing just a few relatively simple examples, we have demonstrated that a parallel computing environment, in some circumstances, is a relatively easy and quick way to improve the runtime of iteration/loop-based code. While not appropriate in all circumstances (depending on the exact use-case), it warrants further investigation, particularly as datasets grow increasingly large and analytical methodologies increasingly complex.\n\n<br>\n\n## **Acknowledgements**\n\nThanks to Wesley Billingham, Matt Cooper and Elizabeth McKinnon for providing feedback on and reviewing this post.\n\nYou can look forward to seeing posts from these other team members here in the coming weeks and months.\n\n<br>\n\n## Further Resources\n\n-   Vignettes\n    -   <https://cran.r-project.org/web/packages/doParallel/vignettes/gettingstartedParallel.pdf>\n-   Tutorials\n    -   <https://unc-libraries-data.github.io/R-Open-Labs/Extras/Parallel/foreach.html>\n\n<br>\n\n## Session Info\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.3.2 (2023-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_Australia.utf8  LC_CTYPE=English_Australia.utf8   \n[3] LC_MONETARY=English_Australia.utf8 LC_NUMERIC=C                      \n[5] LC_TIME=English_Australia.utf8    \n\ntime zone: Australia/Perth\ntzcode source: internal\n\nattached base packages:\n[1] parallel  stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] doParallel_1.0.17 iterators_1.0.14  foreach_1.5.2     lubridate_1.9.3  \n [5] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n [9] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1    \n[13] tidyverse_2.0.0  \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.5       jsonlite_1.8.8     compiler_4.3.2     tidyselect_1.2.1  \n [5] nycflights13_1.0.2 scales_1.3.0       yaml_2.3.8         fastmap_1.1.1     \n [9] R6_2.5.1           generics_0.1.3     knitr_1.46         htmlwidgets_1.6.4 \n[13] munsell_0.5.1      pillar_1.9.0       tzdb_0.4.0         rlang_1.1.3       \n[17] utf8_1.2.4         stringi_1.8.3      xfun_0.43          timechange_0.3.0  \n[21] cli_3.6.2          withr_3.0.0        magrittr_2.0.3     digest_0.6.35     \n[25] grid_4.3.2         rstudioapi_0.16.0  hms_1.1.3          lifecycle_1.0.4   \n[29] vctrs_0.6.5        evaluate_0.23      glue_1.7.0         codetools_0.2-20  \n[33] fansi_1.0.6        colorspace_2.1-0   rmarkdown_2.26     tools_4.3.2       \n[37] pkgconfig_2.0.3    htmltools_0.5.8.1 \n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}