{
  "hash": "a7c461e1165aa36080eb7d87149e1d08",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Parallel Computing in R\"\nauthor: \"Zac Dempsey\"\nformat:\n  html:\n    code-fold: true\n    toc: true\n    toc-location: left\ndate: \"2024-07-01\"\ncategories:\n  - R\n  - Parallel\ndraft: false\n---\n\n\n\n\n```         \n```\n\n# Overview\n\nWhile most of the datasets we work with are of a manageable size here at the Telethon Kids Institute, occasionally we are presented with data containing *millions* of rows and/or *thousands* of columns. Performing almost any operation on these large datasets can take hours to execute -- a real test of patience for exploratory work!\n\nParallel computing is one relatively simple way to reduce computation time on large datasets – leveraging our computer's full processing capabilities by simultaneously distributing tasks \"in parallel\" across multiple processors (without too much extra code). This is particularly useful for loop-based computations, where multiple parameters or conditions must be swept over whether that be as part of variable creation, model tuning/execution, or some other task.\n\nThat being said, while parallel computing is not a \"magical general solution\" to all computationally intensive tasks – it is worth investing some time to understand how it works, and become familiar with the packages and/or functions used within a parallel framework. There are still circumstances where the handy `purrr::map` function (or the like) can be just as, or more, efficient than a parallel distribution.\n\n\n::: {.cell}\n\n:::\n\n\n<p>\n\n# What is Parallel Computing?\n\nParallel computing refers to executing multiple operations in \"parallel\" (i.e., at the same time) across a machine's multiple cores/processors. Most modern computers have at least four cores, and often many more. By default, R uses only one core, running iterative operations sequentially; not starting the next until the previous is complete. Under a parallel framework, however, multiple iterations/calculations can be allocated across multiple cores so they can be executed at the same time — thus saving (overall) time. These time savings accumulate particularly with large underlying tasks *(as opposed to running many small tasks in parallel — a situation that often ultimately ends up slower!)*.\n\nIn this post, we will have a look at a couple of examples using `R`. We provide some simple examples and use-cases, in addition to some additional resources to continue learning.\n\nThe [`doParallel`](#0) and [`foreach`](#0) packages of `R` are one set of packages, intended to be used together, to facilitate parallel computing in a syntactically simple way. Additionally, the base `parallel` package is used for some basic set-up and detection.\n\nThese examples are run using R version 4.3.3 (2024-02-29) on Darwin 23.4.0.\n\n# When should we consider Parallel Computing?\n\nIt is potentially easier to think about when we should *not* consider parallel computing. For parallel computing to be workable, the overall job needs to be something that can be broken down into [independent]{.underline} tasks — tasks that do not depend on the input/output of previous tasks to fully run. So, any piece of computational work - where the underlying tasks must be completed in sequential order - can not be executed (or sped up) with parallel computing. Such jobs may be the sequential (time dependent) processing of data (e.g., need to standardise variables *before* extreme values are identified and tagged) or ordered computation tasks (e.g., matrix algebra within a regression model).\n\nA quick analogy: you can not complete the job of 'making a pizza' in a parallel way *\\[you can not cook the pizza at the same time as you are preparing the dough at the same time as you are putting the toppings on the pizza\\].* But, if are making 10 pizzas for a customer, you can complete parts of this job in parallel, for example 'preparing all the pizza dough' *\\[by getting 10 people with 10 bowls and 10 bags of flour to all mix a serve of dough at the same time\\]*.\n\nAs it relates to data analysis, parallel computing requires us to divvy up tasks in a standalone way, and (generally) assemble the output from those tasks in a sensible way. An example:\n\n-   Six months of minute-level data for 100 patients, where you need to calculate a range of summary statistics (including from some advanced modelling) for each patient.\n-   The data can be subset into '100' chunks as the summary statistics for each 'chunk' do not require access to (or knowledge of) the raw data or output (summary statistics) from any other chunk.\n-   The resulting summary statistics can then be compiled into a table or list for later use.\n\nOf course, there is some overhead to allocating tasks and compiling results to different cores that one may want to consider before proceeding. Generally, it is worthwhile to pilot your code on a subset of the job to see if the code does truly benefit from parallelisation.\n\n<br>\n\n# Setup\n\nLet's first load the packages we will use to render our examples\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)  # For general \"tidyverse\" style manipulations & operations\nlibrary(parallel)   # base R package for parallel processing\nlibrary(foreach)    # For looping\nlibrary(doParallel) # Parallel back-end for the `foreach` package\n```\n:::\n\n\nLet's check the number of cores available on our machine.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparallel::detectCores()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 8\n```\n\n\n:::\n:::\n\n\n<p>\n\nWe should not allocate all of our machine's available cores to parallel computations – this will consume all of the available resources (and everything might grind to a halt)!\\\n\\\nLet's use 6 cores for our examples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::registerDoParallel(6)\n```\n:::\n\n\nWhen we go to the Windows task manager, we see there are now 6 R front-end instances initialised, one for each of the 6 cores.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](parallel_comps.png){fig-align='center' width=700px}\n:::\n:::\n\n\nWhen we no longer require a parallel environment, we must shut down and de-register the parallel cores. Otherwise they will remain active (allocated/'in-use')!\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::stopImplicitCluster()\n```\n:::\n\n\nLet's also set a random seed for sample reproducibility.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2024)\n```\n:::\n\n\n<br>\n\n# Example 1: Bootstrapping\n\nBootstrapping is a method that uses simple random sampling with replacement to mimic random sampling of a population. This allows us to estimate the sampling distribution for a statistic (mean, variance, etc.) and assess its stability/reliability, without making strong assumptions about the underlying population. This can be useful when the data are problematic in some fundamental way (small sample sizes, insufficient variation, violation of model assumptions).\n\nIn `R`, for-loops can be used to derive a set of bootstrap samples. As we will see, parallel environments can much more efficiently handle repeated independent operations like these.\n\nLet's use the `mtcars` data set to run a basic bootstrapped linear regression.\n\nWe would like to explore the association between a vehicle's fuel efficiency (`mpg`) and horsepower (`hp`), weight (`wt`) and transmission type (automatic/manual, `am`).\n\nWe suspect – perhaps due to the small sample size – that some of the assumptions of standard linear regression may be violated (normality of errors). With bootstrapping, we can explore the stability of the regression coefficients by calculating a confidence interval about the bootstrapped coefficient estimates.\n\n<p>\n\nWe will present 3 ways to bootstrap data – two using for-loop structures, and one using a parallel computing environment. In each instance, we will compare how the output is structured and results returned.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Let's set up the data and number of bootstrap samples\ndata(mtcars)\nmtcars <- as_tibble(mtcars)\n\ntrials <- 10000 # Number of bootstrap samples\n```\n:::\n\n\n<p>\n\n## For-loop\n\n-   Let's begin by using a traditional for-loop. For each bootstrap sample, we:\n    -   Initialise a bootstrap sample, `ind`.\n    -   Run our linear regression on the bootstrap sample, `results`\n    -   Extract the coefficients from `results`, and append this to an overall coefficient matrix `bootstrap_coefs`.\n-   Subsequently, we calculate a 95% confidence interval for each of our parameter estimates by taking the 2.5th and 97.5th percentiles from the bootstrap distribution and calling this `bootstrap_cis`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart <- proc.time() # Start our timer!\n\n# Initialise a matrix to store the coefficients from each bootstrap sample\nbootstrap_coefs <- matrix(NA, nrow = trials, ncol = 4)\ncolnames(bootstrap_coefs) <- names(coef(lm(mpg ~ hp + wt + am, data = mtcars)))\n\nfor (i in 1:trials){\n  \n  # Take bootstrap sample\n  ind <- mtcars[sample(nrow(mtcars), \n                       replace = TRUE),\n                ]\n  \n  # Construct linear regression\n  result <- lm(mpg ~ hp + wt + as_factor(am), \n               data = ind)\n  \n  # Extract coefficients and store to `bootstrap_coefs`\n  bootstrap_coefs[i, ] <- coef(result)\n}\n\n# Calculate the 2.5th and 97.5th percentile for each variable's coefficient estimates from the bootstrap distribution.\nbootstrap_cis <- apply(bootstrap_coefs, \n                       2, \n                       function(coefs){quantile(coefs, probs = c(0.025, 0.975))})\n\nend <- proc.time() # End our timer!\ntime1 <- end-start\n```\n:::\n\n\n<p>\n\nLet's visualise the first few lines of the bootstrapped results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(bootstrap_coefs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     (Intercept)          hp        wt         am\n[1,]    34.04596 -0.02741585 -3.158154  0.6330600\n[2,]    31.46670 -0.03097903 -2.375727  5.8142235\n[3,]    35.98084 -0.02535775 -3.763464 -0.2485866\n[4,]    33.47330 -0.04410244 -2.343210  2.6890689\n[5,]    32.21798 -0.04138935 -2.222471  1.2289610\n[6,]    32.78747 -0.02758182 -2.989311  1.1744731\n```\n\n\n:::\n:::\n\n\nand associated 95% confidence interval\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbootstrap_cis\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      (Intercept)          hp        wt         am\n2.5%     29.07843 -0.05539954 -5.134682 -0.7627477\n97.5%    40.74463 -0.02161861 -1.057830  4.9428501\n```\n\n\n:::\n:::\n\n\n<p>\n\nThis had the following run-time (seconds):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(time1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  4.254   0.011   4.293 \n```\n\n\n:::\n:::\n\n\n::: callout-note\n## `proc.time` components\n\n-   *user* = time the CPU has spent executing the R process.\n\n-   *system* = time the CPU has spent on system-level operations that facilitate the R process (e.g., memory management, system calls).\n\n-   *elapsed* = real-world time that has elapsed.\n:::\n\n<p>\n\n## `%do%` loop (not parallel)\n\nAs an alternative, let's also use the `%do%` operator from the `foreach` package. Similar to a for-loop, each bootstrap sample is executed sequentially.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart <- proc.time()\n\nbootstrap_coefs <- foreach::foreach(i = 1:trials, .combine = rbind) %do% {\n  ind <- mtcars[sample(nrow(mtcars), replace = TRUE), ]\n  result <- lm(mpg ~ hp + wt + am, data = ind)\n  coef(result)\n}\n\n# Calculate the 2.5th and 97.5th percentile for each variable's coefficient estimates from the bootstrap distribution.\nbootstrap_cis <- apply(bootstrap_coefs, \n                       2, \n                       function(coefs) {quantile(coefs, probs = c(0.025, 0.975))})\n\nend <- proc.time()\ntime2 <- end-start\n```\n:::\n\n\n<p>\n\nSimilarly, let's visualise the first few lines of the bootstrapped results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(bootstrap_coefs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         (Intercept)          hp        wt       am\nresult.1    32.28476 -0.02807558 -2.989010 2.404865\nresult.2    38.19523 -0.02740136 -4.576854 1.082726\nresult.3    32.87519 -0.06693216 -1.172027 3.879952\nresult.4    29.54068 -0.03806135 -1.468158 1.933494\nresult.5    31.54392 -0.02793433 -2.756060 1.347816\nresult.6    34.42623 -0.03900395 -2.865725 1.167583\n```\n\n\n:::\n:::\n\n\nand associated 95% confidence interval\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbootstrap_cis\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      (Intercept)          hp        wt         am\n2.5%     29.03455 -0.05615444 -5.286909 -0.8228096\n97.5%    41.09610 -0.02124169 -1.055386  4.9293286\n```\n\n\n:::\n:::\n\n\n<p>\n\nThis had the following run-time (seconds):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(time2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  3.680   0.009   3.697 \n```\n\n\n:::\n:::\n\n\n<p>\n\n## `%dopar%` parallelisation\n\nNow, let's run this in parallel across 6 cores. The `%dopar%` operator defines the for-loop in the parallel environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::registerDoParallel(cores = 6) # Initialise parallel cluster\n\nstart <- proc.time()\nbootstrap_coefs <- foreach(i = 1:trials, .combine = rbind, .packages = 'stats') %dopar% {\n  ind <- mtcars[sample(nrow(mtcars), replace = TRUE), ]\n  result <- lm(mpg ~ hp + wt + am, data = ind)\n  coef(result)\n}\n\n# Calculate the 2.5th and 97.5th percentile for each variable's coefficient estimates from the bootstrap distribution.\nbootstrap_cis <- apply(bootstrap_coefs, \n                       2, \n                       function(coefs) {quantile(coefs, probs = c(0.025, 0.975))})\n\nend <- proc.time()\ntime3 <- end-start\n\ndoParallel::stopImplicitCluster() # De-register parallel cluster\n```\n:::\n\n\nAs expected, the output of the bootstrapped coefficient distribution are identical before\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(bootstrap_coefs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         (Intercept)          hp        wt        am\nresult.1    31.48209 -0.03671749 -2.342623 1.4217909\nresult.2    33.95259 -0.02362518 -3.406813 0.9267277\nresult.3    36.59211 -0.03248322 -3.755576 0.9103209\nresult.4    34.87373 -0.03891634 -3.238203 0.8272840\nresult.5    33.20963 -0.04487305 -2.284206 2.1212567\nresult.6    33.79241 -0.04119127 -2.805964 2.6563277\n```\n\n\n:::\n:::\n\n\nas are the associated 95% confidence intervals.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbootstrap_cis\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      (Intercept)          hp        wt        am\n2.5%     29.01510 -0.05586253 -5.215733 -0.786392\n97.5%    40.95525 -0.02165217 -1.047514  4.892388\n```\n\n\n:::\n:::\n\n\nLastly, this had the following run-time (seconds)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(time3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  4.817   0.344   1.359 \n```\n\n\n:::\n:::\n\n\n<p>\n\n## Discussion\n\nImmediately, the syntax of the alternative for-loop structures are **more readable** and **easier to construct** than the traditional for-loop. Because the `foreach::foreach` function easily combines output in a list, we need not define an empty matrix to append output to.\n\nComputation time in the parallel environment is significantly faster — approximately 68% faster than the traditional for-loop! Across multiple analyses and data sets, these time savings certainly add up!\n\n<br>\n\n# Example 2: Random forest model\n\nLet's say we would like a model that predicts the distribution of a plant species based on various environmental factors (temperature, precipitation, elevation, vegetation type). We would like to use a random forest model (using the `randomForest` package of `R`) — a popular model for classification which combines the output across multiple decision trees.\\\n\\\nThis example is inspired by the example from the [vignette of the `foreach` package](https://cran.r-project.org/web/packages/foreach/vignettes/foreach.html).\n\n## Data\n\nFirst, let's simulate our data set and set some parameters:\n\n-   10,000 observations.\n\n-   Independent variables temperature, precipitation and elevation sampled from a random normal distribution and vegetation type (categorical factor) randomly prescribed.\n\n-   Species presence (dichotomous) outcome variable is randomly prescribed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 10000 # Sample size\n\ndata <- data.frame(temperature = rnorm(n, \n                                       mean = 15, \n                                       sd   = 40),\n                   precipitation = rnorm(n, \n                                         mean = 1000, \n                                         sd   = 300),\n                   elevation = rnorm(n, \n                                     mean = 500, \n                                     sd   = 200),\n                   vegetation_type = as_factor(sample(c(\"forest\", \n                                                        \"grassland\", \n                                                        \"wetland\", \n                                                        \"desert\"), \n                                                      n, \n                                                      replace = T)),\n                   species_presence = as_factor(sample(c(\"present\", \n                                                         \"absent\"), \n                                                       n, \n                                                       replace = T)))\n```\n:::\n\n\nLet's assign 70% of the data to our training set, and the remaining 30% to test data set and initialise a random forest model with 1000 trees.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_index <- sample(1:n, 0.7*n)\n\ntrain_data <- data[train_index, ]\ntest_data <- data[-train_index, ]\n\nnum_trees <- 1000\n```\n:::\n\n\n<p>\n\nInstead of running one random forest model comprising 1000 trees, let's combine the results of 4 smaller random forest models models each comprising 250 trees. By doing this, we can return more reliable and robust output (smaller random forest models are less prone to overfitting) and better manage working memory (smaller models require less memory to train and store).\n\n<p>\n\n## For-loop\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart <- proc.time()\nrf <- list()\nfor (i in 1:(num_trees/250)){\n\n  rf[[i]] <- randomForest::randomForest(species_presence ~ ., \n                                        data = train_data, \n                                        ntree = num_trees/4)\n}\n\ncombined_output <- do.call(randomForest::combine, rf)\n\npredictions <- predict(combined_output, test_data %>% select(-species_presence))\n\nend <- proc.time()\ntime1 <- end - start\n```\n:::\n\n\nLet's print the confusion matrix of this output. Because this data was relatively simply simulated, we don't expect the predictive power to be too great.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(predictions, test_data$species_presence)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           \npredictions absent present\n    absent     707     769\n    present    779     745\n```\n\n\n:::\n:::\n\n\nThis has the following run-time (seconds):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(time1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  6.172   0.156   6.366 \n```\n\n\n:::\n:::\n\n\n<p>\n\n## %do% loop (not parallel)\n\nSimilar to the traditional for-loop, we can sequentially execute this code using the `%do%` operator.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart <- proc.time()\nrf <- foreach::foreach(ntree = rep(num_trees/4, 4), \n                       .combine = randomForest::combine, \n                       .packages = 'randomForest') %do%\n  randomForest::randomForest(species_presence ~ ., \n                             data = train_data,\n                             ntree = ntree)\n\npredictions <- predict(rf, test_data %>% select(-species_presence))\n\nend <- proc.time()\ntime2 <- end - start\n```\n:::\n\n\nLet's print the confusion matrix of this output. Because this data was relatively simply simulated, we don't expect the predictive power to be too great.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(predictions, test_data$species_presence)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           \npredictions absent present\n    absent     699     777\n    present    787     737\n```\n\n\n:::\n:::\n\n\nThis has the following run-time (seconds):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(time2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  5.949   0.221   6.216 \n```\n\n\n:::\n:::\n\n\n<p>\n\n## %dopar% parallelisation\n\nFor simplicity, let's allocate 4 cores to the computation and imagine that one core is responsible for processing one of the four random forest models simultaneously.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::registerDoParallel(cores = 4)\n\nstart <- proc.time()\nrf <- foreach::foreach(ntree = rep(num_trees/4, 4), \n                       .combine = randomForest::combine, \n                       .packages = 'randomForest') %dopar%\n  randomForest::randomForest(species_presence ~ ., \n                             data = train_data,\n                             ntree = ntree)\n\npredictions <- predict(rf, test_data %>% select(-species_presence))\n\nend <- proc.time()\ndoParallel::stopImplicitCluster()\n\ntime3 <- end-start\n```\n:::\n\n\nLet's print the confusion matrix of this output. Because this data was relatively simply simulated, we don't expect the predictive power to be too great.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(predictions, test_data$species_presence)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           \npredictions absent present\n    absent     707     772\n    present    779     742\n```\n\n\n:::\n:::\n\n\nThis now has the following run-time (seconds):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(time3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  6.344   0.688   2.907 \n```\n\n\n:::\n:::\n\n\n<p>\n\n## Discussion\n\nAgain, using [easily adaptable and readable syntax]{.underline}, we leverage a parallel environment to significantly lessen the computation time of our large model. Relative to a standard for-loop, the parallelised computation is approximately 54% faster.\n\n<br>\n\n# Example 3: Parallel regressions with different outcomes\n\nAs hinted to in the overview to this blog, parallel computing is not the general solution to running loop-based code more quickly. Instances which run intrinsically \"fast\" will not benefit from a parallel environment and may in fact run *slower* in parallel, after accounting for the overhead in 'managing' the task division and result assembly.\\\n\\\nThere are other packages which may be useful in these circumstances, such as the often handy `purrr::map` which we examine below.\n\nLet's apply this to a situation where we have different model specifications we would like to run on the same data set.\n\n<p>\n\n## Data\n\n-   Let's use `nycflights13::flights` — a set of over 300,000 flight records that departed from all NYC airports in 2013.\n-   We would like to explore how arrival delay (as a continuous and dichotomous (delayed = 1, not delayed = 0) outcome variable) may be influenced by a set of independent variables. We would like to stratify this by month.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights <- nycflights13::flights\nflights <- flights %>%\n  select(year, day, dep_delay, arr_delay, air_time, distance) %>%\n  mutate(arr_delay_bin = as.factor(case_when(arr_delay >  15 ~ 1, TRUE ~ 0)))\n\nflights\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 336,776 × 7\n    year   day dep_delay arr_delay air_time distance arr_delay_bin\n   <int> <int>     <dbl>     <dbl>    <dbl>    <dbl> <fct>        \n 1  2013     1         2        11      227     1400 0            \n 2  2013     1         4        20      227     1416 1            \n 3  2013     1         2        33      160     1089 1            \n 4  2013     1        -1       -18      183     1576 0            \n 5  2013     1        -6       -25      116      762 0            \n 6  2013     1        -4        12      150      719 0            \n 7  2013     1        -5        19      158     1065 1            \n 8  2013     1        -3       -14       53      229 0            \n 9  2013     1        -3        -8      140      944 0            \n10  2013     1        -2         8      138      733 0            \n# ℹ 336,766 more rows\n```\n\n\n:::\n:::\n\n\n<p>\n\nWe would like to specify a set of models which predict overall flight delay, as both continuous (arrival delay time) and dichotomous (delayed yes/no) outcomes.\n\n-   Outcome variables\n\n    -   Arrival delay (continuous)\n    -   Arrival delayed (dichotomous)\n\n-   Independent variables\n\n    -   Flight distance (`distance`)\n\n    -   Air time (`air_time`)\n\n    -   Departure delay (`dep_delay`)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nindep_vars <- c(\"distance\", \"air_time\", \"dep_delay\")\noutcome_vars <- c(\"arr_delay\", \"arr_delay_bin\")\n```\n:::\n\n\nFor each outcome variable, we run a model. If the outcome variable is continuous, we run a simple linear model; otherwise we run a basic logistic regression.\n\n<p>\n\n## For-loop\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart <- proc.time()\nmodels <- list() # To store our model output\n\nfor (i in outcome_vars){\n  if (i == \"arr_delay\"){\n    model <- lm(as.formula(paste(i,\"~\",paste(indep_vars, collapse = \"+\"))),\n                data = flights)\n    \n  } else if (i == \"arr_delay_bin\"){\n    model <- glm(as.formula(paste(i,\"~\",paste(indep_vars, collapse = \"+\"))),\n                 family = binomial,\n                 data = flights)\n  }\n  models[[i]] <- summary(model)\n}\n\nend <- proc.time()\ntime1 <- end-start\n```\n:::\n\n\nThis returns a list with the model output summary for each of our models.\n\nThe for-loop has the following run-time (seconds):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(time1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  0.545   0.075   0.624 \n```\n\n\n:::\n:::\n\n\n<p>\n\n## `purrr::map`\n\nMap functions apply a function to each element of a list/vector and return an object. In cases relying on multiple computations across different values, they often come in handy.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart <- proc.time()\n\n\nmodels <- map(outcome_vars, \n      ~ if (.x == \"arr_delay\"){\n        lm(as.formula(paste(.x, \"~\", \n                            paste(indep_vars, collapse = \" + \"))),\n           data = flights) %>%\n          summary()\n        \n        } else if (.x == \"arr_delay_bin\"){\n          glm(as.formula(paste(.x, \"~\",\n                               paste(indep_vars, collapse = \" + \"))),\n              family = binomial,\n              data = flights) %>%\n            summary()\n        }\n      )\nnames(models) <- outcome_vars\n\nend <- proc.time()\ntime2 <- end-start\n```\n:::\n\n\nThis has the following run-time (seconds):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(time2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  0.453   0.072   0.526 \n```\n\n\n:::\n:::\n\n\n<p>\n\n## `furrr::future_map`\n\nThere is also a parallel implementation of the `purrr::map` function, offered by the `furrr` package. The syntax is (nicely) identical to above, but importantly relies on specifying a parallel (`multisession`) \"plan\" ahead of executing the code (similar to what we did in Example 1 and 2).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(furrr)\n\nplan(multisession, workers = 6) # Initialise parallel environment using furrr\n\nstart <- proc.time()\nmodels <- furrr::future_map(outcome_vars, \n      ~ if (.x == \"arr_delay\"){\n        lm(as.formula(paste(.x, \"~\", \n                            paste(indep_vars, collapse = \" + \"))),\n           data = flights) %>%\n          summary()\n        \n        } else if (.x == \"arr_delay_bin\"){\n          glm(as.formula(paste(.x, \"~\",\n                               paste(indep_vars, collapse = \" + \"))),\n              family = binomial,\n              data = flights) %>%\n            summary()\n        }\n      )\nnames(models) <- outcome_vars\n\nend <- proc.time()\ntime3 <- end-start\n\nplan(sequential) # Revert to sequential processing\n```\n:::\n\n\nThis has the following run-time (seconds):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(time3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  0.098   0.027   1.210 \n```\n\n\n:::\n:::\n\n\n<p>\n\n## `%dopar%` parallelisation\n\nAlternatively, as done earlier, we can turn our non-parallel `%do%` code into parallel `%dopar%` code.\n\n-   We use the `%:%` operator from the `foreach` package to nest a for-loop within a parallel environment.\n\n-   The syntax does not differ too dramatically.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::registerDoParallel(cores = 6)\n\nstart <- proc.time()\nmodels <- foreach(j = outcome_vars, .combine = \"list\") %dopar% {\n  if (j == \"arr_delay\"){\n    model <- lm(as.formula(paste(j,\"~\",paste(indep_vars, collapse = \"+\"))),\n                data = flights)\n  } else if (j == \"arr_delay_bin\"){\n    model <- glm(as.formula(paste(j,\"~\",paste(indep_vars, collapse = \"+\"))),\n                 family = binomial,\n                 data = flights)\n  }\n}\nnames(models) <- outcome_vars\n\nend <- proc.time()\n\ntime4 <- end - start\ndoParallel::stopImplicitCluster()\n```\n:::\n\n\nThis has the following run-time (seconds):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(time4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  0.121   0.138   0.753 \n```\n\n\n:::\n:::\n\n\n<p>\n\n## Discussion\n\nWe now see that the parallel processing of these tasks takes far *longer* – about 17% so! The underlying set of operations — running a series of linear models — are already small and relatively fast, so the overhead of managing the task (splitting, computing and combining results) in a parallel environment far exceeds what what can easily be spun up using a for-loop (or `purrr::map`).\n\n<br>\n\n# Conclusion\n\nUsing just a few relatively simple examples, we have demonstrated that a parallel computing environment can be a relatively easy and quick way to improve the runtime of iteration/loop-based operations (under the right circumstances). Importantly, this does not increase the code complexity nor decrease the code readability!\n\nWhile not *always* appropriate, it is worth staying across the parallel implementations as they become available on `R`, particularly as the size of datasets and/or the complexity of analytical workflows increase.\n\n<br>\n\n# Further Resources\n\n-   Vignettes\n    -   <https://cran.r-project.org/web/packages/doParallel/vignettes/gettingstartedParallel.pdf>\n    -   <https://cran.r-project.org/web/packages/foreach/vignettes/foreach.html>\n-   Tutorials\n    -   <https://unc-libraries-data.github.io/R-Open-Labs/Extras/Parallel/foreach.html>\n\n<br>\n\n## Acknowledgements\n\nThanks to Wesley Billingham, Matt Cooper, and Elizabeth McKinnon for providing feedback on and reviewing this post.\n\nYou can look forward to seeing posts from these other team members here in the coming weeks and months.\n\n<br>\n\n## Reproducibility Information\n\nTo access the .qmd (Quarto markdown) files as well as any R scripts or data that was used in this post, please visit our GitHub:\n\n<https://github.com/The-Kids-Biostats/The-Kids-Biostats.github.io/tree/main/posts/parallel>\n\nThe session information can also be seen below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Australia/Perth\ntzcode source: internal\n\nattached base packages:\n[1] parallel  stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] furrr_0.3.1          future_1.33.2        randomForest_4.7-1.1\n [4] doParallel_1.0.17    iterators_1.0.14     foreach_1.5.2       \n [7] lubridate_1.9.3      forcats_1.0.0        stringr_1.5.1       \n[10] dplyr_1.1.4          purrr_1.0.2          readr_2.1.5         \n[13] tidyr_1.3.1          tibble_3.2.1         ggplot2_3.5.1       \n[16] tidyverse_2.0.0     \n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.4         generics_0.1.3     stringi_1.8.4      listenv_0.9.1     \n [5] hms_1.1.3          digest_0.6.36      magrittr_2.0.3     evaluate_0.24.0   \n [9] grid_4.3.3         timechange_0.3.0   fastmap_1.2.0      jsonlite_1.8.8    \n[13] fansi_1.0.6        scales_1.3.0       codetools_0.2-20   cli_3.6.3         \n[17] rlang_1.1.4        parallelly_1.37.1  munsell_0.5.1      withr_3.0.0       \n[21] yaml_2.3.8         tools_4.3.3        tzdb_0.4.0         colorspace_2.1-0  \n[25] globals_0.16.3     vctrs_0.6.5        R6_2.5.1           lifecycle_1.0.4   \n[29] htmlwidgets_1.6.4  pkgconfig_2.0.3    pillar_1.9.0       gtable_0.3.5      \n[33] glue_1.7.0         xfun_0.45          tidyselect_1.2.1   rstudioapi_0.16.0 \n[37] knitr_1.47         htmltools_0.5.8.1  rmarkdown_2.27     nycflights13_1.0.2\n[41] compiler_4.3.3    \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}