{
  "hash": "c4d170ec2d74d2eebd6416aa5284271d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Small-sample analysis\"\nformat:\n  html:\n    code-fold: true\n    toc: true\n    toc-location: left\n    html-table-processing: none\ndate: \"2024-09-10\"    \nauthor: \"Dr Bethy McKinnon, Dr Matt Cooper\"\ncategories:\n  - Small Samples\n  - R\ndraft: true\n---\n\n\n\n\n# Overview\n\nThe sample size in research projects is often limited by practical constraints e.g. cost of equipment, logistics of animal breeding and management, or the (low) prevalence of patients with a particular condition. Group sizes of 3-10 are typical of many animal studies and laboratory experiments, and while small can mean different things to different researchers, we're going to focus here on sample statistics and sampling variation as the same size in a project heads down into this territory.\n\n# Sample statistics and sampling variation\n\nTo study a population of interest, data is gathered from a representative sample and then summaries statistics such as the sample median, mean, standard deviation or a proportion are derived to characterise that sample. These sample statistics vary about the *(true unknown)* population parameter for which they provide an estimate, and we can calculate confidence intervals to go along with these estimates to give context to a plausible range for the population parameters.\n\nThe smaller the sample size:\n\n-   the larger the distance between the estimate (statistic we have derived) and the *(true unknown)* population parameter is likely to be, and\n-   the wider the confidence interval.\n\nThis distance and width speaks to sampling error.\n\n\n::: {.cell}\n\n:::\n\n\nConsider the following example that demonstrates how sampling variation increases with decreasing sample sizes.\n\n## Example: Samples of mice\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](small_sample_analysis_files/figure-html/unnamed-chunk-2-1.png){width=768}\n:::\n:::\n\n\n::: column-margin\n**Figure 1**\\\nHere, we simulate data to represent a \"population\" of healthy female C57BL/6J mice who are expected to have a mean weight of 23 grams, with a standard deviation of 2 grams, at 14 weeks of age [The Jackson Laboratory](https://www.jax.org/jax-mice-and-services/strain-data-sheet-pages/body-weight-chart-000664).\n:::\n\nIn practice researchers would only have data on a sample (subset) of a *(usually hypothetical)* population, from which to calculate summary statistics as estimates of general population characteristics. There is a potentially obvious, but perhaps nuanced, point here that needs to be stated, and that is that we do not know the population parameter we are estimating - if we did, we would not be trying to estimate it.\n\nAs may be expected, for larger datasets the calculated summary statistic is more likely to be closer to the *(true unknown)* population parameter from which the sample was drawn. Let's see how the calculated mean might be expected to vary from sample to sample for our mouse weight example.\n\nTo understand this variability, we'll start by randomly drawing 500 samples that are each of size n=50.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](small_sample_analysis_files/figure-html/unnamed-chunk-4-1.png){width=768}\n:::\n:::\n\n\n::: column-margin\n**Figure 2**\\\na) The first 5 samples (of 500) with a sample size of 50, from a simulated population with mean=23 and std dev=2.\\\n<br> b) The distribution of the sample means calculated from each of the 500 draws.\n:::\n\n::: callout-note\nThe mean of the sample means takes a value of 23.00, the same value, to 2 decimal places, as the *(in this instance, because we set it, known)* parameter for the population from which the sample was drawn.\n:::\n\nThe spread (in Fig 2b) appears much reduced, but that is because it is not the spread of individual observations - it is the spread of sample means. In fact, the standard deviation of the sample means (i.e. the standard error of the mean) is 0.3, in line with the expected reduction of the population standard deviation of 2 by a factor of $1/\\sqrt{n}$.\n\nNow let's see what happens as the sample size decreases.\n\nHere, a sample of n=20.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](small_sample_analysis_files/figure-html/unnamed-chunk-6-1.png){width=768}\n:::\n:::\n\n\n::: column-margin\n**Figure 3**\\\na) The first 5 samples (of 500) with a sample size of 20, from a simulated population with mean=23 and std dev=2.\\\n<br> b) The distribution of the sample means calculated from each of the 500 draws.\n:::\n\nThis spread here (in Fig 3b) does not look too different to that seen above (in Fig 2b).\n\nAnd here, a sample of n=5.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](small_sample_analysis_files/figure-html/unnamed-chunk-7-1.png){width=768}\n:::\n:::\n\n\n::: column-margin\n**Figure 4**\\\na) The first 5 samples (of 500) with a sample size of 5, from a simulated population with mean=23 and std dev=2.\\\n<br> b) The distribution of the sample means calculated from each of the 500 draws.\n:::\n\nIt is clear that as the sample size decreases the estimates of the population mean become progressively more variable. This increased variability is taken into account in the formula for calculating the 95% confidence interval which becomes accordingly wider as precision decreases:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](small_sample_analysis_files/figure-html/unnamed-chunk-8-1.png){width=768}\n:::\n:::\n\n\n::: column-margin\n**Figure 5**\\\nRandom samples of increasing size, annotated with means and 95% confidence intervals.\n:::\n\n# Comparative studies with small group sizes\n\nExpanding on these observations, they highlight a major limitation of small-sample studies in that they lack power to detect anything other than large differences or large â€œeffects\". This is because the sampling distribution of the test statistic needs to take account of the increased variability of the sample statistics on which the test procedure is based.\n\nFor comparisons of mean values (e.g. a t-test) a large effect translates to a large difference between group means relative to the within-group variation. Power (which relates to Type-II error) is an indication of the probability of a study to observe (or conclude) a difference if a difference (in the underlying populations being compared) truly exists.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](small_sample_analysis_files/figure-html/unnamed-chunk-10-1.png){width=768}\n:::\n:::\n\n\n::: column-margin\n**Figure 6**\\\nPower plots for a 2-sample t-test, with alpha=0.05 and equal sample sizes, assuming equal variances.\n:::\n\n::: callout-note\nIf n=5 for both groups, the minimum difference at which a 2-sample t-test has at least 80% power to detect is just over 2 standard deviations.\n\nIn line with the example above, if we were comparing weights of 2 treatment groups of mice, and it's reasonable to assume a common SD of 2g, this would correspond to an absolute difference of 4g in the population mean weights - quite a large treatment effect.\n:::\n\nAt the design stage of an experiment, it is important to determine the minimum clinically meaningful difference/effect size you want to be able to detect, and power your study accordingly by including a sufficiently large sample.\n\nFor example, one could consider increasing power by extending the simple 2-sample approach to a factorial design, where the between treatment group differences are simultaneously examined across one or more factors (eg sex, age group).\n\n# Using confidence intervals to guide decision-making\n\nConfidence intervals can be a little tricky to get your head around. They don't provide a range in which XX% (typically 95%) of the data values lie, but rather relate to our belief regarding where the population parameter of interest (e.g. the true mean or proportion) lies.\n\n::: callout-important\nThe \"confidence\" comes from **sampling theory**: Suppose we sample the same population *many many times*, and for each sample calculate a 95% confidence interval *around the sample mean*, we would expect that 95% of the time the constructed interval will capture the true population mean.\n:::\n\nSometimes, just by chance (or bad luck), our whole sample may lie towards one end (the tail) of the population distribution (see S-3 for n=5 in Fig 4a/Fig 5c above) resulting in a 95% CI that happens to \"miss\" the true value. **The real problem here is that for *our* study - we don't actually know if this has happened!**. This is because, in practice, we *only* have a single sample and don't know the true population parameter for the population from which the sample was drawn; hence, the level of confidence simply reflects a rather nominal degree of plausibility.\n\n## Example: Proportions\n\nSo, we know that for small sample sizes the calculated confidence intervals will be wide. This can be even more evident with proportions. Despite this, the upper and/or lower limits of these confidence intervals can still offer useful bounds in which to aid with decision making.\n\n\n::: {.cell}\n\n:::\n\n\nFor example, consider patient recruitment to an early-phase clinical trial of a new drug that is under development. Recruitment may cease early if there is sufficient evidence of high toxicity or provisional efficacy - according to *pre-defined* criteria. Here, stopping rules based on one-sided binomial confidence intervals (think, proportions) can inform trial planning and implementation.\n\nSay enrollment into a dose expansion cohort is planned, with an upper threshold of 40% for allowable dose limiting toxicity and treatment efficacy indicated if response rates are at least 50%. Assuming the planned cohort size is 10, Figure 7 presents 80% lower confidence bounds (LCBs) as a reference for response counts, and more conservative 90% upper confidence bounds (UCBs) for toxicity counts .\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](small_sample_analysis_files/figure-html/unnamed-chunk-12-1.png){width=768}\n:::\n:::\n\n\n::: column-margin\n**Figure 7**\\\n90% upper confidence bounds (UCBs, teal) for dose-limiting toxicity (DLT) and 80% lower confidence bounds (LCBs, ochre) for treatment response, for a sample size of 10. Bounds were constructed from one-sided binomial confidence intervals, using the Clopper-Pearson method.\n:::\n\nThe drug would be deemed to have an acceptable toxicity profile if **no more than 1 subject was impacted by a dose-limiting toxicity (DLT)**. Given the small sample size, if there are 2 or more impacted subjects then the confidence interval for the true toxicity rate will be in excess of the 40% threshold, even though the observed rate may be no more than 20%. The stopping rule for unacceptable toxicity would be invoked if a second DLT occurs prior to the final enrollment.\n\nSimilarly, if treatment elicits **an efficacious response in at least 8 patients**, there would be reasonable confidence that the treatment has demonstrated sufficient efficacy to warrant continued investigation, and should this be established prior to enrolment of the complete cohort, the trial may be stopped early.\n\nConfidence intervals for proportions are notoriously wide, and typically large sample sizes are sought to obtain a required level of precision for *(around)* the rate that is being estimated. If available resources are limited, it may be that a more efficient experimental could be considered.\n\n# Further thoughts\n\nFor small-sample studies, as with those based on larger samples, consider if the results can be appropriately generalised to fit with the aim of the study.\n\n**Population sampling:** Consider if there are subtle biases resulting from recruitment strategies that may impact how representative the sample is of the population being characterized.\n\n**Laboratory experiments or animal studies:** Ensure that environmental factors have been adequately controlled and appropriate randomization undertaken so that differences between groups can be confidently attributed to the condition/treatment under study. Small-sample experiments require tight control of between-individual variation to achieve maximum power and appropriate randomization must be undertaken for results to be valid.\n",
    "supporting": [
      "small_sample_analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}