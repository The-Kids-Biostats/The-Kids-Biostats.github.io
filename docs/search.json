[
  {
    "objectID": "posts/lmer-missing/index.html",
    "href": "posts/lmer-missing/index.html",
    "title": "Handling Missing Data with LMER",
    "section": "",
    "text": "As consultant statisticians, we are often approached by people who have already carried out some preliminary data analysis and who are now looking to move onto something more complex. As missing data is generally present (or rather not present!) in health-related datasets, we find this is a question that is regularly raised:\n“Can we use mixed models, since they use all available data?”\nWorking through even the basics on this topic will mean one will also have to work through challenging and varied (often cryptic) statistical nomenclature. This post is a worked example and was motivated by reading this paper. It has an interesting example of ‘missing data and mixed models’ that we thought could benefit from some figures and commentary to aid in the understanding of what is achieved."
  },
  {
    "objectID": "posts/lmer-missing/index.html#research-question",
    "href": "posts/lmer-missing/index.html#research-question",
    "title": "Handling Missing Data with LMER",
    "section": "Research question",
    "text": "Research question\nAre there differences in the rate of wage growth between males and females over time (in this dataset)?\nThat question is quite straightforward to answer here, but the motivating commentary is really around how mixed effects model can be beneficial in the presence of systematic missing (follow-up) data - with a focus on parameter estimates and their graphical interpretation.\nMissing follow-up data (lost to follow, attrition, drop out) is often seen in health research datasets. The data might be Missing At Random, it my be Missing Completely At Random, the important nuances of these are largely out of scope for this post."
  },
  {
    "objectID": "posts/lmer-missing/index.html#erroneous-basic-linear-regression-model",
    "href": "posts/lmer-missing/index.html#erroneous-basic-linear-regression-model",
    "title": "Handling Missing Data with LMER",
    "section": "Erroneous basic linear regression model",
    "text": "Erroneous basic linear regression model\nTo address the question of “are there differences between genders in the rate of wage growth”, we are going to fit a gender by time interaction term which will give us an indication of if ‘as time changes’ whether the outcome (logged wage) changes at a different rate for each gender.\n\n\nCode\nmod1 &lt;- lm(lwage ~ gender * t, data = dat)\nexport_summs(mod1, error_format = \"[{conf.low}, {conf.high}]\",\n             error_pos = \"right\", digits = 3,\n             statistics = c(N = \"nobs\"))\n\n\n\n\nModel 1\n\n(Intercept)6.340 ***[6.312, 6.368]\n\ngenderFemale-0.456 ***[-0.540, -0.372]\n\nt0.097 ***[0.091, 0.104]\n\ngenderFemale:t-0.005    [-0.024, 0.014]\n\nN4165             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nWe can see there is an effect of gender present, and an effect of time (the growth overtime we saw in the original plot), but the very small beta coefficient (relative to the scale of data we are working with) and the p-value of 0.6 are suggestive that the rate of wage growth over time does not differ significantly between genders.\nTo view this, we’re not going to use geom_smooth or stat_summary as we might do when graphing on-the-fly, rather we will use the predict() function to create data for our line of best fit.\nTo set a coding framework which we’ll use again later in the post, we’ll create a new dataset and use predictions to draw our (straight) line.\n\n\nCode\nnewdat &lt;- expand.grid(t = 1:7, gender = c(\"Male\", \"Female\")) %&gt;% \n  mutate(gender = factor(gender),\n         gender = relevel(gender, \"Male\"))\n\np_mod1 &lt;- cbind(newdat, \n                lwage = predict(mod1, newdat, interval = \"prediction\"))\n\n\n\n\nCode\ndat %&gt;% \n  ggplot(aes(t, lwage, colour = gender)) +\n  geom_point(alpha = 0.15) +\n  geom_line(aes(group = id), alpha = 0.15) + \n  facet_wrap(~ gender) +\n  geom_line(data = p_mod1, aes(y = lwage.fit), colour = \"red\", linewidth = 1) +\n  scale_colour_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  theme(legend.position=\"none\") +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\") +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9))\n\n\n\n\n\n\n\n\n\nOkay, these red fitted lines look like quite good as a ‘line of best fit’; they pass the eye test of broadly representing the trends of the data well.\nFitted with a prediction confidence interval, we see.\n\n\nCode\np_mod1 %&gt;% \n  ggplot(aes(t, lwage.fit, ymin = lwage.lwr, ymax = lwage.upr, fill = gender)) +\n  geom_ribbon(alpha = 0.5) +\n  geom_line(colour = \"black\", linewidth = 1) + \n  facet_wrap(~ gender) +\n  scale_fill_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  theme(legend.position=\"none\") +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\")  +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9)) -&gt; p1\np1\n\n\n\n\n\n\n\n\n\nThese lines look parallel - suggestive of no difference in growth rates between the genders (in line with the non-significant interaction term we saw).\nOf course, we have not adjusted for the within person correlation present in the data. The model above is inappropriate as one of the main assumptions of the model is violated - the data points are not all independent (we know there are 7 from each individual)."
  },
  {
    "objectID": "posts/lmer-missing/index.html#mixed-effects-model",
    "href": "posts/lmer-missing/index.html#mixed-effects-model",
    "title": "Handling Missing Data with LMER",
    "section": "Mixed effects model",
    "text": "Mixed effects model\nHere we run a fairly basic linear mixed effects model, the model has the same fixed effects terms as above (the interaction term we are curious about) but also includes a random effect, that is, the intercept is allowed to varied for each individual.\n\n\nCode\nmod2 &lt;- lmer(lwage ~ gender * t + (1 | id), data = dat)\nexport_summs(mod2, error_format = \"[{conf.low}, {conf.high}]\",\n             error_pos = \"right\", digits = 3,\n             statistics = c(N = \"nobs\"))\n\n\n\n\nModel 1\n\n(Intercept)6.340 ***[6.307, 6.373]\n\ngenderFemale-0.456 ***[-0.553, -0.358]\n\nt0.097 ***[0.095, 0.100]\n\ngenderFemale:t-0.005    [-0.012, 0.003]\n\nN4165             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nLets also fit the predicted values from this model.\n\n\nCode\nnewdat &lt;- expand.grid(t = 1:7, gender = c(\"Male\", \"Female\")) %&gt;% \n  mutate(gender = factor(gender),\n         gender = relevel(gender, \"Male\"),\n         id = c(rep(1, 7), rep(2, 7)))\n\nstore &lt;- simulate(mod2, seed=1, newdata=newdat, re.form=NA,\n                  allow.new.levels=T, nsim = 500)\n\np_mod2 &lt;- cbind(newdat,\n                store %&gt;% \n                  rowwise() %&gt;% \n                  mutate(fit = mean(c_across(sim_1:sim_500)),\n                         lwr = quantile(c_across(sim_1:sim_500), 0.025),\n                         upr = quantile(c_across(sim_1:sim_500), 0.975)) %&gt;% \n                  select(fit, lwr, upr))\n\n\n\n– Slight segue\nWith standard linear regression model (first used), we used predict to generate a ‘prediction interval’. With linear mixed effects models, we do no have the same function available (that will incorporate the random effects variability into the prediction interval), so rather than calculating these with a formula, we simulate! Some extra content on this can be read here or (somewhat less so) here.\nThis use of simulation is part of the reason behind the confidence intervals not being parallel."
  },
  {
    "objectID": "posts/lmer-missing/index.html#back-to-our-mixed-effects-model",
    "href": "posts/lmer-missing/index.html#back-to-our-mixed-effects-model",
    "title": "Handling Missing Data with LMER",
    "section": "Back to our Mixed effects model",
    "text": "Back to our Mixed effects model\n\n\nCode\np_mod2 %&gt;% \n  ggplot(aes(t, fit, ymin = lwr, ymax = upr, fill = gender)) +\n  geom_ribbon(alpha = 0.5) +\n  geom_line(colour = \"black\", linewidth = 1) + \n  facet_wrap(~ gender) +\n  scale_fill_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  theme(legend.position=\"none\") +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\") +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9)) -&gt; p2\np2\n\n\n\n\n\n\n\n\n\nWhen we compare the output of this model with the earlier (erroneous) model, we see two expected things.\n\n\nCode\nexport_summs(mod1, mod2,\n             error_format = \"[{conf.low}, {conf.high}]\",\n             error_pos = \"right\", digits = 3,\n             statistics = c(N = \"nobs\"),\n             model.names = c(\"Erroneous model\", \"Mixed efects model\"))\n\n\n\n\nErroneous modelMixed efects model\n\n(Intercept)6.340 ***[6.312, 6.368]6.340 ***[6.307, 6.373]\n\ngenderFemale-0.456 ***[-0.540, -0.372]-0.456 ***[-0.553, -0.358]\n\nt0.097 ***[0.091, 0.104]0.097 ***[0.095, 0.100]\n\ngenderFemale:t-0.005    [-0.024, 0.014]-0.005    [-0.012, 0.003]\n\nN4165             4165             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\n\nThe coefficients have the same value in each model as these represent the fixed effect\nThe confidence intervals for those coefficients are slightly narrower in Model 2, this is because some of the variation present [within Model 1] is explained by the random effects [present in Model 2 and not Model 1].\n\nWe can see this when we plot the predicted values side by side.\n\n\nCode\n(p1 + labs(title = \"Erroneous model\")) / (p2 + labs(title = \"Mixed effects model\"))"
  },
  {
    "objectID": "posts/lmer-missing/index.html#missing---erroneous-basic-linear-regression-model",
    "href": "posts/lmer-missing/index.html#missing---erroneous-basic-linear-regression-model",
    "title": "Handling Missing Data with LMER",
    "section": "Missing - Erroneous basic linear regression model",
    "text": "Missing - Erroneous basic linear regression model\n\n\nCode\nmod3 &lt;- lm(mlwage ~ gender * t, data = mdat)\nexport_summs(mod3, error_format = \"[{conf.low}, {conf.high}]\",\n             error_pos = \"right\", digits = 3,\n             statistics = c(N = \"nobs\"))\n\n\n\n\nModel 1\n\n(Intercept)6.379 ***[6.354, 6.405]\n\ngenderFemale-0.462 ***[-0.533, -0.390]\n\nt0.047 ***[0.040, 0.053]\n\ngenderFemale:t0.032 ***[0.016, 0.049]\n\nN3101             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nNow the model is indicating that there is a strong interaction effect for gender by time. The coefficient of the interaction term implies that (logged) wages increase at a faster rate (over time) for females than they do for males.\nLet’s visual this alongside our modified dataset.\n\n\nCode\nnewdat &lt;- expand.grid(t = 1:7, gender = c(\"Male\", \"Female\"), id = 1) %&gt;% \n  mutate(gender = factor(gender),\n         gender = relevel(gender, \"Male\"))\n\np_mod3 &lt;- cbind(newdat, \n                lwage = predict(mod3, newdat, interval = \"prediction\"))\n\n\n\n\nCode\nmdat %&gt;% \n  ggplot(aes(t, mlwage, colour = gender)) +\n  geom_ribbon(data = p_mod3, aes(t, lwage.fit, ymin = lwage.lwr, ymax = lwage.upr, fill = gender), alpha = 0.5) +\n  geom_line(data = p_mod3, aes(t, lwage.fit), colour = \"black\", linewidth = 1) + \n  geom_point(data = mdat %&gt;% filter(is_zero_following == 1),\n             aes(y = lwage, group = id), alpha = 0.1, colour = \"red\") +\n  geom_line(data = mdat %&gt;% filter(is_zero_following == 1),\n            aes(y = lwage, group = id), alpha = 0.15, colour = \"red\") + \n  geom_point(alpha = 0.15) +\n  geom_line(aes(group = id), alpha = 0.15) + \n  facet_wrap(~ gender) +\n  scale_colour_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  theme(legend.position=\"none\",\n        plot.subtitle=element_text(colour = \"red\")) +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\",\n       subtitle = \"Missing data shown in red\")  +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9)) -&gt; p4\np4\n\n\n\n\n\n\n\n\n\nWe can see the predicted line and bands (95% prediction interval) represent the (non-missing) data well, and we can see the difference in slope between genders - the observed significant interaction term."
  },
  {
    "objectID": "posts/lmer-missing/index.html#missing---mixed-effects-model",
    "href": "posts/lmer-missing/index.html#missing---mixed-effects-model",
    "title": "Handling Missing Data with LMER",
    "section": "Missing - Mixed effects model",
    "text": "Missing - Mixed effects model\n\n\nCode\nmod4 &lt;- lmer(mlwage ~ gender * t + (1 | id), data = mdat)\nexport_summs(mod4, error_format = \"[{conf.low}, {conf.high}]\",\n             error_pos = \"right\", digits = 3,\n             statistics = c(N = \"nobs\"))\n\n\n\n\nModel 1\n\n(Intercept)6.341 ***[6.311, 6.372]\n\ngenderFemale-0.457 ***[-0.546, -0.368]\n\nt0.090 ***[0.087, 0.093]\n\ngenderFemale:t0.003    [-0.004, 0.011]\n\nN3101             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nWhen we run the mixed effects model on the dataset with missing data, we (correctly) do not see a significant interaction effect.\nIn fact:\n\n\nCode\nexport_summs(mod4, mod2,\n             error_format = \"[{conf.low}, {conf.high}]\",\n             error_pos = \"right\", digits = 3,\n             model.names = c(\"ME - Missing\", \"ME - Complete\"),\n             statistics = c(N = \"nobs\"))\n\n\n\n\nME - MissingME - Complete\n\n(Intercept)6.341 ***[6.311, 6.372]6.340 ***[6.307, 6.373]\n\ngenderFemale-0.457 ***[-0.546, -0.368]-0.456 ***[-0.553, -0.358]\n\nt0.090 ***[0.087, 0.093]0.097 ***[0.095, 0.100]\n\ngenderFemale:t0.003    [-0.004, 0.011]-0.005    [-0.012, 0.003]\n\nN3101             4165             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nOur mixed effects model on the dataset with a lot of missing data (ME - Missing) generates quite similar estimates to what we know the ‘truth’ to be from the model on the complete data (ME - Complete).\nTo comparatively visualise this.\n\n\nCode\nnewdat &lt;- expand.grid(t = 1:7, gender = c(\"Male\", \"Female\")) %&gt;% \n  mutate(gender = factor(gender),\n         gender = relevel(gender, \"Male\"),\n         id = c(rep(4, 7), rep(8, 7)))\n\nstore &lt;- simulate(mod4, seed=1, newdata=newdat, re.form=NA,\n                  allow.new.levels=T, nsim = 500)\n\np_mod4 &lt;- cbind(newdat,\n                store %&gt;% \n                  rowwise() %&gt;% \n                  mutate(fit = mean(c_across(sim_1:sim_500)),\n                         lwr = quantile(c_across(sim_1:sim_500), 0.025),\n                         upr = quantile(c_across(sim_1:sim_500), 0.975)) %&gt;% \n                  select(fit, lwr, upr))\n\n\n\n\nCode\nmdat %&gt;% \n  ggplot(aes(t, mlwage, colour = gender)) +\n  geom_ribbon(data = p_mod4, aes(t, fit, ymin = lwr, ymax = upr, fill = gender), alpha = 0.5) +\n  geom_line(data = p_mod4, aes(t, fit), colour = \"black\", linewidth = 1) + \n  geom_point(data = mdat %&gt;% filter(is_zero_following == 1),\n             aes(y = lwage, group = id), alpha = 0.1, colour = \"red\") +\n  geom_line(data = mdat %&gt;% filter(is_zero_following == 1),\n            aes(y = lwage, group = id), alpha = 0.15, colour = \"red\") + \n  geom_point(alpha = 0.15) +\n  geom_line(aes(group = id), alpha = 0.15) + \n  facet_wrap(~ gender) +\n  scale_colour_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  theme(legend.position=\"none\",\n        plot.subtitle=element_text(colour = \"red\")) +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\",\n       subtitle = \"Missing data shown in red\")  +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9)) -&gt; p4\np4\n\n\n\n\n\n\n\n\n\nThese lines (predicted lines; by gender) look parallel (as they should). Notably with the Males, we see the predicted line “pulled up” in the direction of the missing data even though that data was not available to the model - this is because the model has leveraged the slope of the data it did have access to, at the individual (person) level, when converging on its estimates.\nIf the erroneous interaction effect (non-parallel lines) was not obvious in the plot separated by gender, here we see the predicted lines on the same plot for each model.\n\n\nCode\np_mod3 %&gt;% \n  ggplot(aes(t, lwage.fit, ymin = lwage.lwr, ymax = lwage.upr, fill = gender)) +\n  geom_ribbon(alpha = 0.5) +\n  geom_line(colour = \"black\", linewidth = 1) + \n  scale_fill_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\",\n       title = \"Erroneous basic linear regression\",\n       subtitle = \"Missing data present\") +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9)) -&gt; p5\n\np_mod4 %&gt;% \n  ggplot(aes(t, fit, ymin = lwr, ymax = upr, fill = gender)) +\n  geom_ribbon(alpha = 0.5) +\n  geom_line(colour = \"black\", linewidth = 1) + \n  scale_fill_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\",\n       title = \"Mixed effects regression\",\n       subtitle = \"Missing data present\")  +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9)) -&gt; p6\n\np5 + p6 + plot_layout(guides = \"collect\") & theme(legend.position='bottom')"
  },
  {
    "objectID": "posts/lmer-missing/index.html#less-aggressive-missingness",
    "href": "posts/lmer-missing/index.html#less-aggressive-missingness",
    "title": "Handling Missing Data with LMER",
    "section": "Less aggressive missingness",
    "text": "Less aggressive missingness\nWhat if we whip through the same process and comparison, in a setting that ‘less aggressively’ has drop out with increasing wage and also some additional random missingness throughout.\n\n\nCode\nmdat &lt;- dat %&gt;% \n  group_by(id) %&gt;% \n  mutate(plwage = c(0, lwage[1:6]),\n         pt = 1 / (1+exp(-7.2 + plwage))) %&gt;% # Less aggressive dropout as a function of age\n  rowwise() %&gt;% \n  mutate(pt = case_when(pt &gt; 0.5 & runif(1) &lt; 0.10 & t &gt; 2 ~ 0, # Adding an underlying random component to dropout\n                        T ~ pt)) %&gt;% \n  group_by(id) %&gt;% \n  mutate(mlwage = case_when(pt &gt; 0.5 ~ lwage,\n                            pt &lt; 0.5 ~ 0),\n         is_zero_following = ifelse(mlwage == 0, 1, 0),\n         is_zero_following = cummax(is_zero_following),\n         mlwage = ifelse(is_zero_following == 1, NA, mlwage)) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nMale, N = 2,8041\nFemale, N = 3521\n\n\n\n\nt\n\n\n\n\n\n\n    1\n528\n67\n\n\n    2\n528\n67\n\n\n    3\n478\n57\n\n\n    4\n396\n49\n\n\n    5\n338\n43\n\n\n    6\n290\n38\n\n\n    7\n246\n31\n\n\n\n1 n\n\n\n\n\n\n\n\n\n\n\n\nCode\nmdat %&gt;% \n  ggplot(aes(t, mlwage, colour = gender)) +\n  geom_point(data = mdat %&gt;% filter(is_zero_following == 1),\n             aes(y = lwage, group = id), alpha = 0.1, colour = \"red\") +\n  geom_line(data = mdat %&gt;% filter(is_zero_following == 1),\n            aes(y = lwage, group = id), alpha = 0.15, colour = \"red\") + \n  geom_point(alpha = 0.15) +\n  geom_line(aes(group = id), alpha = 0.15) + \n  facet_wrap(~ gender) +\n  scale_colour_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  theme(legend.position=\"none\") +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\")  +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9))\n\n\n\n\n\n\n\n\n\n\n\nCode\nb1_mod1 &lt;- lm(mlwage ~ gender * t, data = mdat)\nb1_mod2 &lt;- lmer(mlwage ~ gender * t + (1 | id), data = mdat)\n\nexport_summs(mod2, b1_mod1, b1_mod2,\n             error_format = \"[{conf.low}, {conf.high}]\",\n             error_pos = \"right\", digits = 3,\n             model.names = c(\"ME - Complete\", \"Basic - Missing\", \"ME - Missing\"),\n             statistics = c(N = \"nobs\"))\n\n\n\n\nME - CompleteBasic - MissingME - Missing\n\n(Intercept)6.340 ***[6.307, 6.373]6.380 ***[6.352, 6.407]6.342 ***[6.311, 6.373]\n\ngenderFemale-0.456 ***[-0.553, -0.358]-0.491 ***[-0.573, -0.410]-0.465 ***[-0.557, -0.372]\n\nt0.097 ***[0.095, 0.100]0.072 ***[0.065, 0.079]0.094 ***[0.091, 0.097]\n\ngenderFemale:t-0.005    [-0.012, 0.003]0.024 *  [0.003, 0.044]0.003    [-0.006, 0.012]\n\nN4165             3156             3156             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nWe can see, the (erroneous) basic linear regression still inappropriately suggests a significant interaction effect is present, while the mixed effects models continues to perform well (relative to the model using the complete data)."
  },
  {
    "objectID": "posts/lmer-missing/index.html#completely-random-missingness",
    "href": "posts/lmer-missing/index.html#completely-random-missingness",
    "title": "Handling Missing Data with LMER",
    "section": "Completely random missingness",
    "text": "Completely random missingness\nWhat if the dropout is completely at random?\nNote, this is dropout at random, not sporadic missingness, these are two different things.\n\n\nCode\nmdat &lt;- dat %&gt;% \n  group_by(id) %&gt;% \n  rowwise() %&gt;% \n  mutate(pt = 1,\n         pt = case_when(runif(1) &lt; 0.10 & t &gt; 2 ~ 0, # Implementing completely random dropout\n                        T ~ pt)) %&gt;% \n  group_by(id) %&gt;% \n  mutate(mlwage = case_when(pt &gt; 0.5 ~ lwage,\n                            pt &lt; 0.5 ~ 0),\n         is_zero_following = ifelse(mlwage == 0, 1, 0),\n         is_zero_following = cummax(is_zero_following),\n         mlwage = ifelse(is_zero_following == 1, NA, mlwage)) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nME - CompleteBasic - MissingME - Missing\n\n(Intercept)6.340 ***[6.307, 6.373]6.330 ***[6.300, 6.359]6.333 ***[6.301, 6.366]\n\ngenderFemale-0.456 ***[-0.553, -0.358]-0.456 ***[-0.544, -0.368]-0.456 ***[-0.553, -0.359]\n\nt0.097 ***[0.095, 0.100]0.102 ***[0.095, 0.109]0.099 ***[0.096, 0.102]\n\ngenderFemale:t-0.005    [-0.012, 0.003]-0.007    [-0.029, 0.014]-0.003    [-0.012, 0.005]\n\nN4165             3379             3379             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nThe basic regression model is no long suggesting there is a significant gender by time interaction effect, and comparatively, all three models give similar estimates."
  },
  {
    "objectID": "posts/lmer-missing/index.html#acknowledgements",
    "href": "posts/lmer-missing/index.html#acknowledgements",
    "title": "Handling Missing Data with LMER",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Elizabeth McKinnon, Zac Dempsey, and Wesley Billingham for providing feedback on and reviewing this post.\nYou can look forward to seeing posts from these other team members here in the coming weeks and months."
  },
  {
    "objectID": "posts/lmer-missing/index.html#reproducibility-information",
    "href": "posts/lmer-missing/index.html#reproducibility-information",
    "title": "Handling Missing Data with LMER",
    "section": "Reproducibility Information",
    "text": "Reproducibility Information\nTo access the .qmd (Quarto markdown) files as well as any R scripts or data that was used in this post, please visit our GitHub:\nhttps://github.com/The-Kids-Biostats/The-Kids-Biostats.github.io/tree/main/posts/lmer-missingx\nThe session information can also be seen below.\n\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Australia/Perth\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n [5] purrr_1.0.2       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n [9] ggplot2_3.5.1     tidyverse_2.0.0   patchwork_1.2.0   parameters_0.21.7\n[13] jtools_2.2.2      gtsummary_1.7.2   AER_1.2-12        survival_3.6-4   \n[17] sandwich_3.1-0    lmtest_0.9-40     zoo_1.8-12        car_3.1-2        \n[21] carData_3.0-5     merTools_0.6.2    arm_1.14-4        MASS_7.3-60.0.1  \n[25] lme4_1.1-35.3     Matrix_1.6-5     \n\nloaded via a namespace (and not attached):\n [1] rlang_1.1.3          magrittr_2.0.3       multcomp_1.4-25     \n [4] furrr_0.3.1          compiler_4.3.3       vctrs_0.6.5         \n [7] pkgconfig_2.0.3      crayon_1.5.2         fastmap_1.2.0       \n[10] backports_1.5.0      labeling_0.4.3       pander_0.6.5        \n[13] utf8_1.2.4           promises_1.3.0       rmarkdown_2.27      \n[16] markdown_1.12        tzdb_0.4.0           nloptr_2.0.3        \n[19] xfun_0.44            jsonlite_1.8.8       later_1.3.2         \n[22] broom_1.0.6          parallel_4.3.3       R6_2.5.1            \n[25] stringi_1.8.4        parallelly_1.37.1    boot_1.3-30         \n[28] numDeriv_2016.8-1.1  estimability_1.5.1   assertthat_0.2.1    \n[31] Rcpp_1.0.12          iterators_1.0.14     knitr_1.45          \n[34] httpuv_1.6.15        splines_4.3.3        timechange_0.3.0    \n[37] tidyselect_1.2.1     rstudioapi_0.16.0    abind_1.4-5         \n[40] yaml_2.3.8           codetools_0.2-20     listenv_0.9.1       \n[43] lmerTest_3.1-3       lattice_0.22-6       shiny_1.8.1.1       \n[46] withr_3.0.0          bayestestR_0.13.2    coda_0.19-4.1       \n[49] evaluate_0.23        future_1.33.2        huxtable_5.5.6      \n[52] xml2_1.3.6           pillar_1.9.0         foreach_1.5.2       \n[55] insight_0.19.11      generics_0.1.3       hms_1.1.3           \n[58] munsell_0.5.1        commonmark_1.9.1     scales_1.3.0        \n[61] minqa_1.2.7          globals_0.16.3       xtable_1.8-4        \n[64] glue_1.7.0           emmeans_1.10.2       tools_4.3.3         \n[67] mvtnorm_1.2-5        grid_4.3.3           datawizard_0.10.0   \n[70] colorspace_2.1-0     nlme_3.1-164         Formula_1.2-5       \n[73] cli_3.6.2            fansi_1.0.6          viridisLite_0.4.2   \n[76] broom.helpers_1.15.0 gt_0.10.1            gtable_0.3.5        \n[79] broom.mixed_0.2.9.5  sass_0.4.9           digest_0.6.35       \n[82] TH.data_1.1-2        farver_2.1.2         htmlwidgets_1.6.4   \n[85] htmltools_0.5.8.1    lifecycle_1.0.4      mime_0.12           \n[88] blme_1.0-5"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Kids Biostats",
    "section": "",
    "text": "Parallel Computing in R\n\n\n\n\n\n\nR\n\n\nParallel\n\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\nZac Dempsey\n\n\n\n\n\n\n\n\n\n\n\n\nHandling Missing Data with LMER\n\n\n\n\n\n\nMissing Data\n\n\nMixed Models\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 7, 2024\n\n\nDr Matthew Cooper\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "We are members of the biostatistics team at Telethon Kids Institute in Perth, Western Australia. Our work involves providing statistical consultation and collaboration to child health researchers at all stages of the scientific pipeline, from design to analysis to publication.\n\nPrimarily R users, we encounter particular coding, methodology, or analysis challenges - often that have recurring or related themes. The idea of this site is to be a centralised repository of our solutions to or narration of these recurrent challenges, for both our reference and yours!\nWe hope you’ll find something useful here, and are always open to feedback and conversation on these topics."
  },
  {
    "objectID": "posts/parallel/index.html",
    "href": "posts/parallel/index.html",
    "title": "Parallel Computing in R",
    "section": "",
    "text": "While most of the datasets we work with are of a manageable size here at the Telethon Kids Institute, occasionally we are presented with data containing millions of rows and/or thousands of columns. Performing almost any operation on these large datasets can take hours to execute – a real test of patience for exploratory work!\nParallel computing is one relatively simple way to reduce computation time on large datasets – leveraging our computer’s full processing capabilities by simultaneously distributing tasks “in parallel” across multiple processors (without too much extra code). This is particularly useful for loop-based computations, where multiple parameters or conditions must be swept over whether that as part of variable creation, model tuning/execution, or some other task.\nThat being said, while parallel computing is not a “magical general solution” to all computationally intensive tasks – it is worth investing some time to understand how it works, and become familiar with the packages and/or functions used within a parallel framework. There are still circumstances where the handy purrr::map function (or the like) can be just as, or more, efficient than a parallel distribution."
  },
  {
    "objectID": "posts/parallel/index.html#for-loop",
    "href": "posts/parallel/index.html#for-loop",
    "title": "Parallel Computing in R",
    "section": "For-loop",
    "text": "For-loop\n\nLet’s begin by using a traditional for-loop. For each bootstrap sample, we:\n\nInitialise a bootstrap sample, ind.\nRun our linear regression on the bootstrap sample, results\nExtract the coefficients from results, and append this to an overall coefficient matrix bootstrap_coefs.\n\nSubsequently, we calculate a 95% confidence interval for each of our parameter estimates by taking the 2.5th and 97.5th percentiles from the bootstrap distribution and calling this bootstrap_cis.\n\n\n\nCode\nstart &lt;- proc.time() # Start our timer!\n\n# Initialise a matrix to store the coefficients from each bootstrap sample\nbootstrap_coefs &lt;- matrix(NA, nrow = trials, ncol = 4)\ncolnames(bootstrap_coefs) &lt;- names(coef(lm(mpg ~ hp + wt + am, data = mtcars)))\n\nfor (i in 1:trials){\n  \n  # Take bootstrap sample\n  ind &lt;- mtcars[sample(nrow(mtcars), \n                       replace = TRUE),\n                ]\n  \n  # Construct linear regression\n  result &lt;- lm(mpg ~ hp + wt + as_factor(am), \n               data = ind)\n  \n  # Extract coefficients and store to `bootstrap_coefs`\n  bootstrap_coefs[i, ] &lt;- coef(result)\n}\n\n# Calculate the 2.5th and 97.5th percentile for each variable's coefficient estimates from the bootstrap distribution.\nbootstrap_cis &lt;- apply(bootstrap_coefs, \n                       2, \n                       function(coefs){quantile(coefs, probs = c(0.025, 0.975))})\n\nend &lt;- proc.time() # End our timer!\ntime1 &lt;- end-start\n\n\n\nLet’s visualise the first few lines of the bootstrapped results\n\n\nCode\nhead(bootstrap_coefs)\n\n\n     (Intercept)          hp        wt         am\n[1,]    34.04596 -0.02741585 -3.158154  0.6330600\n[2,]    31.46670 -0.03097903 -2.375727  5.8142235\n[3,]    35.98084 -0.02535775 -3.763464 -0.2485866\n[4,]    33.47330 -0.04410244 -2.343210  2.6890689\n[5,]    32.21798 -0.04138935 -2.222471  1.2289610\n[6,]    32.78747 -0.02758182 -2.989311  1.1744731\n\n\nand associated 95% confidence interval\n\n\nCode\nbootstrap_cis\n\n\n      (Intercept)          hp        wt         am\n2.5%     29.07843 -0.05539954 -5.134682 -0.7627477\n97.5%    40.74463 -0.02161861 -1.057830  4.9428501\n\n\n\nThis had the following run-time (seconds):\n\n\nCode\nprint(time1)\n\n\n   user  system elapsed \n   9.80    0.06    9.99"
  },
  {
    "objectID": "posts/parallel/index.html#do-loop-not-parallel",
    "href": "posts/parallel/index.html#do-loop-not-parallel",
    "title": "Parallel Computing in R",
    "section": "%do% loop (not parallel)",
    "text": "%do% loop (not parallel)\nAs an alternative, let’s also use the %do% operator from the foreach package. Similar to a for-loop, each bootstrap sample is executed sequentially.\n\n\nCode\nstart &lt;- proc.time()\n\nbootstrap_coefs &lt;- foreach::foreach(i = 1:trials, .combine = rbind) %do% {\n  ind &lt;- mtcars[sample(nrow(mtcars), replace = TRUE), ]\n  result &lt;- lm(mpg ~ hp + wt + am, data = ind)\n  coef(result)\n}\n\n# Calculate the 2.5th and 97.5th percentile for each variable's coefficient estimates from the bootstrap distribution.\nbootstrap_cis &lt;- apply(bootstrap_coefs, \n                       2, \n                       function(coefs) {quantile(coefs, probs = c(0.025, 0.975))})\n\nend &lt;- proc.time()\ntime2 &lt;- end-start\n\n\n\nSimilarly, let’s visualise the first few lines of the bootstrapped results\n\n\nCode\nhead(bootstrap_coefs)\n\n\n         (Intercept)          hp        wt       am\nresult.1    32.28476 -0.02807558 -2.989010 2.404865\nresult.2    38.19523 -0.02740136 -4.576854 1.082726\nresult.3    32.87519 -0.06693216 -1.172027 3.879952\nresult.4    29.54068 -0.03806135 -1.468158 1.933494\nresult.5    31.54392 -0.02793433 -2.756060 1.347816\nresult.6    34.42623 -0.03900395 -2.865725 1.167583\n\n\nand associated 95% confidence interval\n\n\nCode\nbootstrap_cis\n\n\n      (Intercept)          hp        wt         am\n2.5%     29.03455 -0.05615444 -5.286909 -0.8228096\n97.5%    41.09610 -0.02124169 -1.055386  4.9293286\n\n\n\nThis had the following run-time (seconds):\n\n\nCode\nprint(time2)\n\n\n   user  system elapsed \n   9.30    0.12    9.99"
  },
  {
    "objectID": "posts/parallel/index.html#dopar-parallelisation",
    "href": "posts/parallel/index.html#dopar-parallelisation",
    "title": "Parallel Computing in R",
    "section": "%dopar% parallelisation",
    "text": "%dopar% parallelisation\nNow, let’s run this in parallel across 6 cores. The %dopar% operator defines the for-loop in the parallel environment.\n\n\nCode\ndoParallel::registerDoParallel(cores = 6) # Initialise parallel cluster\n\nstart &lt;- proc.time()\nbootstrap_coefs &lt;- foreach(i = 1:trials, .combine = rbind, .packages = 'stats') %dopar% {\n  ind &lt;- mtcars[sample(nrow(mtcars), replace = TRUE), ]\n  result &lt;- lm(mpg ~ hp + wt + am, data = ind)\n  coef(result)\n}\n\n# Calculate the 2.5th and 97.5th percentile for each variable's coefficient estimates from the bootstrap distribution.\nbootstrap_cis &lt;- apply(bootstrap_coefs, \n                       2, \n                       function(coefs) {quantile(coefs, probs = c(0.025, 0.975))})\n\nend &lt;- proc.time()\ntime3 &lt;- end-start\n\ndoParallel::stopImplicitCluster() # De-register parallel cluster\n\n\nAs expected, the output of the bootstrapped coefficient distribution are identical before\n\n\nCode\nhead(bootstrap_coefs)\n\n\n         (Intercept)          hp        wt         am\nresult.1    29.77207 -0.05288730 -1.058038  3.9123258\nresult.2    33.58996 -0.04755404 -2.197781  2.3569617\nresult.3    35.98690 -0.03708263 -3.266833  1.4526964\nresult.4    42.25028 -0.01822239 -5.825143 -2.8374742\nresult.5    33.29648 -0.04161578 -2.510220  2.7786428\nresult.6    35.12179 -0.03283584 -3.369380  0.8513454\n\n\nas are the associated 95% confidence intervals.\n\n\nCode\nbootstrap_cis\n\n\n      (Intercept)          hp        wt         am\n2.5%     29.00207 -0.05509635 -5.216696 -0.7757867\n97.5%    40.88382 -0.02141655 -1.078642  4.8896085\n\n\nLastly, this had the following run-time (seconds)\n\n\nCode\nprint(time3)\n\n\n   user  system elapsed \n   2.71    0.40    5.16"
  },
  {
    "objectID": "posts/parallel/index.html#discussion",
    "href": "posts/parallel/index.html#discussion",
    "title": "Parallel Computing in R",
    "section": "Discussion",
    "text": "Discussion\nImmediately, the syntax of the alternative for-loop structures are more readable and easier to construct than the traditional for-loop. Because the foreach::foreach function easily combines output in a list, we need not define an empty matrix to append output to.\nComputation time in the parallel environment is significantly faster — approximately 48% faster than the traditional for-loop! Across multiple analyses and data sets, these time savings certainly add up!"
  },
  {
    "objectID": "posts/parallel/index.html#data",
    "href": "posts/parallel/index.html#data",
    "title": "Parallel Computing in R",
    "section": "Data",
    "text": "Data\nFirst, let’s simulate our data set and set some parameters:\n\n10,000 observations.\nIndependent variables temperature, precipitation and elevation sampled from a random normal distribution and vegetation type (categorical factor) randomly prescribed.\nSpecies presence (dichotomous) outcome variable is randomly prescribed.\n\n\n\nCode\nn &lt;- 10000 # Sample size\n\ndata &lt;- data.frame(temperature = rnorm(n, \n                                       mean = 15, \n                                       sd   = 40),\n                   precipitation = rnorm(n, \n                                         mean = 1000, \n                                         sd   = 300),\n                   elevation = rnorm(n, \n                                     mean = 500, \n                                     sd   = 200),\n                   vegetation_type = as_factor(sample(c(\"forest\", \n                                                        \"grassland\", \n                                                        \"wetland\", \n                                                        \"desert\"), \n                                                      n, \n                                                      replace = T)),\n                   species_presence = as_factor(sample(c(\"present\", \n                                                         \"absent\"), \n                                                       n, \n                                                       replace = T)))\n\n\nLet’s assign 70% of the data to our training set, and the remaining 30% to test data set and initialise a random forest model with 1000 trees.\n\n\nCode\ntrain_index &lt;- sample(1:n, 0.7*n)\n\ntrain_data &lt;- data[train_index, ]\ntest_data &lt;- data[-train_index, ]\n\nnum_trees &lt;- 1000\n\n\n\nInstead of running one random forest model comprising 1000 trees, let’s combine the results of 4 smaller random forest models models each comprising 250 trees. By doing this, we can return more reliable and robust output (smaller random forest models are less prone to overfitting) and better manage working memory (smaller models require less memory to train and store)."
  },
  {
    "objectID": "posts/parallel/index.html#for-loop-1",
    "href": "posts/parallel/index.html#for-loop-1",
    "title": "Parallel Computing in R",
    "section": "For-loop",
    "text": "For-loop\n\n\nCode\nstart &lt;- proc.time()\nrf &lt;- list()\nfor (i in 1:(num_trees/250)){\n\n  rf[[i]] &lt;- randomForest::randomForest(species_presence ~ ., \n                                        data = train_data, \n                                        ntree = num_trees/4)\n}\n\ncombined_output &lt;- do.call(randomForest::combine, rf)\n\npredictions &lt;- predict(combined_output, test_data %&gt;% select(-species_presence))\n\nend &lt;- proc.time()\ntime1 &lt;- end - start\n\n\nLet’s print the confusion matrix of this output. Because this data was relatively simply simulated, we don’t expect the predictive power to be too great.\n\n\nCode\ntable(predictions, test_data$species_presence)\n\n\n           \npredictions absent present\n    absent     705     777\n    present    781     737\n\n\nThis has the following run-time (seconds):\n\n\nCode\nprint(time1)\n\n\n   user  system elapsed \n   6.14    0.47    6.69"
  },
  {
    "objectID": "posts/parallel/index.html#do-loop-not-parallel-1",
    "href": "posts/parallel/index.html#do-loop-not-parallel-1",
    "title": "Parallel Computing in R",
    "section": "%do% loop (not parallel)",
    "text": "%do% loop (not parallel)\nSimilar to the traditional for-loop, we can sequentially execute this code using the %do% operator.\n\n\nCode\nstart &lt;- proc.time()\nrf &lt;- foreach::foreach(ntree = rep(num_trees/4, 4), \n                       .combine = randomForest::combine, \n                       .packages = 'randomForest') %do%\n  randomForest::randomForest(species_presence ~ ., \n                             data = train_data,\n                             ntree = ntree)\n\npredictions &lt;- predict(rf, test_data %&gt;% select(-species_presence))\n\nend &lt;- proc.time()\ntime2 &lt;- end - start\n\n\nLet’s print the confusion matrix of this output. Because this data was relatively simply simulated, we don’t expect the predictive power to be too great.\n\n\nCode\ntable(predictions, test_data$species_presence)\n\n\n           \npredictions absent present\n    absent     706     778\n    present    780     736\n\n\nThis has the following run-time (seconds):\n\n\nCode\nprint(time2)\n\n\n   user  system elapsed \n   5.76    0.64    6.44"
  },
  {
    "objectID": "posts/parallel/index.html#dopar-parallelisation-1",
    "href": "posts/parallel/index.html#dopar-parallelisation-1",
    "title": "Parallel Computing in R",
    "section": "%dopar% parallelisation",
    "text": "%dopar% parallelisation\nFor simplicity, let’s allocate 4 cores to the computation and imagine that one core is responsible for processing one of the four random forest models simultaneously.\n\n\nCode\ndoParallel::registerDoParallel(cores = 4)\n\nstart &lt;- proc.time()\nrf &lt;- foreach::foreach(ntree = rep(num_trees/4, 4), \n                       .combine = randomForest::combine, \n                       .packages = 'randomForest') %dopar%\n  randomForest::randomForest(species_presence ~ ., \n                             data = train_data,\n                             ntree = ntree)\n\npredictions &lt;- predict(rf, test_data %&gt;% select(-species_presence))\n\nend &lt;- proc.time()\ndoParallel::stopImplicitCluster()\n\ntime3 &lt;- end-start\n\n\nLet’s print the confusion matrix of this output. Because this data was relatively simply simulated, we don’t expect the predictive power to be too great.\n\n\nCode\ntable(predictions, test_data$species_presence)\n\n\n           \npredictions absent present\n    absent     705     774\n    present    781     740\n\n\nThis now has the following run-time (seconds):\n\n\nCode\nprint(time3)\n\n\n   user  system elapsed \n   1.20    0.55    4.14"
  },
  {
    "objectID": "posts/parallel/index.html#discussion-1",
    "href": "posts/parallel/index.html#discussion-1",
    "title": "Parallel Computing in R",
    "section": "Discussion",
    "text": "Discussion\nAgain, using easily adaptable and readable syntax, we leverage a parallel environment to significantly lessen the computation time of our large model. Relative to a standard for-loop, the parallelised computation is approximately 38% faster."
  },
  {
    "objectID": "posts/parallel/index.html#data-1",
    "href": "posts/parallel/index.html#data-1",
    "title": "Parallel Computing in R",
    "section": "Data",
    "text": "Data\n\nLet’s use nycflights13::flights — a set of over 300,000 flight records that departed from all NYC airports in 2013.\nWe would like to explore how arrival delay (as a continuous and dichotomous (delayed = 1, not delayed = 0) outcome variable) may be influenced by a set of independent variables. We would like to stratify this by month.\n\n\n\nCode\nflights &lt;- nycflights13::flights\nflights &lt;- flights %&gt;%\n  select(year, day, dep_delay, arr_delay, air_time, distance) %&gt;%\n  mutate(arr_delay_bin = as.factor(case_when(arr_delay &gt;  15 ~ 1, TRUE ~ 0)))\n\nflights\n\n\n# A tibble: 336,776 × 7\n    year   day dep_delay arr_delay air_time distance arr_delay_bin\n   &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;        \n 1  2013     1         2        11      227     1400 0            \n 2  2013     1         4        20      227     1416 1            \n 3  2013     1         2        33      160     1089 1            \n 4  2013     1        -1       -18      183     1576 0            \n 5  2013     1        -6       -25      116      762 0            \n 6  2013     1        -4        12      150      719 0            \n 7  2013     1        -5        19      158     1065 1            \n 8  2013     1        -3       -14       53      229 0            \n 9  2013     1        -3        -8      140      944 0            \n10  2013     1        -2         8      138      733 0            \n# ℹ 336,766 more rows\n\n\n\nWe would like to specify a set of models which predict overall flight delay, as both continuous (arrival delay time) and dichotomous (delayed yes/no) outcomes.\n\nOutcome variables\n\nArrival delay (continuous)\nArrival delayed (dichotomous)\n\nIndependent variables\n\nFlight distance (distance)\nAir time (air_time)\nDeparture delay (dep_delay)\n\n\n\n\nCode\nindep_vars &lt;- c(\"distance\", \"air_time\", \"dep_delay\")\noutcome_vars &lt;- c(\"arr_delay\", \"arr_delay_bin\")\n\n\nFor each outcome variable, we run a model. If the outcome variable is continuous, we run a simple linear model; otherwise we run a basic logistic regression."
  },
  {
    "objectID": "posts/parallel/index.html#for-loop-2",
    "href": "posts/parallel/index.html#for-loop-2",
    "title": "Parallel Computing in R",
    "section": "For-loop",
    "text": "For-loop\n\n\nCode\nstart &lt;- proc.time()\nmodels &lt;- list() # To store our model output\n\nfor (i in outcome_vars){\n  if (i == \"arr_delay\"){\n    model &lt;- lm(as.formula(paste(i,\"~\",paste(indep_vars, collapse = \"+\"))),\n                data = flights)\n    \n  } else if (i == \"arr_delay_bin\"){\n    model &lt;- glm(as.formula(paste(i,\"~\",paste(indep_vars, collapse = \"+\"))),\n                 family = binomial,\n                 data = flights)\n  }\n  models[[i]] &lt;- summary(model)\n}\n\nend &lt;- proc.time()\ntime1 &lt;- end-start\n\n\nThis returns a list with the model output summary for each of our models.\nThe for-loop has the following run-time (seconds):\n\n\nCode\nprint(time1)\n\n\n   user  system elapsed \n   1.27    0.30    1.61"
  },
  {
    "objectID": "posts/parallel/index.html#purrrmap",
    "href": "posts/parallel/index.html#purrrmap",
    "title": "Parallel Computing in R",
    "section": "purrr:map",
    "text": "purrr:map\nMap functions apply a function to each element of a list/vector and return an object. In cases relying on multiple computations across different values, they often come in handy.\n\n\nCode\nstart &lt;- proc.time()\n\n\nmodels &lt;- map(outcome_vars, \n      ~ if (.x == \"arr_delay\"){\n        lm(as.formula(paste(.x, \"~\", \n                            paste(indep_vars, collapse = \" + \"))),\n           data = flights) %&gt;%\n          summary()\n        \n        } else if (.x == \"arr_delay_bin\"){\n          glm(as.formula(paste(.x, \"~\",\n                               paste(indep_vars, collapse = \" + \"))),\n              family = binomial,\n              data = flights) %&gt;%\n            summary()\n        }\n      )\nnames(models) &lt;- outcome_vars\n\nend &lt;- proc.time()\ntime2 &lt;- end-start\n\n\nThis has the following run-time (seconds):\n\n\nCode\nprint(time2)\n\n\n   user  system elapsed \n   1.22    0.17    1.39"
  },
  {
    "objectID": "posts/parallel/index.html#furrrfuture_map",
    "href": "posts/parallel/index.html#furrrfuture_map",
    "title": "Parallel Computing in R",
    "section": "furrr::future_map",
    "text": "furrr::future_map\nThere is also a parallel implementation of the purrr::map function, offered by the furrr package. The syntax is (nicely) identical to above, but importantly relies on specifying a parallel (multisession) “plan” ahead of executing the code (similar to what we did in Example 1 and 2).\n\n\nCode\nlibrary(furrr)\n\nplan(multisession, workers = 6) # Initialise parallel environment using furrr\n\nstart &lt;- proc.time()\nmodels &lt;- furrr::future_map(outcome_vars, \n      ~ if (.x == \"arr_delay\"){\n        lm(as.formula(paste(.x, \"~\", \n                            paste(indep_vars, collapse = \" + \"))),\n           data = flights) %&gt;%\n          summary()\n        \n        } else if (.x == \"arr_delay_bin\"){\n          glm(as.formula(paste(.x, \"~\",\n                               paste(indep_vars, collapse = \" + \"))),\n              family = binomial,\n              data = flights) %&gt;%\n            summary()\n        }\n      )\nnames(models) &lt;- outcome_vars\n\nend &lt;- proc.time()\ntime3 &lt;- end-start\n\nplan(sequential) # Revert to sequential processing\n\n\nThis has the following run-time (seconds):\n\n\nCode\nprint(time3)\n\n\n   user  system elapsed \n   0.09    0.05    2.53"
  },
  {
    "objectID": "posts/parallel/index.html#dopar-parallelisation-2",
    "href": "posts/parallel/index.html#dopar-parallelisation-2",
    "title": "Parallel Computing in R",
    "section": "%dopar% parallelisation",
    "text": "%dopar% parallelisation\nAlternatively, as done earlier, we can turn our non-parallel %do% code into parallel %dopar% code.\n\nWe use the %:% operator from the foreach package to nest a for-loop within a parallel environment.\nThe syntax does not differ too dramatically.\n\n\n\nCode\ndoParallel::registerDoParallel(cores = 6)\n\nstart &lt;- proc.time()\nmodels &lt;- foreach(j = outcome_vars, .combine = \"list\") %dopar% {\n  if (j == \"arr_delay\"){\n    model &lt;- lm(as.formula(paste(j,\"~\",paste(indep_vars, collapse = \"+\"))),\n                data = flights)\n  } else if (j == \"arr_delay_bin\"){\n    model &lt;- glm(as.formula(paste(j,\"~\",paste(indep_vars, collapse = \"+\"))),\n                 family = binomial,\n                 data = flights)\n  }\n}\nnames(models) &lt;- outcome_vars\n\nend &lt;- proc.time()\n\ntime4 &lt;- end - start\ndoParallel::stopImplicitCluster()\n\n\nThis has the following run-time (seconds):\n\n\nCode\nprint(time4)\n\n\n   user  system elapsed \n   1.42    0.77    5.22"
  },
  {
    "objectID": "posts/parallel/index.html#discussion-2",
    "href": "posts/parallel/index.html#discussion-2",
    "title": "Parallel Computing in R",
    "section": "Discussion",
    "text": "Discussion\nWe now see that the parallel processing of these tasks takes far longer – about 69% so! The underlying set of operations — running a series of linear models — are already small and relatively fast, so the overhead of managing the task (splitting, computing and combining results) in a parallel environment far exceeds what what can easily be spun up using a for-loop (or purrr::map)."
  },
  {
    "objectID": "posts/parallel/index.html#acknowledgements",
    "href": "posts/parallel/index.html#acknowledgements",
    "title": "Parallel Computing in R",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Wesley Billingham, Matt Cooper, and Elizabeth McKinnon for providing feedback on and reviewing this post.\nYou can look forward to seeing posts from these other team members here in the coming weeks and months."
  },
  {
    "objectID": "posts/parallel/index.html#reproducibility-information",
    "href": "posts/parallel/index.html#reproducibility-information",
    "title": "Parallel Computing in R",
    "section": "Reproducibility Information",
    "text": "Reproducibility Information\nTo access the .qmd (Quarto markdown) files as well as any R scripts or data that was used in this post, please visit our GitHub:\nhttps://github.com/The-Kids-Biostats/The-Kids-Biostats.github.io/tree/main/posts/parallel\nThe session information can also be seen below.\n\n\nCode\nsessionInfo()\n\n\nR version 4.3.2 (2023-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_Australia.utf8  LC_CTYPE=English_Australia.utf8   \n[3] LC_MONETARY=English_Australia.utf8 LC_NUMERIC=C                      \n[5] LC_TIME=English_Australia.utf8    \n\ntime zone: Australia/Perth\ntzcode source: internal\n\nattached base packages:\n[1] parallel  stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] furrr_0.3.1          future_1.33.2        randomForest_4.7-1.1\n [4] doParallel_1.0.17    iterators_1.0.14     foreach_1.5.2       \n [7] lubridate_1.9.3      forcats_1.0.0        stringr_1.5.1       \n[10] dplyr_1.1.4          purrr_1.0.2          readr_2.1.5         \n[13] tidyr_1.3.1          tibble_3.2.1         ggplot2_3.5.1       \n[16] tidyverse_2.0.0     \n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.4         generics_0.1.3     stringi_1.8.3      listenv_0.9.1     \n [5] hms_1.1.3          digest_0.6.35      magrittr_2.0.3     evaluate_0.23     \n [9] grid_4.3.2         timechange_0.3.0   fastmap_1.1.1      jsonlite_1.8.8    \n[13] fansi_1.0.6        scales_1.3.0       codetools_0.2-20   cli_3.6.2         \n[17] rlang_1.1.3        parallelly_1.37.1  munsell_0.5.1      withr_3.0.0       \n[21] yaml_2.3.8         tools_4.3.2        tzdb_0.4.0         colorspace_2.1-0  \n[25] globals_0.16.3     vctrs_0.6.5        R6_2.5.1           lifecycle_1.0.4   \n[29] htmlwidgets_1.6.4  pkgconfig_2.0.3    pillar_1.9.0       gtable_0.3.5      \n[33] glue_1.7.0         xfun_0.43          tidyselect_1.2.1   rstudioapi_0.16.0 \n[37] knitr_1.46         htmltools_0.5.8.1  rmarkdown_2.26     nycflights13_1.0.2\n[41] compiler_4.3.2"
  }
]