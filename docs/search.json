[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "We are members of the biostatistics team at Telethon Kids Institute in Perth, Western Australia. Our work involves providing statistical consultation and collaboration to child health researchers at all stages of the scientific pipeline, from design to analysis to publication.\n\nPrimarily R users, we encounter particular coding, methodology, or analysis challenges - often that have recurring or related themes. The idea of this site is to be a centralised repository of our solutions to or narration of these recurrent challenges, for both our reference and yours!\nWe hope you’ll find something useful here, and are always open to feedback and conversation on these topics."
  },
  {
    "objectID": "posts/2024-11-19_code_v_functions/code_v_functions.html",
    "href": "posts/2024-11-19_code_v_functions/code_v_functions.html",
    "title": "Sharing - Code v Functions",
    "section": "",
    "text": "(Lengthy) readable code. Efficient functions comprised of the minimum number of characters necessary. An age old battle in programming.\nWay back when there were only two users on Bluesky, I linked to one of our earlier blog posts where we presented a nice way to visualise pre-post ordinal Likert data.\n\n\n\nThe figure\n\n\nThe only other user on Bluesky at the time commented:\n\n\n\nThe comment\n\n\nAll of the points raised were correct. It was a good post, but there was a lot of repetition in the code. Which got me thinking about the age old battle in programming…"
  },
  {
    "objectID": "posts/2024-11-19_code_v_functions/code_v_functions.html#reproducibility-information",
    "href": "posts/2024-11-19_code_v_functions/code_v_functions.html#reproducibility-information",
    "title": "Sharing - Code v Functions",
    "section": "Reproducibility Information",
    "text": "Reproducibility Information\nTo access the .qmd (Quarto markdown) files as well as any R scripts or data that was used in this post, please visit our GitHub:\nhttps://github.com/The-Kids-Biostats/The-Kids-Biostats.github.io/tree/main/posts/\nThe session information can also be seen below.\n\n\nCode\nsessionInfo()\n\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Australia/Perth\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.3.3    fastmap_1.2.0     cli_3.6.3        \n [5] tools_4.3.3       htmltools_0.5.8.1 rstudioapi_0.17.1 yaml_2.3.10      \n [9] rmarkdown_2.29    knitr_1.49        jsonlite_1.8.9    xfun_0.49        \n[13] digest_0.6.37     rlang_1.1.4       evaluate_1.0.1"
  },
  {
    "objectID": "posts/2025-10-10_AIHW_api/aihw_api_guess_plot.html",
    "href": "posts/2025-10-10_AIHW_api/aihw_api_guess_plot.html",
    "title": "Accessing national health data via API",
    "section": "",
    "text": "Did you know, the Australian Institute of Health and Welfare (AIHW) is a treasure trove of available health data?\nThe AIHW works with a range of stakeholders from across Australia to carry out high-quality research covering a broad range of health domains; they also produce many reports each year contextualising the state of the nation’s health. While the data that supports these reports is often available for download from their website (.xslx, .csv), a range of the data can can also by accessed via Application Programming Interface (API). The MyHospitals API opens the door to automated, real-time access, and offers much greater flexibility in how you explore and analyse the data.\nIn this post, we’ll demonstrate—as an example—how to access emergency department presentation (ED) data for major hospitals here in Western Australia, process it in R, and create an insightful visualisation.\n\n\n\n\n\n\nFigure 1: All 4 “Guess The Plots” Together\n\n\n\n\n\n\n\n\n\nFigure 2: The final plot"
  },
  {
    "objectID": "posts/2025-10-10_AIHW_api/aihw_api_guess_plot.html#where-did-the-myh-ed-code-come-from",
    "href": "posts/2025-10-10_AIHW_api/aihw_api_guess_plot.html#where-did-the-myh-ed-code-come-from",
    "title": "Accessing national health data via API",
    "section": "Where did the \"MYH-ED\" code come from?",
    "text": "Where did the \"MYH-ED\" code come from?\nGreat question!\nThere are many different variables that can be sourced. You can find a descriptive list online, but we can also use the API (and some of those packages mentioned earlier) to import the list of codes directly into R!\n\n\nCode\nlibrary(httr)\nlibrary(jsonlite)\n\nmeasures &lt;- GET(\"https://myhospitalsapi.aihw.gov.au/api/v1/Measures\")\nmeasures &lt;- content(measures, as = \"text\")\nmeasures &lt;- fromJSON(measures, flatten = TRUE)\nmeasures$result |&gt; \n  select(measure_code, measure_name) |&gt; \n  head(10) |&gt; \n  thekids_table(colour = \"AzureBlue\")\n\n\nmeasure_codemeasure_nameMYH0001Number of surgeries for malignant cancerMYH0002Median waiting time for surgery for malignant cancerMYH0003Percentage of people who received surgery for malignant cancer within 30 daysMYH0004Percentage of people who received surgery for malignant cancer within 45 daysMYH0005Percentage of patients who depart the emergency department within four hours of arrivalMYH0006Number of elective surgeriesMYH0007Percentage of patients who waited longer than 365 days for elective surgeryMYH0008Percentage of patients who received their surgery within clinically recommended timesMYH0009Median waiting time for elective surgeryMYH0010Percentage of patients who commenced treatment within the recommended time\n\n\nFrom here, the world (of AIHW MyHospital data) is your oyster!"
  },
  {
    "objectID": "posts/2025-10-10_AIHW_api/aihw_api_guess_plot.html#but-first-some-notes",
    "href": "posts/2025-10-10_AIHW_api/aihw_api_guess_plot.html#but-first-some-notes",
    "title": "Accessing national health data via API",
    "section": "But first, some notes …",
    "text": "But first, some notes …\nFor something a little different, we inset a map of Perth into these plots with a marker for the point of interest. This involves using the ggmaps package and registering here at Stadia Maps for a free account to receive a token to allow us to access the map tile images.\nWe obviously can’t share our token; the full .qmd for this post won’t run until you update the code with your own token. For that reason, we insert .pngs of the figures that were saved earlier.\nNow we will create a base map of the Perth area and define a function to generate small inset maps for each hospital. Each inset will mark a hospital’s location with a red star on the map. These inset maps can be added to other plots to visually show the exact location of each hospital, while keeping the map clean and free of axes or labels.\n\nlibrary(ggmap)\nlibrary(patchwork)\n\nregister_stadiamaps(\"YOUR-TOKEN-GOES-HERE\", write = FALSE)\n\n# Define the Perth area\nperth_bbox &lt;- c(left = 115.65, bottom = -32.1, right = 115.95, top = -31.70)\n\n# Source the map tiles for the area we need\nperth_map &lt;- get_stadiamap(\n  bbox = as.matrix(perth_bbox),\n  maptype = \"stamen_terrain_background\"\n)\n\n# Form a grob object of the map that we will reuse in each plot\ninset_map &lt;- function(latitude, longitude) {\n  latitude &lt;- as.numeric(latitude)\n  longitude &lt;- as.numeric(longitude)\n  \n  ggplotGrob(\n    ggmap(perth_map) +\n      annotate(\"point\", x = longitude, y = latitude,\n        color = \"red\", size = 3, shape = 8) +\n      theme_void())}\n\nIn addition:\n\nWe use an lapply to iterate over each hospital. Typically, we might look to purrr::map or other functions from the tidyverse for tasks like this. We acknowledge here that AI assisted with some of the code here.\nWe use wrap_plots() from the patchwork package to assemble the figures. We originally looked at using facet_wrap() (or similar) from the ggplot2 package, but this proved quite challenging when incorporating the inset map with varying red X marker.\nWe use white text on a white background, as opposed to element_blank() because it keeps all dimensions of the plots consistent as we work through to the reveal! This is really important so plot elements don’t change relative size and position!"
  },
  {
    "objectID": "posts/2025-10-10_AIHW_api/aihw_api_guess_plot.html#figure-1-not-much-to-go-off",
    "href": "posts/2025-10-10_AIHW_api/aihw_api_guess_plot.html#figure-1-not-much-to-go-off",
    "title": "Accessing national health data via API",
    "section": "Figure 1: Not much to go off!",
    "text": "Figure 1: Not much to go off!\nThis plot only shows the time series where both x- and y-axis labels and hospital names are hidden, but perhaps the markings on the maps are a hint? Particularly if you remember which year certain hospitals opened…\n\n\nCode\nhospital_plots &lt;- lapply(split(my_aihw_dat, my_aihw_dat$hospital), function(df) {\n  \n  lat &lt;- df$latitude[1]\n  lon &lt;- df$longitude[1]\n  \n  ts_plot &lt;- ggplot(df, aes(date, count)) +\n    geom_line() +\n    geom_point() +\n    scale_y_continuous(labels = scales::label_comma(), limits = c(0, 120000)) +\n    scale_x_date(date_breaks = \"2 years\", date_labels = \"%Y\",\n                 limits = c(as.Date(\"2013-01-01\"), as.Date(\"2024-12-31\"))) +\n    theme_thekids() +\n    theme(axis.line = element_line(),\n          axis.ticks = element_line(),\n          axis.text = element_text(colour = \"white\")) +\n    labs(x = NULL, y = \" \", title = \" \")\n  \n  ts_plot + inset_element(\n    inset_map(lat, lon),\n    left = 0.8, right = 1,\n    bottom = 0.02, top = 0.55\n  ) \n})\n\nfig1 &lt;- wrap_plots(hospital_plots, ncol = 2) +\n  plot_annotation(caption = \" \")\n\n\n\n\n\n\n\n\nFigure 3: Reveal 1"
  },
  {
    "objectID": "posts/2025-10-10_AIHW_api/aihw_api_guess_plot.html#figure-2-time-series-with-x-axis-labels",
    "href": "posts/2025-10-10_AIHW_api/aihw_api_guess_plot.html#figure-2-time-series-with-x-axis-labels",
    "title": "Accessing national health data via API",
    "section": "Figure 2: Time Series with x-axis Labels",
    "text": "Figure 2: Time Series with x-axis Labels\nSimilar to Figure 1, but the x-axis labels are visible, allowing the timeline to be interpreted while keeping the y-axis blank.\n\n\nCode\nhospital_plots &lt;- lapply(split(my_aihw_dat, my_aihw_dat$hospital), function(df) {\n  \n  lat &lt;- df$latitude[1]\n  lon &lt;- df$longitude[1]\n  \n  ts_plot &lt;- ggplot(df, aes(date, count)) +\n    geom_line() +\n    geom_point() +\n    scale_y_continuous(labels = scales::label_comma(), limits = c(0, 120000)) +\n    scale_x_date(date_breaks = \"2 years\", date_labels = \"%Y\",\n                 limits = c(as.Date(\"2013-01-01\"), as.Date(\"2024-12-31\"))) +\n    theme_thekids() +\n    theme(axis.line = element_line(),\n          axis.ticks = element_line(),\n          axis.text.y = element_text(colour = \"white\")) +\n    labs(x = NULL, y = \" \", title = \" \")\n  \n  ts_plot + inset_element(\n    inset_map(lat, lon),\n    left = 0.8, right = 1,\n    bottom = 0.02, top = 0.55\n  ) \n})\n\nfig2 &lt;- wrap_plots(hospital_plots, ncol = 2) +\n  plot_annotation(caption = \" \")\n\n\n\n\n\n\n\n\nFigure 4: Reveal 2"
  },
  {
    "objectID": "posts/2025-10-10_AIHW_api/aihw_api_guess_plot.html#figure-3-time-series-with-caption",
    "href": "posts/2025-10-10_AIHW_api/aihw_api_guess_plot.html#figure-3-time-series-with-caption",
    "title": "Accessing national health data via API",
    "section": "Figure 3: Time Series with Caption",
    "text": "Figure 3: Time Series with Caption\nThis version includes visible axes and ticks, making the data easier to interpret. A caption indicating the data source (AIHW API) is added as a real nudge to what the data might be. Hospital names are still hidden.\n\n\nCode\nhospital_plots &lt;- lapply(split(my_aihw_dat, my_aihw_dat$hospital), function(df) {\n  \n  lat &lt;- df$latitude[1]\n  lon &lt;- df$longitude[1]\n  \n  ts_plot &lt;- ggplot(df, aes(date, count)) +\n    geom_line() +\n    geom_point() +\n    scale_y_continuous(labels = scales::label_comma(), limits = c(0, 120000)) +\n    scale_x_date(date_breaks = \"2 years\", date_labels = \"%Y\",\n                 limits = c(as.Date(\"2013-01-01\"), as.Date(\"2024-12-31\"))) +\n    theme_thekids() +\n    theme(axis.line = element_line(),\n          axis.ticks = element_line()) +\n    labs(x = NULL, y = \" \", title = \" \")\n  \n  ts_plot + inset_element(\n    inset_map(lat, lon),\n    left = 0.8, right = 1,\n    bottom = 0.02, top = 0.55\n  ) \n})\n\nfig3 &lt;- wrap_plots(hospital_plots, ncol = 2) +\n  plot_annotation(caption = \"Data source: AIHW API, code: 'XXX-XX'\")\n\n\n\n\n\n\n\n\nFigure 5: Reveal 3"
  },
  {
    "objectID": "posts/2025-10-10_AIHW_api/aihw_api_guess_plot.html#figure-4-the-reveal",
    "href": "posts/2025-10-10_AIHW_api/aihw_api_guess_plot.html#figure-4-the-reveal",
    "title": "Accessing national health data via API",
    "section": "Figure 4: The reveal!",
    "text": "Figure 4: The reveal!\nAnd here, everything is revealed.\n\n\nCode\nhospital_plots &lt;- lapply(split(my_aihw_dat, my_aihw_dat$hospital), function(df) {\n  \n  lat &lt;- df$latitude[1]\n  lon &lt;- df$longitude[1]\n  \n  ts_plot &lt;- ggplot(df, aes(date, count)) +\n    geom_line() +\n    geom_point() +\n    scale_y_continuous(labels = scales::label_comma(), limits = c(0, 120000)) +\n    scale_x_date(date_breaks = \"2 years\", date_labels = \"%Y\",\n                 limits = c(as.Date(\"2013-01-01\"), as.Date(\"2024-12-31\"))) +\n    theme_thekids() +\n    theme(axis.line = element_line(),\n          axis.ticks = element_line()) +\n    labs(x = NULL, y = \"ED Presentations (n)\", title = unique(df$hospital))\n  \n  ts_plot + inset_element(\n    inset_map(lat, lon),\n    left = 0.8, right = 1,\n    bottom = 0.02, top = 0.55\n  ) \n})\n\nfig4 &lt;- wrap_plots(hospital_plots, ncol = 2)  +\n  plot_annotation(caption = \"Data source: AIHW API, code: 'MYH-ED'\")\n\n\n\n\n\n\n\n\nFigure 6: The final reveal"
  },
  {
    "objectID": "posts/2025-10-10_AIHW_api/aihw_api_guess_plot.html#putting-them-all-together",
    "href": "posts/2025-10-10_AIHW_api/aihw_api_guess_plot.html#putting-them-all-together",
    "title": "Accessing national health data via API",
    "section": "Putting them all together",
    "text": "Putting them all together\nPutting these four stages of the reveal together also posed a few challenges, namely combining multiple plots that themselves were assembled with patchwork.\nFor this, we again turned to AI, and the resulting code is available below.\n\n\nCode\n# Divider \"plots\" (vertical and horizontal)\nv_thickness &lt;- 0.005  # fraction of width for vertical divider (adjust)\nh_thickness &lt;- 0.01  # fraction of height for horizontal divider (adjust)\n\ndivider_v &lt;- ggplot() +\n  theme_void() +\n  theme(plot.background = element_rect(fill = \"black\", colour = NA))\n\ndivider_h &lt;- divider_v  # same look for horizontal divider\n\n# Make rows with a vertical divider between plots\ntop_row &lt;- wrap_plots(p1, divider_v, p2, ncol = 3, widths = c(1, v_thickness, 1))\nbottom_row &lt;- wrap_plots(p3, divider_v, p4, ncol = 3, widths = c(1, v_thickness, 1))\n\n# Combine rows with a horizontal divider between them\nfinal &lt;- wrap_plots(top_row, divider_h, bottom_row, \n                    ncol = 1, heights = c(1, h_thickness, 1)) +\n  plot_layout(ncol = 1) \n\nfinal\n\nggsave(paste0(Sys.Date(), \"_fig1-4.png\"),\n       width = 20, height = 10)\n\n\n\n\n\n\n\n\nFigure 7: All 4 Guess The Plots Together"
  },
  {
    "objectID": "posts/2025-10-10_AIHW_api/aihw_api_guess_plot.html#acknowledgements",
    "href": "posts/2025-10-10_AIHW_api/aihw_api_guess_plot.html#acknowledgements",
    "title": "Accessing national health data via API",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Zac Dempsey, Robin Cook, Elizabeth McKinnon, and Wes Billingham for providing feedback on and reviewing this post."
  },
  {
    "objectID": "posts/2025-10-10_AIHW_api/aihw_api_guess_plot.html#reproducibility-information",
    "href": "posts/2025-10-10_AIHW_api/aihw_api_guess_plot.html#reproducibility-information",
    "title": "Accessing national health data via API",
    "section": "Reproducibility Information",
    "text": "Reproducibility Information\nThe session information can also be seen below.\n\n\nR version 4.5.0 (2025-04-11 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n  LAPACK version 3.12.1\n\nlocale:\n[1] LC_COLLATE=English_Australia.utf8  LC_CTYPE=English_Australia.utf8   \n[3] LC_MONETARY=English_Australia.utf8 LC_NUMERIC=C                      \n[5] LC_TIME=English_Australia.utf8    \n\ntime zone: Australia/Sydney\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] jsonlite_2.0.0        httr_1.4.7            readaihw_0.0.2       \n [4] thekidsbiostats_1.4.3 extrafont_0.20        flextable_0.9.10     \n [7] gtsummary_2.4.0       lubridate_1.9.4       forcats_1.0.1        \n[10] stringr_1.5.2         dplyr_1.1.4           purrr_1.1.0          \n[13] readr_2.1.5           tidyr_1.3.1           tibble_3.3.0         \n[16] ggplot2_4.0.0         tidyverse_2.0.0      \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6            httr2_1.2.1             xfun_0.53              \n [4] htmlwidgets_1.6.4       tzdb_0.5.0              vctrs_0.6.5            \n [7] tools_4.5.0             generics_0.1.4          curl_7.0.0             \n[10] pkgconfig_2.0.3         data.table_1.17.8       RColorBrewer_1.1-3     \n[13] S7_0.2.0                readxl_1.4.5            assertthat_0.2.1       \n[16] uuid_1.2-1              lifecycle_1.0.4         compiler_4.5.0         \n[19] farver_2.1.2            textshaping_1.0.4       janitor_2.2.1          \n[22] snakecase_0.11.1        httpuv_1.6.16           fontquiver_0.2.1       \n[25] fontLiberation_0.1.0    htmltools_0.5.8.1       yaml_2.3.10            \n[28] Rttf2pt1_1.3.14         extrafontdb_1.1         later_1.4.4            \n[31] pillar_1.11.1           openssl_2.3.4           mime_0.13              \n[34] fontBitstreamVera_0.1.1 tidyselect_1.2.1        zip_2.3.3              \n[37] digest_0.6.37           stringi_1.8.7           labelled_2.15.0        \n[40] fastmap_1.2.0           grid_4.5.0              cli_3.6.5              \n[43] magrittr_2.0.4          patchwork_1.3.2         withr_3.0.2            \n[46] rappdirs_0.3.3          gdtools_0.4.4           scales_1.4.0           \n[49] promises_1.3.3          timechange_0.3.0        rmarkdown_2.30         \n[52] officer_0.7.0           cellranger_1.1.0        askpass_1.2.1          \n[55] ragg_1.5.0              hms_1.1.4               shiny_1.11.1           \n[58] evaluate_1.0.5          knitr_1.50              haven_2.5.5            \n[61] rlang_1.1.6             Rcpp_1.1.0              xtable_1.8-4           \n[64] glue_1.8.0              xml2_1.4.0              rstudioapi_0.17.1      \n[67] R6_2.6.1                systemfonts_1.3.1       fs_1.6.6               \n[70] shinyFiles_0.9.3"
  },
  {
    "objectID": "posts/2024_08_09_ai_part_two/ai_part_two.html",
    "href": "posts/2024_08_09_ai_part_two/ai_part_two.html",
    "title": "AI in Biostatistics - Part 2",
    "section": "",
    "text": "If you haven’t read part one of this two-part series on AI, do that first here:\nhttps://the-kids-biostats.github.io/posts/2024_08_08_ai_part_one/ai_part_one.html\nLast time we looked at three cases where our use of AI aided our productivity and gave us the desired result. In this post, we’ll be looking at the other side of the coin, to situations where AI has struggled and where attempts to pursue its use would hinder our productivity and have us produced errors.\nTo demonstrate this, we’ll look through the results of a few published papers and the concepts they introduce. Then, as a wrap of Part 1 and 2, we propose some guidelines that can be used to maximise ones efficiency when using (or thinking about using) AI."
  },
  {
    "objectID": "posts/2024_08_09_ai_part_two/ai_part_two.html#example-1---jagged-frontier",
    "href": "posts/2024_08_09_ai_part_two/ai_part_two.html#example-1---jagged-frontier",
    "title": "AI in Biostatistics - Part 2",
    "section": "Example 1 - Jagged Frontier",
    "text": "Example 1 - Jagged Frontier\nOur first example is drawn from a 2023 paper which looked at the effect of AI use on productivity and quality of work. You can read the paper here:\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=4573321\nIt introduces a very helpful concept - the ‘Jagged Frontier’. Below we use ggplot2 to demonstrate this idea.\n\nWhat is the Jagged Frontier?\nImagine you have 5 tasks to complete today, and you are trying to decide which of these you can delegate to AI (in this case, ChatGPT). You might have in mind that each task has some implicit difficulty associated with it (rated 1- 10), and your perception of current AI technology is that in is capable of tackling tasks up to a difficulty of 5. This is represented by the blue dashed line below.\nYour 5 tasks have various difficulties, represented by the black points. At this point, based on your perception of the difficulty of your tasks and the capability of AI, you would assign tasks 2, 3 and 5 to ChatGPT, since these have a difficulty of &lt;=5.\nUnfortunately, the reality is that our perception of difficulty does not line up well with AI’s capabilities. As we have seen in the last post, it can complete some pretty incredible tasks, such as coding Shiny apps and performing advanced statistical analyses. But as we will also see later, it can struggle with some pretty mundane tasks.\nAnd so the capability of AI cannot be measured by the blue dashed line, but rather the red, jagged line, a seemingly random level of capability which is unrelated to our ideas of difficulty. This is what is referred to as the “Jagged Frontier”.\n\n\n\n\n\n\n\n\n\n\n\nWhy should we care?\nWhen a task can successfully be completed using AI, we refer to that task as being ‘Inside the Frontier’; that is, below the red jagged line above. When AI would struggle or be unable to complete a task, we refer to that as being ‘Outside the Frontier’.\nThe study above set out to investigate the difference in worker productivity when AI is used for tasks considered ‘Inside the Frontier’ compared with those considered ‘Outside the Frontier’. A large study was set up, with the following methodology:\n\nAll 758 participants (experts in the relevant field) complete a baseline project without any AI use. Speed and quality are measured.\nParticipants are randomised into one of four groups (see table below).\n\n\n\n\n\n\nMetric\nInside.Frontier\nOutside.Frontier\n\n\n\n\nChatGPT\n189\n190\n\n\nChatGPT + Overview\n190\n189\n\n\n\n\n\n\n\n\nThose in the “Inside Frontier” group were assigned a project that had been determined beforehand to be within the capabilities of ChatGPT 3.5. In contrast, the “Outside Frontier” group were assigned a project beyond the capabilities of ChatGPT 3.5.\nThe participants were further divided into “ChatGPT” and “ChatGPT + Overview” groups. The latter received some training on prompt engineering (how be to use ChaptGPT) before undertaking the project.\nAll participants completed their project, and their speed and quality was compared to their own individual baseline score.\n\nThe results are quite striking:\n\n\n\n\n\nMetric\nInside.Frontier\n\n\n\n\nChatGPT\n\n\nSpeed\n28%\n\n\nQuality\n38%\n\n\nChatGPT + Overview\n\n\nSpeed\n23%\n\n\nQuality\n43%\n\n\n\n\n\n\n\nUsing ChatGPT for the project ‘Inside the Frontier’ resulted in considerable improvements in both speed and quality of work, regardless of whether participants received training.\nBut perhaps more interestingly, the groups with projects ‘Outside the Frontier’ saw a decrease in quality of work, even though the work was completed faster…\nWe will return to these results in the discussion below."
  },
  {
    "objectID": "posts/2024_08_09_ai_part_two/ai_part_two.html#example-2---biostatistical-questions",
    "href": "posts/2024_08_09_ai_part_two/ai_part_two.html#example-2---biostatistical-questions",
    "title": "AI in Biostatistics - Part 2",
    "section": "Example 2 - Biostatistical Questions",
    "text": "Example 2 - Biostatistical Questions\nAs promised, we are going to investigate some examples of tasks that lie ‘Outside the Frontier’. These examples are drawn from this paper:\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC10646144/\nAs a brief summary, 10 different biostatistics-related problems were posed to ChatGPT 3.5 and subsequently to ChatGPT 4.0. For each problem, the team attempted up to three times to get the correct answer from the AI. The results are below:\n\n\n\n\n\n\n\n\n\nWhile, ChatGPT 4.0 made significant improvements on 3.5, 40% of answers were still incorrect on the first attempt.\nWe attempted to reproduce one of these results, using their Question 2, which was:\n“Suppose the probability of surviving from a particular disease is 0.9 and there are 20 patients. The number surviving will follow a Binomial distribution with p=0.9 and n=20. What is the probability that no more than 1 patient dies?”\nThe expected answer to this question is 39.2%, and there are two ways we could get this answer in R using the pbinom function:\n\nsize &lt;- 20 # Number of trials\nprob &lt;- 0.1 # Probability of *dying*\n\npbinom(1, size, prob)\n\n[1] 0.391747\n\n\n\nprob &lt;- 0.9 # Probability of *surviving*\n\n1 - pbinom(18, size, prob)\n\n[1] 0.391747\n\n\nGraphically, we can highlight the area of the distribution we are interested in calculating - this will be helpful later to compare with ChatGPT’s responses."
  },
  {
    "objectID": "posts/2024_08_09_ai_part_two/ai_part_two.html#first-attempt",
    "href": "posts/2024_08_09_ai_part_two/ai_part_two.html#first-attempt",
    "title": "AI in Biostatistics - Part 2",
    "section": "First Attempt:",
    "text": "First Attempt:\nBelow is our prompt history with ChatGPT 4.0 as we attempted to reproduce the results from the paper - namely, using a maximum of three attempts to get the correct answer.\n\n\n\n\n\n\nNote.\n\n\n\nHighlighted sections and markings were added for emphasis and to assist with the live presentation that these two blog posts have been adapted from.\n\n\n\n\n\n\n\n\n\n\n\nOn ChatGPT’s first attempt, it gives us an answer of 87.8%, a far cry from 39.2%! The blue icon at the end of its answer reveals the Python code it used to generate this answer, however we asked it to provide the equivalent code in R:\n\n\n\n\n\n\n\n\n\nWe can run the code here to verify the answer:\n\nn &lt;- 20 # number of trials\np &lt;- 0.9 # probability of success\n\nprob &lt;- pbinom(19, n, p)\n\nprob\n\n[1] 0.8784233\n\n\nInterestingly, the binom package is not actually used!\nThe AI has seemingly ‘understood’ the question, as shown by its correct inference that “no more than 1 patient dies” is equivalent to “at least 19 patients surviving” in its code documentation. However, the code provided has gotten things the wrong way around.\nAbove, we showed the correct area of the distribution we are interested in. Now we can see this side by side with what the AI code is calculating:"
  },
  {
    "objectID": "posts/2024_08_09_ai_part_two/ai_part_two.html#second-attempt",
    "href": "posts/2024_08_09_ai_part_two/ai_part_two.html#second-attempt",
    "title": "AI in Biostatistics - Part 2",
    "section": "Second Attempt",
    "text": "Second Attempt\nIn the paper, the way in which ChatGPT was prompted to try again at a particular problem was to simply say “I got ”. We do the same here:\n\n\n\n\n\n\n\n\n\nWe see that the AI has doubled down and is suggesting we have misunderstood the question. It describes the problem back to us perfectly, with one exception:\n“The focus should be on the cumulative probability of having up to 19 survivors.”\nWhile this is incorrect, it is the first time anything it has said has matched the calculation it carried out."
  },
  {
    "objectID": "posts/2024_08_09_ai_part_two/ai_part_two.html#third-attempt",
    "href": "posts/2024_08_09_ai_part_two/ai_part_two.html#third-attempt",
    "title": "AI in Biostatistics - Part 2",
    "section": "Third Attempt",
    "text": "Third Attempt\nWith final attempt, it is up for debate whether the response is correct or not:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe code it provides does give the correct answer now, however there is still a ‘belief’ that 0.392 was and is incorrect.\n\nn &lt;- 20 # number of trials\np &lt;- 0.9 # probability of success\n\nprob_no_more_than_1_dying &lt;- pbinom(1, n, 1- p)\n\nprint(prob_no_more_than_1_dying)\n\n[1] 0.391747"
  },
  {
    "objectID": "posts/2024_08_09_ai_part_two/ai_part_two.html#takeaway",
    "href": "posts/2024_08_09_ai_part_two/ai_part_two.html#takeaway",
    "title": "AI in Biostatistics - Part 2",
    "section": "Takeaway",
    "text": "Takeaway\nWithout knowing how pbinom works (and even then, noticing it had been implemented incorrectly!), it is likely that a user would trust the initial incorrect answer. The AI showed signs of understanding the question, the code documentation made sense, and the code itself looked completely plausible…\nHow much more so for more complicated scenarios?"
  },
  {
    "objectID": "posts/2024_08_09_ai_part_two/ai_part_two.html#where-to-from-here-our-recommendations",
    "href": "posts/2024_08_09_ai_part_two/ai_part_two.html#where-to-from-here-our-recommendations",
    "title": "AI in Biostatistics - Part 2",
    "section": "Where to from here? Our recommendations…",
    "text": "Where to from here? Our recommendations…\nThe ‘jagged frontier’ of AI is invisible to us, and hence we often cannot know whether the task we are about to delegate is within its capability or not (and it doesn’t know!). For this reason, we propose the following guidelines to ensure you are using AI effectively and responsibly:\n\nYou should be able to verify the output is true/accurate.\nThe time it would take to verify the AI output should be less than the time it would take to achieve the same output yourself.\n\nLooking back at our examples throughout the two posts:\n\nWeather plot\nIn this example, we could verify the more complex plot by visually comparing it to simple plots we coded ourselves. Additionally, we could understand the R code that AI produced. Whether the use of AI in this case was justified or not depends on how quickly we could have remembered all the ggplot2 commands to do this ourselves!\n\n\nStatistical Analysis\nFor the statistical analysis, we were well-placed to verify the suggestions of ChatGPT. However, this would be a dangerous use-case for someone who is unfamiliar with statistical methodology - advanced or otherwise. We know that the suggested hierarchical mixed models is a good suggestion, but this would not be immediately verifiable to everyone. Therefore we would recommend against this use-case for non-statisticians.\n\n\nShiny App\nThis is probably the best example of AI saving time and increasing quality. The objective is simple - a Shiny App which takes and randomises data. Even as someone who has never coded a Shiny App before, it is easy to verify whether it works by simply using the app.\n\n\nMathematical Questions\nFinally, we have seen that AI can make mistakes when calculating numerical results, and remain confidently incorrect in the face of correction. To verify the answer requires working line by line through each piece of code to ensure it is doing what we asked. Hence it is unlikely to save us time and seemingly likely to be incorrect."
  },
  {
    "objectID": "posts/2024_08_09_ai_part_two/ai_part_two.html#further-reading",
    "href": "posts/2024_08_09_ai_part_two/ai_part_two.html#further-reading",
    "title": "AI in Biostatistics - Part 2",
    "section": "Further reading",
    "text": "Further reading\nA similar paper to Example 2, testing responses to a set of 70 medical questions: https://onlinelibrary.wiley.com/doi/full/10.1111/iej.13985\nDespite the humourous title, a fascinating paper on where Artificial Intelligence is, well, intelligent: https://link.springer.com/article/10.1007/s10676-024-09775-5"
  },
  {
    "objectID": "posts/2024-09-10_small_n/small_sample_analysis.html",
    "href": "posts/2024-09-10_small_n/small_sample_analysis.html",
    "title": "Small-sample analysis",
    "section": "",
    "text": "The sample size in research projects is often limited by practical constraints e.g. cost of equipment, logistics of animal breeding and management, or the (low) prevalence of patients with a particular condition. Group sizes of 3-10 are typical of many animal studies and laboratory experiments, and while small can mean different things to different researchers, we’re going to focus here on sample statistics and sampling variation as the same size in a project heads down into this territory."
  },
  {
    "objectID": "posts/2024-09-10_small_n/small_sample_analysis.html#example-samples-of-mice",
    "href": "posts/2024-09-10_small_n/small_sample_analysis.html#example-samples-of-mice",
    "title": "Small-sample analysis",
    "section": "Example: Samples of mice",
    "text": "Example: Samples of mice\n\n\n\n\n\n\n\n\n\n\n\nFigure 1\nHere, we simulate data to represent a “population” of healthy female C57BL/6J mice who are expected to have a mean weight of 23 grams, with a standard deviation of 2 grams, at 14 weeks of age The Jackson Laboratory.\nIn practice researchers would only have data on a sample (subset) of a (usually hypothetical) population, from which to calculate summary statistics as estimates of general population characteristics. There is a potentially obvious, but perhaps nuanced, point here that needs to be stated, and that is that we do not know the population parameter we are estimating - if we did, we would not be trying to estimate it.\nIn practice researchers would only have data on a sample (subset) of a (usually hypothetical) population, from which to calculate summary statistics as estimates of general population characteristics. There is a potentially obviously and perhaps nuanced point here that needs to be stated, and that is that we do not know the population parameter we are estimating - if we did, we would not be trying to estimate it.\nAs may be expected, for larger datasets the calculated summary statistic is more likely to be closer to the (true unknown) population parameter from which the sample was drawn. Let’s see how the calculated mean might be expected to vary from sample to sample for our mouse weight example.\nTo understand this variability, we’ll start by randomly drawing 500 samples that are each of size n=50.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2\na) The first 5 samples (of 500) with a sample size of 50, from a simulated population with mean=23 and std dev=2.\n b) The distribution of the sample means calculated from each of the 500 draws.\n\n\n\n\n\n\nNote\n\n\n\nThe mean of the sample means takes a value of 23.00, the same value, to 2 decimal places, as the (in this instance, because we set it, known) parameter for the population from which the sample was drawn.\n\n\nThe spread (in Fig 2b) appears much reduced, but that is because it is not the spread of individual observations - it is the spread of sample means. In fact, the standard deviation of the sample means (i.e. the standard error of the mean) is 0.3, in line with the expected reduction of the population standard deviation of 2 by a factor of \\(1/\\sqrt{n}\\).\nThe spread (in Fig 2b) appears much reduced, but that is because it is not the spread of individual observations - it is the spread of sample means.In fact, the standard deviation of the sample means (i.e. the standard error of the mean) is 0.3, in line with the expected reduction of the population standard deviation of 2 by a factor of \\(1/\\sqrt{n}\\).\nNow let’s see what happens as the sample size decreases.\nHere, a sample of n=20.\n\n\n\n\n\n\n\n\n\n\n\nFigure 3\na) The first 5 samples (of 500) with a sample size of 20, from a simulated population with mean=23 and std dev=2.\n b) The distribution of the sample means calculated from each of the 500 draws.\nThis spread here (in Fig 3b) does not look too different to that seen above (in Fig 2b).\nAnd here, a sample of n=5.\n\n\n\n\n\n\n\n\n\n\n\nFigure 4\na) The first 5 samples (of 500) with a sample size of 5, from a simulated population with mean=23 and std dev=2.\n b) The distribution of the sample means calculated from each of the 500 draws.\nIt is clear that as the sample size decreases the estimates of the population mean become progressively more variable. This increased variability is taken into account in the formula for calculating the 95% confidence interval which becomes accordingly wider as precision decreases:\n\n\n\n\n\n\n\n\n\n\n\nFigure 5\nRandom samples of increasing size, annotated with means and 95% confidence intervals."
  },
  {
    "objectID": "posts/2024-09-10_small_n/small_sample_analysis.html#example-proportions",
    "href": "posts/2024-09-10_small_n/small_sample_analysis.html#example-proportions",
    "title": "Small-sample analysis",
    "section": "Example: Proportions",
    "text": "Example: Proportions\nSo, we know that for small sample sizes the calculated confidence intervals will be wide. This can be even more evident with proportions. Despite this, the upper and/or lower limits of these confidence intervals can still offer useful bounds in which to aid with decision making.\nFor example, consider patient recruitment to an early-phase clinical trial of a new drug that is under development. Recruitment may cease early if there is sufficient evidence of high toxicity or provisional efficacy - according to pre-defined criteria. Here, stopping rules based on one-sided binomial confidence intervals (think, proportions) can inform trial planning and implementation.\nSay enrollment into a dose expansion cohort is planned, with an upper threshold of 40% for allowable dose limiting toxicity and treatment efficacy indicated if response rates are at least 50%. Assuming the planned cohort size is 10, Figure 7 presents 80% lower confidence bounds (LCBs) as a reference for response counts, and more conservative 90% upper confidence bounds (UCBs) for toxicity counts .\n\n\n\n\n\n\n\n\n\n\n\nFigure 7\n90% upper confidence bounds (UCBs, teal) for dose-limiting toxicity (DLT) and 80% lower confidence bounds (LCBs, ochre) for treatment response, for a sample size of 10. Bounds were constructed from one-sided binomial confidence intervals, using the Clopper-Pearson method.\nThe drug would be deemed to have an acceptable toxicity profile if no more than 1 subject was impacted by a dose-limiting toxicity (DLT). Given the small sample size, if there are 2 or more impacted subjects then the confidence interval for the true toxicity rate will be in excess of the 40% threshold, even though the observed rate may be no more than 20%. The stopping rule for unacceptable toxicity would be invoked if a second DLT occurs prior to the final enrollment.\nSimilarly, if treatment elicits an efficacious response in at least 8 patients, there would be reasonable confidence that the treatment has demonstrated sufficient efficacy to warrant continued investigation, and should this be established prior to enrolment of the complete cohort, the trial may be stopped early.\nConfidence intervals for proportions are notoriously wide, and typically large sample sizes are sought to obtain a required level of precision for (around) the rate that is being estimated. If available resources are limited, it may be that a more efficient experimental could be considered."
  },
  {
    "objectID": "posts/2024-09-10_small_n/small_sample_analysis.html#reproducibility-information",
    "href": "posts/2024-09-10_small_n/small_sample_analysis.html#reproducibility-information",
    "title": "Small-sample analysis",
    "section": "Reproducibility Information",
    "text": "Reproducibility Information\nTo access the .qmd (Quarto markdown) files as well as any R scripts or data that was used in this post, please visit our GitHub:\nThe session information can also be seen below.\n\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Australia/Perth\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] DescTools_0.99.56 pwr_1.3-0         cowplot_1.1.3     ggpubr_0.6.0     \n [5] biometrics_1.2.4  lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1    \n [9] dplyr_1.1.4       purrr_1.0.2       readr_2.1.5       tidyr_1.3.1      \n[13] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   extrafont_0.19   \n\nloaded via a namespace (and not attached):\n [1] gld_2.6.6               readxl_1.4.3            rlang_1.1.4            \n [4] magrittr_2.0.3          snakecase_0.11.1        e1071_1.7-14           \n [7] compiler_4.3.3          systemfonts_1.1.0       vctrs_0.6.5            \n[10] httpcode_0.3.0          pkgconfig_2.0.3         crayon_1.5.3           \n[13] fastmap_1.2.0           backports_1.5.0         labeling_0.4.3         \n[16] utf8_1.2.4              promises_1.3.0          rmarkdown_2.28         \n[19] tzdb_0.4.0              haven_2.5.4             ragg_1.3.2             \n[22] xfun_0.47               labelled_2.13.0         jsonlite_1.8.8         \n[25] later_1.3.2             uuid_1.2-1              broom_1.0.6            \n[28] R6_2.5.1                stringi_1.8.4           car_3.1-2              \n[31] boot_1.3-30             extrafontdb_1.0         cellranger_1.1.0       \n[34] Rcpp_1.0.13             knitr_1.48              httpuv_1.6.15          \n[37] Matrix_1.6-5            igraph_2.0.3            timechange_0.3.0       \n[40] tidyselect_1.2.1        rstudioapi_0.16.0       abind_1.4-5            \n[43] yaml_2.3.10             curl_5.2.2              lattice_0.22-6         \n[46] shiny_1.9.1             withr_3.0.1             flextable_0.9.6        \n[49] askpass_1.2.0           evaluate_0.24.0         proxy_0.4-27           \n[52] zip_2.3.1               xml2_1.3.6              pillar_1.9.0           \n[55] carData_3.0-5           generics_0.1.3          hms_1.1.3              \n[58] rootSolve_1.8.2.4       munsell_0.5.1           scales_1.3.0           \n[61] xtable_1.8-4            class_7.3-22            glue_1.7.0             \n[64] janitor_2.2.0           gdtools_0.3.7           lmom_3.0               \n[67] tools_4.3.3             gfonts_0.2.0            data.table_1.16.0      \n[70] ggsignif_0.6.4          Exact_3.3               mvtnorm_1.2-6          \n[73] grid_4.3.3              Rttf2pt1_1.3.12         colorspace_2.1-1       \n[76] cli_3.6.3               kableExtra_1.4.0        textshaping_0.4.0      \n[79] officer_0.6.6           fontBitstreamVera_0.1.1 fansi_1.0.6            \n[82] expm_1.0-0              viridisLite_0.4.2       svglite_2.1.3          \n[85] gtable_0.3.5            rstatix_0.7.2           digest_0.6.37          \n[88] fontquiver_0.2.1        crul_1.5.0              farver_2.1.2           \n[91] htmlwidgets_1.6.4       htmltools_0.5.8.1       lifecycle_1.0.4        \n[94] httr_1.4.7              mime_0.12               fontLiberation_0.1.0   \n[97] openssl_2.2.1           MASS_7.3-60.0.1"
  },
  {
    "objectID": "posts/2024-07-22_pre_post/pre-post.html",
    "href": "posts/2024-07-22_pre_post/pre-post.html",
    "title": "Analysing pre-post data",
    "section": "",
    "text": "Pre-post designs are in use everywhere we look. Before and after treatment, e.g. in a randomised clinical trial, might be the example that most readily comes to mind for analysts in the health sciences area. However, the pre-post concept occurs in many non-randomised settings as well, like observational studies or retrospective policy evaluation studies, and can feature in our daily lives without us even knowing it (like in A/B testing to see if you spend just a little bit longer on social media or buy that chocolate bar at the checkout).\nWhile in principle, the set-up (premise) for such a question seems straightforward, there is extensive literature debating the different approaches to the analysis of this data. The discussions around these choices may confuse the lay or even semi-experienced analyst. Often these discussions are held without a serviceable example to illustrate implementation.\nBelow is an example to help provide some direct links between what we see with raw data and model output, when tackling this question."
  },
  {
    "objectID": "posts/2024-07-22_pre_post/pre-post.html#change-score-analysis",
    "href": "posts/2024-07-22_pre_post/pre-post.html#change-score-analysis",
    "title": "Analysing pre-post data",
    "section": "Change score analysis",
    "text": "Change score analysis\nLet’s reduce the two observations we have per participant into one ‘difference’ (change score: post-value minus pre-value).\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\ncontrol, N = 1971\ntreatment, N = 2041\n\n\n\n\nchange\n20.34 (4.7) [197]\n50.24 (4.5) [204]\n\n\n\n1 Mean (SD)\n[No. obs.]\n\n\n\n\n\n\n\n\nThis makes sense, the mean change of ~20 and ~50 for the control and treatment groups (respectively) aligns with what we asked for in the data generation process.\n\n\nCode\ndat %&gt;% \n  tibble %&gt;% \n  pivot_wider(id_cols = c(id, group), names_from = timepoint, values_from = c(out, time)) %&gt;% \n  mutate(change = out_post - out_pre) %&gt;% \n  select(group, change) %&gt;% \n  ggplot(aes(group, change, colour = group)) +\n  geom_violin(aes(fill = group), alpha = 0.1) +\n  geom_jitter(width = 0.1, height = 0, alpha = 0.3) +\n  theme_clean() +\n  scale_colour_viridis_d(option = \"plasma\", end = 0.85) +\n  scale_fill_viridis_d(option = \"plasma\", end = 0.85) +\n  labs(y = \"Outcome\", x = \"Timepoint\",\n       title = \"Pre-post change score analysis\", \n       subtitle = \"Two-groups, violin plot with jittered points\", colour = \"Group\", fill = \"Group\")\n\n\n\n\n\n\n\n\n\nWith one observation per participant and two groups (and approximately normally distributed data), we can use a t.test to evaluate the difference between groups.\n\n\nRegistered S3 methods overwritten by 'broom':\n  method            from  \n  tidy.glht         jtools\n  tidy.summary.glht jtools\n\n\n# A tibble: 1 × 7\n  estimate statistic   p.value conf.low conf.high method             alternative\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;      \n1    -29.9     -64.9 3.13e-213    -30.8     -29.0 Welch Two Sample … two.sided  \n\n\nThe output suggests:\n\nA difference between groups means of ~30.\n\nThis exact result, with using a change score as the outcome, can also be reached using linear regression (left to the reader)."
  },
  {
    "objectID": "posts/2024-07-22_pre_post/pre-post.html#linear-regression-ancova-framework",
    "href": "posts/2024-07-22_pre_post/pre-post.html#linear-regression-ancova-framework",
    "title": "Analysing pre-post data",
    "section": "Linear regression (ANCOVA framework)",
    "text": "Linear regression (ANCOVA framework)\nHere, the outcome variable is the post-value with the treatment variable as the exposure of interest (RHS of the model).\nlm(out_post ~ out_pre + group, data = dat)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n20.77\n19.66, 21.88\n&lt;0.001\n\n\nout_pre\n0.96\n0.87, 1.05\n&lt;0.001\n\n\ngroup\n\n\n\n\n\n\n\n\n    control\n—\n—\n\n\n\n\n    treatment\n30.36\n29.06, 31.65\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nThis output suggests (in order):\n\nThe intercept (the mean value for a control participant with 0 as a pre-value) is ~21\n\nThis may be curious, we know the post-values for the control group have a mean of ~30, however, the mean pre -alue for the control groups is ~10. Take that 10 away from the ~30, and this is why we see ~20 here for a control participant with 0 as a pre-value)\n\nFor every 1 unit increase in a participants pre-value, their post-value will be 0.97 units higher (… than if their pre-value was 1 unit lower)\n[Perhaps of most interest] The post-values for the treatment group are (on average) ~30 units higher than the control group.\n\n\n\nCode\ndat %&gt;% \n  tibble %&gt;% \n  mutate(timepoint = as.numeric(factor(timepoint, levels = c(\"pre\", \"post\"))) - 1,\n         group = as.numeric(factor(group, levels = c(\"control\", \"treatment\"))) -1) %&gt;% \n  mutate(jittered_pos = jitter(timepoint, amount = 0.05),\n         dodge_pos = group * 0.25 ) %&gt;% #- 0.25/2)\n  mutate(group = factor(group, labels = c(\"control\", \"treatment\"))) %&gt;% \n  ggplot(aes(x = dodge_pos + jittered_pos, y = out, group = id, colour = group)) +\n  geom_point(aes(x = dodge_pos + jittered_pos), \n             position = position_dodge(width = 0.25), alpha = 0.3) +\n  geom_line(aes(x = dodge_pos + jittered_pos), alpha = 0.2) +\n  scale_x_continuous(breaks = 0:1, labels = c(\"pre\", \"post\")) +\n  theme_clean() +\n  scale_colour_viridis_d(option = \"plasma\", end = 0.85) +\n  labs(y = \"Outcome\", x = \"Timepoint\",\n       title = \"Pre-post analysis\", \n       subtitle = \"Two-groups, jitterplot with line connecting pairs of observations\", colour = \"Group\")\n\n\n\n\n\n\n\n\n\nWith lines connecting each pairs’ data points together, we can perhaps more clearly see that those with a higher pre-value also have a higher post-value! (What might the data look like if this wasn’t the case…)\nBut you may be wondering …?\n\n\nCode\ndat_b &lt;- dat %&gt;% \n  mutate(timepoint = factor(timepoint, levels = c(\"pre\", \"post\"))) %&gt;% \n  group_by(group, timepoint) %&gt;% \n  summarise(out_sd = sd(out),\n            out = mean(out)) %&gt;% \n  filter(timepoint == \"post\")\n\ndat %&gt;% \n  mutate(timepoint = factor(timepoint, levels = c(\"pre\", \"post\"))) %&gt;% \n  ggplot(aes(timepoint, out)) +\n  geom_jitter(aes(colour = group, group = group), \n              position = position_jitterdodge(jitter.width = 0.1, dodge.width = 0.35), alpha = 0.3) +\n  geom_smooth(aes(colour = group, group = group), \n              position = position_dodge(width = 0.45),\n              method = \"lm\", formula= y ~ x, alpha = 0.6) +\n  theme_clean() +\n  theme(legend.position = \"bottom\") +\n  scale_colour_viridis_d(option = \"plasma\", end = 0.85) +\n  labs(y = \"Outcome\", x = \"Timepoint\",\n       title = \"Pre-post analysis\", \n       subtitle = \"Two-groups, jitterplot with line connecting observed means\", colour = \"Group\") +\n  stat_brace(data = dat_b, aes(group = timepoint), \n             rotate = 90, width = 0.1, outerstart = 2.2, bending = 1) +\n  scale_y_continuous(breaks = seq(0,100,10)) +\n  geom_text(data = tibble(timepoint = c(2.35),\n                          out = c(50.8), # (71.2-30.4) / 2 + 30.4\n                          label = c(\"A difference of ~40?\\nThe model said ~30?\")),\n            aes(label = label), size = 3.5, hjust = 0) +\n  coord_cartesian(xlim = c(1.25, 2.35)) \n\n\n\n\n\n\n\n\n\n\n\nggbrace()\nIt was a bit of a battle with stat_brace() to get the annotation in there, but it was worth it, Shirley!\nWe know the post-value means are ~40 units apart, yet the model has returned the value of ~30 for the ‘treatment effect’, which (implicitly) has adjusted for the baseline difference between groups of ~10 units."
  },
  {
    "objectID": "posts/2024-07-22_pre_post/pre-post.html#linear-mixed-effects-models",
    "href": "posts/2024-07-22_pre_post/pre-post.html#linear-mixed-effects-models",
    "title": "Analysing pre-post data",
    "section": "Linear mixed effects models",
    "text": "Linear mixed effects models\nHere, the outcome variable includes both the pre- and post-measures with the interaction on the RHS of the model allowing the effect of the treatment variable (as the exposure of interest) to vary according to time.\nlmer(out ~ group*timepoint + (1 | id), data = dat)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\n\n\n\n\n(Intercept)\n10.11\n9.28, 10.94\n\n\ngroup\n\n\n\n\n\n\n    control\n—\n—\n\n\n    treatment\n10.44\n9.27, 11.61\n\n\ntimepoint\n\n\n\n\n\n\n    pre\n—\n—\n\n\n    post\n20.34\n19.69, 20.98\n\n\ngroup * timepoint\n\n\n\n\n\n\n    treatment * post\n29.91\n29.00, 30.81\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nThis output suggests (in order):\n\nThe intercept (mean value for a control participant at the pre-measure) is ~10 (makes sense)\nThe treatment group mean (at the pre-measure) is ~11 units higher than the control group mean (makes sense)\nThe post-values in the control group are (on average) ~20 units higher than the pre-values \n[Perhaps of most interest] Compared to the ~20 unit difference (over time) in the control group, the post-values in the treatment group are (an additional) ~30 units higher than the pre-values (total of 50 units difference between pre and post)\n\nIn looking at, and breaking down, this output, you might think that the linear mixed effects model gives you the same answer but with a much more explicit breakdown of many other ‘things going on’ with your outcome data - and you’d be right."
  },
  {
    "objectID": "posts/2024-07-22_pre_post/pre-post.html#acknowledgements",
    "href": "posts/2024-07-22_pre_post/pre-post.html#acknowledgements",
    "title": "Analysing pre-post data",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Elizabeth McKinnon, Zac Dempsey, and Wesley Billingham for providing feedback on and reviewing this post.\nYou can look forward to seeing posts from these other team members here in the coming weeks and months."
  },
  {
    "objectID": "posts/2024-07-22_pre_post/pre-post.html#reproducibility-information",
    "href": "posts/2024-07-22_pre_post/pre-post.html#reproducibility-information",
    "title": "Analysing pre-post data",
    "section": "Reproducibility Information",
    "text": "Reproducibility Information\nTo access the .qmd (Quarto markdown) files as well as any R scripts or data that was used in this post, please visit our GitHub:\nThe session information can also be seen below.\n\n\nR version 4.4.0 (2024-04-24)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.5\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Australia/Perth\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggbrace_0.1.1     gtsummary_1.7.2   jtools_2.2.2      lme4_1.1-35.4    \n [5] Matrix_1.7-0      data.table_1.15.4 simstudy_0.8.0    lubridate_1.9.3  \n [9] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[13] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1    \n[17] tidyverse_2.0.0  \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.5         xfun_0.46            htmlwidgets_1.6.4   \n [4] lattice_0.22-6       tzdb_0.4.0           vctrs_0.6.5         \n [7] tools_4.4.0          generics_0.1.3       parallel_4.4.0      \n[10] fansi_1.0.6          pkgconfig_2.0.3      uuid_1.2-0          \n[13] gt_0.10.1            lifecycle_1.0.4      farver_2.1.2        \n[16] compiler_4.4.0       munsell_0.5.1        codetools_0.2-20    \n[19] htmltools_0.5.8.1    sass_0.4.9           yaml_2.3.9          \n[22] furrr_0.3.1          pillar_1.9.0         nloptr_2.1.0        \n[25] crayon_1.5.3         MASS_7.3-60.2        broom.helpers_1.15.0\n[28] broom.mixed_0.2.9.5  boot_1.3-30          parallelly_1.37.1   \n[31] nlme_3.1-164         commonmark_1.9.1     tidyselect_1.2.1    \n[34] digest_0.6.36        future_1.33.2        stringi_1.8.4       \n[37] listenv_0.9.1        pander_0.6.5         labeling_0.4.3      \n[40] splines_4.4.0        labelled_2.13.0      fastmap_1.2.0       \n[43] grid_4.4.0           colorspace_2.1-0     cli_3.6.3           \n[46] magrittr_2.0.3       utf8_1.2.4           broom_1.0.6         \n[49] bigmemory.sri_0.1.8  bigmemory_4.6.4      withr_3.0.0         \n[52] scales_1.3.0         backports_1.5.0      timechange_0.3.0    \n[55] rmarkdown_2.27       globals_0.16.3       hms_1.1.3           \n[58] fastglm_0.0.3        evaluate_0.24.0      haven_2.5.4         \n[61] knitr_1.48           viridisLite_0.4.2    mgcv_1.9-1          \n[64] markdown_1.13        rlang_1.1.4          Rcpp_1.0.13         \n[67] glue_1.7.0           xml2_1.3.6           rstudioapi_0.16.0   \n[70] minqa_1.2.7          jsonlite_1.8.8       R6_2.5.1"
  },
  {
    "objectID": "posts/2025-04-14_rsam/rsam.html",
    "href": "posts/2025-04-14_rsam/rsam.html",
    "title": "Managing RStudio addins using the rsam package",
    "section": "",
    "text": "library(thekidsbiostats) # install with remotes::install_github(\"The-Kids-Biostats/thekidsbiostats\", build_vignettes = TRUE)"
  },
  {
    "objectID": "posts/2025-04-14_rsam/rsam.html#addins-within-rstudio",
    "href": "posts/2025-04-14_rsam/rsam.html#addins-within-rstudio",
    "title": "Managing RStudio addins using the rsam package",
    "section": "Addins within RStudio",
    "text": "Addins within RStudio\nAddins are accessed in the top toolbar in RStudio. Figure 1 shows the (long) list of addins offered by various packages—most of which, we suspect, will not regularly (or ever) be used by the average user.\n\n\n\n\n\n\nFigure 1: Existing RStudio addins."
  },
  {
    "objectID": "posts/2025-04-14_rsam/rsam.html#the-usecase-for-rsam",
    "href": "posts/2025-04-14_rsam/rsam.html#the-usecase-for-rsam",
    "title": "Managing RStudio addins using the rsam package",
    "section": "The usecase for rsam",
    "text": "The usecase for rsam\nDespite this, RStudio offers no in-built functionalities to manage this list. That is where the rsam package comes in handy—a relatively old and little-known package enabling users to curate their addins list, meaning you can trim the drop down menu to include only those addins you intend to use. In doing so, addins will become more readily available (easier to find) meaning they can be more efficiently incorporated into your workflow.\nFor the Biostatistics team at The Kids Research Institute Australia, we have a number of addins in our package to automate routine elements of our workflows. Therefore, we preference having these addins readily accessible.\nThis post will walk through:\n\nInstallation of the package, and\nManaging addins using the rsam interface."
  },
  {
    "objectID": "posts/2025-04-14_rsam/rsam.html#loading-the-package",
    "href": "posts/2025-04-14_rsam/rsam.html#loading-the-package",
    "title": "Managing RStudio addins using the rsam package",
    "section": "Loading the Package",
    "text": "Loading the Package\nNow we can load the package:\n\nlibrary(rsam)\n\nUpon loading, we must explicitly allow the package to write to disk. We feel you can safely accept both of these request, acknowledging this is a personal choice to be made.\n\n\n\n\n\n\nFigure 2: Allowing package permissions.\n\n\n\n\n\nThese numeric values (and labels) swap around each time the package is loaded. This stops the user from simply parsing over the permissions each time without actively reading them!\nWhile the dropdown menu shows us a list of the available addins, we can use the fetch_addins function from rsam to view these in a table format alongside some additional information about their functionality.\n\nfetch_addins() %&gt;%\n  thekids_table()\n\n\n\nPackageNameDescriptionBindingInteractiveKeylibpathShortcutcliprValue to clipboardCopies the results of a selected expression to the system clipboardclipr_resultfalseclipr::clipr_resultC:/Users/zdempsey/AppData/Local/R/win-library/4.4/clipr/rstudio/_addins.dcfcliprOutput to clipboardCopies the console output of a selected expression to the system clipboardclipr_outputfalseclipr::clipr_outputC:/Users/zdempsey/AppData/Local/R/win-library/4.4/clipr/rstudio/_addins.dcfdevtoolsRun a test fileRun the current test file, using `devtools::test_active_file()`.test_active_filetruedevtools::test_active_fileC:/Users/zdempsey/AppData/Local/R/win-library/4.4/devtools/rstudio/_addins.dcfdevtoolsReport test coverage for a fileCalculate and report test coverage for the current test file, using `devtools::test_coverage_active_file()`.test_coverage_active_filetruedevtools::test_coverage_active_fileC:/Users/zdempsey/AppData/Local/R/win-library/4.4/devtools/rstudio/_addins.dcfdevtoolsReport test coverage for a packageCalculate and report the test coverage for the current package, using `devtools::test_coverage()`.test_coveragetruedevtools::test_coverageC:/Users/zdempsey/AppData/Local/R/win-library/4.4/devtools/rstudio/_addins.dcfdevtoolsDocument a packageA wrapper for `roxygen`'s `roxygen2::roxygenize()`documenttruedevtools::documentC:/Users/zdempsey/AppData/Local/R/win-library/4.4/devtools/rstudio/_addins.dcfdevtoolsRun examplesRuns R code in examples using `devtools::run_examples()`run_examplestruedevtools::run_examplesC:/Users/zdempsey/AppData/Local/R/win-library/4.4/devtools/rstudio/_addins.dcfpkgdownBuild full siteBuild website for current packagebuild_sitetruepkgdown::build_siteC:/Users/zdempsey/AppData/Local/R/win-library/4.4/pkgdown/rstudio/_addins.dcfProjectTemplateLoad ProjectLoad data and packages in the project.loadproject_addinfalseProjectTemplate::loadproject_addinC:/Users/zdempsey/AppData/Local/R/win-library/4.4/ProjectTemplate/rstudio/_addins.dcfProjectTemplateReload ProjectClear the global environment and reload the project.reloadproject_addinfalseProjectTemplate::reloadproject_addinC:/Users/zdempsey/AppData/Local/R/win-library/4.4/ProjectTemplate/rstudio/_addins.dcfRdpackRepromptUpdates Rd file based on editor contentsRStudio_repromptfalseRdpack::RStudio_repromptC:/Users/zdempsey/AppData/Local/R/win-library/4.4/Rdpack/rstudio/_addins.dcfreprexRender reprex...Run `reprex::reprex()` to prepare a reproducible example for sharing.reprex_addintruereprex::reprex_addinC:/Users/zdempsey/AppData/Local/R/win-library/4.4/reprex/rstudio/_addins.dcfreprexReprex selectionPrepare reprex from current selectionreprex_selectionfalsereprex::reprex_selectionC:/Users/zdempsey/AppData/Local/R/win-library/4.4/reprex/rstudio/_addins.dcfrhandsontableEdit a Data FrameInteractively edit a data frame.editAddintruerhandsontable::editAddinC:/Users/zdempsey/AppData/Local/R/win-library/4.4/rhandsontable/rstudio/_addins.dcfrsamlla1Wrap any global objects in rsam_fn_1() and use this addin to run them.lla1truersam::lla1C:/Users/zdempsey/AppData/Local/R/win-library/4.4/rsam/rstudio/_addins.dcfrsamlla2Wrap any global objects in rsam_fn_2() and use this addin to run them.lla2truersam::lla2C:/Users/zdempsey/AppData/Local/R/win-library/4.4/rsam/rstudio/_addins.dcfrsamlla3Wrap any global objects in rsam_fn_3() and use this addin to run them.lla3truersam::lla3C:/Users/zdempsey/AppData/Local/R/win-library/4.4/rsam/rstudio/_addins.dcftargetsEdit _targets.ROpen the file _targets.R for editing. Requires the usethis package.tar_editfalsetargets::tar_editC:/Users/zdempsey/AppData/Local/R/win-library/4.4/targets/rstudio/_addins.dcftargetsRun a targets pipeline in the foregroundCalls tar_make() in the current R process. Requires a _targets.R configuration file.tar_makefalsetargets::tar_makeC:/Users/zdempsey/AppData/Local/R/win-library/4.4/targets/rstudio/_addins.dcftargetsRun a targets pipeline in the backgroundCalls tar_make() in a background process. Requires a _targets.R configuration file.rstudio_addin_tar_make_bgfalsetargets::rstudio_addin_tar_make_bgC:/Users/zdempsey/AppData/Local/R/win-library/4.4/targets/rstudio/_addins.dcftargetsLaunch app to watch progressCalls tar_watch(). Requires a _targets.R file and packages bslib, pingr, shiny, and visNetwork.tar_watchfalsetargets::tar_watchC:/Users/zdempsey/AppData/Local/R/win-library/4.4/targets/rstudio/_addins.dcftargetsVisualize a targets pipelineCalls tar_visnetwork(). Requires a _targets.R configuration file and the visNetwork package.rstudio_addin_tar_visnetworkfalsetargets::rstudio_addin_tar_visnetworkC:/Users/zdempsey/AppData/Local/R/win-library/4.4/targets/rstudio/_addins.dcftargetsGlimpse a targets pipelineCalls tar_glimpse(). Requires a _targets.R configuration file and the visNetwork package.rstudio_addin_tar_glimpsefalsetargets::rstudio_addin_tar_glimpseC:/Users/zdempsey/AppData/Local/R/win-library/4.4/targets/rstudio/_addins.dcftargetsSee outdated targetsCalls tar_outdated(). Requries a _targets.R configuration filerstudio_addin_tar_outdatedfalsetargets::rstudio_addin_tar_outdatedC:/Users/zdempsey/AppData/Local/R/win-library/4.4/targets/rstudio/_addins.dcftargetsPrint recent progressRun tar_progress() and print the tail() of the result.rstudio_addin_tar_progressfalsetargets::rstudio_addin_tar_progressC:/Users/zdempsey/AppData/Local/R/win-library/4.4/targets/rstudio/_addins.dcftargetsLoad target at cursorLoad the target identified by the symbol at the cursor position from the _targets data store.rstudio_addin_tar_loadfalsetargets::rstudio_addin_tar_loadC:/Users/zdempsey/AppData/Local/R/win-library/4.4/targets/rstudio/_addins.dcftargetsRead target at cursorRead the target identified by the symbol at the cursor position from the _targets data store.rstudio_addin_tar_readfalsetargets::rstudio_addin_tar_readC:/Users/zdempsey/AppData/Local/R/win-library/4.4/targets/rstudio/_addins.dcftargetsWrite target at cursorWrite tar_target() at the cursor position.rstudio_addin_tar_targetfalsetargets::rstudio_addin_tar_targetC:/Users/zdempsey/AppData/Local/R/win-library/4.4/targets/rstudio/_addins.dcfthekidsbiostatsInsert CalloutInserts a Quarto callout at the current cursor position.insert_callouttruethekidsbiostats::insert_calloutC:/Users/zdempsey/AppData/Local/R/win-library/4.4/thekidsbiostats/rstudio/_addins.dcfthekidsbiostatsInsert Margin CommentInserts a Quarto margin comment at the current cursor position.insert_margintruethekidsbiostats::insert_marginC:/Users/zdempsey/AppData/Local/R/win-library/4.4/thekidsbiostats/rstudio/_addins.dcfthekidsbiostatsCreate Project (Shiny)Launch a Shiny app to create a new project with a structured directory.create_project_addintruethekidsbiostats::create_project_addinC:/Users/zdempsey/AppData/Local/R/win-library/4.4/thekidsbiostats/rstudio/_addins.dcfthekidsbiostatsInsert Model Panel TabsetInsert Quarto tabset for model output with optional explanationsinsert_model_tabsettruethekidsbiostats::insert_model_tabsetC:/Users/zdempsey/AppData/Local/R/win-library/4.4/thekidsbiostats/rstudio/_addins.dcf"
  },
  {
    "objectID": "posts/2025-04-14_rsam/rsam.html#result",
    "href": "posts/2025-04-14_rsam/rsam.html#result",
    "title": "Managing RStudio addins using the rsam package",
    "section": "Result",
    "text": "Result\nNow, back in the addins toolbar in RStudio, we can see only the ones we selected (I chose to preserve only the targets and thekidsbiostats addins).\n\n\n\n\n\n\nFigure 4: Trimmed down list of addins"
  },
  {
    "objectID": "posts/2025-04-14_rsam/rsam.html#reproducibility-information",
    "href": "posts/2025-04-14_rsam/rsam.html#reproducibility-information",
    "title": "Managing RStudio addins using the rsam package",
    "section": "Reproducibility Information",
    "text": "Reproducibility Information\nTo access the .qmd (Quarto markdown) files as well as any R scripts or data that was used in this post, please visit our GitHub:\nhttps://github.com/The-Kids-Biostats/The-Kids-Biostats.github.io/tree/main/posts/\nThe session information can also be seen below.\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.1 (2024-06-14 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_Australia.utf8  LC_CTYPE=English_Australia.utf8   \n[3] LC_MONETARY=English_Australia.utf8 LC_NUMERIC=C                      \n[5] LC_TIME=English_Australia.utf8    \n\ntime zone: Australia/Sydney\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] thekidsbiostats_0.0.2 flextable_0.9.7       gtsummary_2.1.0      \n [4] lubridate_1.9.4       forcats_1.0.0         stringr_1.5.1        \n [7] dplyr_1.1.4           purrr_1.0.4           readr_2.1.5          \n[10] tidyr_1.3.1           tibble_3.2.1          ggplot2_3.5.2        \n[13] tidyverse_2.0.0       extrafont_0.19       \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6            xfun_0.52               htmlwidgets_1.6.4      \n [4] tzdb_0.5.0              vctrs_0.6.5             tools_4.4.1            \n [7] generics_0.1.3          pkgconfig_2.0.3         data.table_1.17.0      \n[10] uuid_1.2-1              lifecycle_1.0.4         farver_2.1.2           \n[13] compiler_4.4.1          textshaping_1.0.0       munsell_0.5.1          \n[16] janitor_2.2.1           snakecase_0.11.1        httpuv_1.6.15          \n[19] fontquiver_0.2.1        fontLiberation_0.1.0    htmltools_0.5.8.1      \n[22] yaml_2.3.10             Rttf2pt1_1.3.12         pillar_1.10.2          \n[25] later_1.4.2             extrafontdb_1.0         openssl_2.3.2          \n[28] mime_0.13               fontBitstreamVera_0.1.1 tidyselect_1.2.1       \n[31] zip_2.3.2               digest_0.6.37           stringi_1.8.7          \n[34] labelled_2.14.0         fastmap_1.2.0           grid_4.4.1             \n[37] colorspace_2.1-1        cli_3.6.4               magrittr_2.0.3         \n[40] patchwork_1.3.0         withr_3.0.2             gdtools_0.4.2          \n[43] scales_1.3.0            promises_1.3.2          timechange_0.3.0       \n[46] rmarkdown_2.29          officer_0.6.8           askpass_1.2.1          \n[49] ragg_1.3.3              hms_1.1.3               shiny_1.10.0           \n[52] evaluate_1.0.3          haven_2.5.4             knitr_1.50             \n[55] rlang_1.1.5             Rcpp_1.0.14             xtable_1.8-4           \n[58] glue_1.8.0              xml2_1.3.8              rstudioapi_0.17.1      \n[61] jsonlite_2.0.0          R6_2.6.1                systemfonts_1.2.2      \n[64] fs_1.6.5                shinyFiles_0.9.3"
  },
  {
    "objectID": "posts/2024_08_08_ai_part_one/ai_part_one.html",
    "href": "posts/2024_08_08_ai_part_one/ai_part_one.html",
    "title": "AI in Biostatistics - Part 1",
    "section": "",
    "text": "AI is a hot topic in most fields right now, and biostatistics is no exception! We (the Telethon Kids Institute biostats team) were asked to present at the weekly Institute seminar on the use of AI in statistical workflows.\nThe term AI has many meanings depending on the context - in this article we are referring exclusively to (and use the term interchangeably with) large language models (LLMs) such as ChatGPT and Claude. These tools are useful not just for writing sentences and paragraphs of text, but also for writing functioning code!\nSince first investigating the capabilities of ChatGPT in writing R code, our team has been working to utilise it safely and effectively in our everyday workflows.\nThe summary of our message to the Institute staff was that we cannot ignore the massive increase in efficiency that AI can bring if used properly. However, we also need to be very aware of its limitations, and the necessary role of human experts in the process of validating any non-trivial output.\nThis is Part One in a two-part series. This part will look at the capabilities of AI to benefit our workflows and increase efficiency. The next part will look at some limitations of the technology as it currently exists, including some practical recommendations to identify these issues.\n\n\nWe will work through a couple of practical examples of how we might use AI in an everyday workflow:\n\nData Visualisation\nStatistical Analysis\nBonus - Shiny App Development\n\nIn these examples, we will increasingly take our hands off the wheel and allow AI (in these examples, ChatGPT 4.0) to perform more and more of the task."
  },
  {
    "objectID": "posts/2024_08_08_ai_part_one/ai_part_one.html#format-of-this-post",
    "href": "posts/2024_08_08_ai_part_one/ai_part_one.html#format-of-this-post",
    "title": "AI in Biostatistics - Part 1",
    "section": "",
    "text": "We will work through a couple of practical examples of how we might use AI in an everyday workflow:\n\nData Visualisation\nStatistical Analysis\nBonus - Shiny App Development\n\nIn these examples, we will increasingly take our hands off the wheel and allow AI (in these examples, ChatGPT 4.0) to perform more and more of the task."
  },
  {
    "objectID": "posts/2024_08_08_ai_part_one/ai_part_one.html#providing-the-blueprint",
    "href": "posts/2024_08_08_ai_part_one/ai_part_one.html#providing-the-blueprint",
    "title": "AI in Biostatistics - Part 1",
    "section": "Providing the Blueprint",
    "text": "Providing the Blueprint\nChatGPT allows the user to provide images as a part of the prompt. With that in mind, the image was uploaded first by itself. ChatGPT was able to successfully identify the key elements of the plot just from the image (!), and so the next step was to give some guidance to make the plot more visually appealing.\nWith just one prompt: “can you facet wrap the min max data”, in addition to the original plot, it got to work. Once the original data was provided, it returned the requested modifcation - complete with Python code for the plot (R cannot be run within ChatGPT, though we certainly could have requested R code to run ourselves instead).\nHere is the entire prompt history, from beginning to end:\n\nOne image\nOne command\nSome (undescribed) data\n\nwas all the context required to understand and return, approximately, the desired output. We say approximately, since some key information such as the counts above certain levels, is absent from this first attempt:"
  },
  {
    "objectID": "posts/2024_08_08_ai_part_one/ai_part_one.html#end-result-and-discussion",
    "href": "posts/2024_08_08_ai_part_one/ai_part_one.html#end-result-and-discussion",
    "title": "AI in Biostatistics - Part 1",
    "section": "End Result and Discussion",
    "text": "End Result and Discussion\nAfter some further refinement and back-and-forth, we arrive at a much-improved plot which more closely captures the spirit of the original, while improving the colour scheme and facetting the data.\n\n\n\n\n\n\n\n\n\nPlots are a great use-case of AI, since the nitty gritty of colours, panes, text, etc can be a time-consuming hassle to navigate manually. AI in our experience does a good job of translating our descriptive visual prompts into code that achieves the described vision. Furthermore, the output is instantly verifiable (keep this point in mind!) and easy to refine."
  },
  {
    "objectID": "posts/2024_08_08_ai_part_one/ai_part_one.html#providing-the-problem",
    "href": "posts/2024_08_08_ai_part_one/ai_part_one.html#providing-the-problem",
    "title": "AI in Biostatistics - Part 1",
    "section": "Providing the Problem",
    "text": "Providing the Problem\nIn this example, we provided ChatGPT with a very high-level summary of our data and research question, as seen here:\n\n\n\n\n\n\n\n\n\nAgain, we have tried to use prompts that anyone with some familiarity with stats and data analysis would be able to replicate and understand. Here, we simply mention to ChatGPT that there are patients, doctors and hospitals (highlighted above)."
  },
  {
    "objectID": "posts/2024_08_08_ai_part_one/ai_part_one.html#the-response",
    "href": "posts/2024_08_08_ai_part_one/ai_part_one.html#the-response",
    "title": "AI in Biostatistics - Part 1",
    "section": "The Response",
    "text": "The Response\nAmazingly, especially if you have had limited exposure to AI, these high-level, lay descriptions of our data and research questions were sufficient for ChatGPT to suggest a mixed-effects logistic regression model. It recognised the binary outcome and the hierarchical structure of the data, and proceeded to provide R code to perform the analysis.\n\n\n\n\n\n\n\n\n\nHere is where we encounter a difference from our previous visualisation example, however. For someone unfamiliar with statistics, would they have known if this answer happened to be incorrect? Broadly, this response is on track, but should Doctor be nested within Hospital? (keep this in mind! #2).\nFor now, we do recognise that it has selected an appropriate model to get started. An impressive feat!"
  },
  {
    "objectID": "posts/2024_08_08_ai_part_one/ai_part_one.html#end-result-and-discussion-1",
    "href": "posts/2024_08_08_ai_part_one/ai_part_one.html#end-result-and-discussion-1",
    "title": "AI in Biostatistics - Part 1",
    "section": "End Result and Discussion",
    "text": "End Result and Discussion\nFor brevity’s sake, we will not include the whole prompting process here. However, following the model specification, ChatGPT was able to successfully guide us through adding a random slope, and then summarising our model using the broom.mixed package (broom for mixed models, as the name suggests).\nIt then gave us code to check the model fit, run model diagnostics, and perform sensitivity analysis. At each stage, it was able to give advice on interpretation and next steps that we could not fault. Here are some highlights:"
  },
  {
    "objectID": "posts/2024_08_08_ai_part_one/ai_part_one.html#why",
    "href": "posts/2024_08_08_ai_part_one/ai_part_one.html#why",
    "title": "AI in Biostatistics - Part 1",
    "section": "Why?",
    "text": "Why?\nWell yes, you could just use random data between 0 and 1 - but we (and we presume a lot of people) find less mental load involved when the data being worked with resembles the original data (as in, the values are between 140 and 200 for our “blood pressure” histogram, or between 70 and 130 for the “IQ scores” we’re plotting against values between 1.4m-2.1m for “height” etc)."
  },
  {
    "objectID": "posts/2024_08_08_ai_part_one/ai_part_one.html#how-do-i-access-it",
    "href": "posts/2024_08_08_ai_part_one/ai_part_one.html#how-do-i-access-it",
    "title": "AI in Biostatistics - Part 1",
    "section": "How do I access it?",
    "text": "How do I access it?\nIt would be ironic if we provided this for you on our website, because then you would be sending your data to our servers. At our Institute, we run this on an internal server that is not accessible from outside our internal network.\nYou can download a copy from our github (it is a Shiny app that runs within R), and then run it locally on the desktop (or virtual) machine you are working on - anonymising your data without it leaving your analysis environment!"
  },
  {
    "objectID": "posts/2024_08_08_ai_part_one/ai_part_one.html#anything-else",
    "href": "posts/2024_08_08_ai_part_one/ai_part_one.html#anything-else",
    "title": "AI in Biostatistics - Part 1",
    "section": "Anything else?",
    "text": "Anything else?\nOutside of adding the instructional blurb and some minor aesthetic tweaks, this entire app was written by ChatGPT. ChatGPT made the code available for download (as a zip file) and worked through a series of prompts (after the initial prompt) to make small tweaks to get the app doing exactly what we wanted.\nWe then thoroughly inspected the code (knowing roughly how it should have been written/structured) and tested it with a range of test datasets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere is a test dataset (.xlsx) available for download here (click downward arrow on right hand size of the page) alongside the app.\nHow was that created? You guessed it!"
  },
  {
    "objectID": "posts/2024-07-19_data_anonymiser/data_anonymiser.html",
    "href": "posts/2024-07-19_data_anonymiser/data_anonymiser.html",
    "title": "Data Anonymiser",
    "section": "",
    "text": "Data access and security is a really important topic and one that can be challenging to navigate. Research ethics and governance approvals typically require detail about where and how data will be stored and analysed - and a researcher typically does not have approval for the data they are working with to be moved to other locations.\nThe rapid emergence of AI tools and online data analytics platforms (e.g. dashboards) may tempt researchers with the promise of short cuts to writing code and/or producing attractive figures with ease. For example, sites like this that will take an uploaded dataset and draw a figure from it following a user prompt – but where did your precious private research data just go?\nBelow, we share a data anonymising (obscuring) shiny app that you can run on your local machine. It will return to you, a “similar” dataset to your original dataset, allowing you to then safely use an online chat bot to assisted you to code up that complex figure or model."
  },
  {
    "objectID": "posts/2024-07-19_data_anonymiser/data_anonymiser.html#the-process",
    "href": "posts/2024-07-19_data_anonymiser/data_anonymiser.html#the-process",
    "title": "Data Anonymiser",
    "section": "The process",
    "text": "The process\nThe app simply does the following:\n\ntakes an uploaded dataset (.csv or .xlsx),\nlets you choose which variables to keep and how many rows you want returned,\nreturns to you a fully obscured dataset (.csv) for download (see example above).\n\nContained on a tab within the app is a brief tabular report that compares your original data to the anonymised data.\n\nGetting started\nTo use the app, you can either download a .zip (or clone the github repository) at the following link:\nhttps://github.com/TelethonKids/data_anonymiser\nThen:\n\nUnzip the folder\nIn your R session, ensure you are in directory of the unzipped files\nIn your R console, type runApp()\n\nYou may need to install.packages() a few packages, though this app only leverages very popular packages that most R users are likely to already have."
  },
  {
    "objectID": "posts/2024-07-19_data_anonymiser/data_anonymiser.html#data-obscuring-process",
    "href": "posts/2024-07-19_data_anonymiser/data_anonymiser.html#data-obscuring-process",
    "title": "Data Anonymiser",
    "section": "Data obscuring process",
    "text": "Data obscuring process\nThis is also explained on the app’s landing page – but the detail of what is happening in the data obscuring process is as follows:\n\nColumn names: These will all be replaced with a generic name of the form “VAR_1”, “VAR_2”, etc.\nContinuous variables\n\n15 or fewer unique values: These will be randomly (evenly) replaced with integers 0 through to the n (where n is the number of unique values within that variable).\nMore than 15 unique values: Will be replaced with random normally distributed data that has the same mean and standard deviation as the original data.\n\nDate variables: The min and max date will be found within the provided data, then uniformly distributed dates between that min and max date will be returned.\nCategorical variables (includes dichotomous variables)\n\n15 or fewer unique categories: These will be randomly (evenly) replaced with letters “A” through “O”\nMore than 15 unique categories: (E.g., studyIDs, names, addresses etc) these will be replaced with a random string of length 6 (may or may not be unique)."
  },
  {
    "objectID": "posts/2024-07-19_data_anonymiser/data_anonymiser.html#acknowledgements",
    "href": "posts/2024-07-19_data_anonymiser/data_anonymiser.html#acknowledgements",
    "title": "Data Anonymiser",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Wesley Billingham and Zac Dempsey for providing feedback on and reviewing this post.\nYou can look forward to seeing posts from these other team members here in the coming weeks and months."
  },
  {
    "objectID": "posts/2024-07-19_data_anonymiser/data_anonymiser.html#reproducibility-information",
    "href": "posts/2024-07-19_data_anonymiser/data_anonymiser.html#reproducibility-information",
    "title": "Data Anonymiser",
    "section": "Reproducibility Information",
    "text": "Reproducibility Information\nSession information:\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.0 (2024-04-24)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.5\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Australia/Perth\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.4.0    fastmap_1.2.0     cli_3.6.3        \n [5] tools_4.4.0       htmltools_0.5.8.1 rstudioapi_0.16.0 yaml_2.3.9       \n [9] rmarkdown_2.27    knitr_1.48        jsonlite_1.8.8    xfun_0.46        \n[13] digest_0.6.36     rlang_1.1.4       evaluate_0.24.0"
  },
  {
    "objectID": "posts/2025-01-10_model_function/model_function.html",
    "href": "posts/2025-01-10_model_function/model_function.html",
    "title": "Models - Condensed code and output",
    "section": "",
    "text": "We are constantly working on the appearance of the reports we generate for our colleagues and collaborators. This includes managing .html and .docx output for different use cases, all while navigating the move from R Markdown to Quarto. At this stage, we have all but completely abandoned the idea of rendering directly to .pdf as we find .docx more friendly (tracking changes with collaborators, certain output manipulating functions) and it is readily ‘pdf-able’ - but more on all that in a future post.\nOur primary report output format is .html and we have been looking for ways to harness certain Quarto features to improve both the aesthetic of our reports as well as our use of real estate. Often, we are presenting output for a range of models separated by narration. It is important that all models are presented with sufficient context (as opposed to output just dumped on a page) and we’ve settled on what we think is a nice way to ensure relevant information (like model diagnostics) are readily available (and digestable) to the reader, as opposed to being tucked away at the end - or worse - left out altogether.\nBelow, we present the way we currently present output from a linear regression in our .html reports."
  },
  {
    "objectID": "posts/2025-01-10_model_function/model_function.html#presentation-overview",
    "href": "posts/2025-01-10_model_function/model_function.html#presentation-overview",
    "title": "Models - Condensed code and output",
    "section": "Presentation overview",
    "text": "Presentation overview\nWe use a combination of tabset panels (implemented with .panel-tabset) and margin content (implemented with .column-margin) to achieve what we think is a nice balance between real estate utilisation and content compartmentalisation."
  },
  {
    "objectID": "posts/2025-01-10_model_function/model_function.html#data-for-the-demonstration",
    "href": "posts/2025-01-10_model_function/model_function.html#data-for-the-demonstration",
    "title": "Models - Condensed code and output",
    "section": "Data for the demonstration",
    "text": "Data for the demonstration\nTo demonstrate, we will use a slightly modified dataset of Birth Weight data (read more with ?MASS::birthwt) from the MASS package (MASS::birthwt).\nThe dataset has 189 observations (rows) and in the code below we just tidy up some variables prior to running the model.\n\n\nCode\ndat_bwt &lt;- MASS::birthwt\ndat_bwt &lt;- dat_bwt %&gt;% \n  tibble() %&gt;% \n  mutate(smoke = factor(case_when(smoke == 1 ~ \"Yes\",\n                                  smoke == 0 ~ \"No\",\n                                  T ~ NA_character_)),\n         ht = factor(case_when(ht == 1 ~ \"Yes\",\n                               ht == 0 ~ \"No\",\n                               T ~ NA_character_)))\n\n\nWe then implement the following linear regression (lm) model:\n\nOutcome: infant birth weight (bwt).\nExposure of interest: maternal smoking status during pregnancy (smoke).\nCovariates: maternal age (age) and history of hypertension (ht)."
  },
  {
    "objectID": "posts/2025-01-10_model_function/model_function.html#demonstration",
    "href": "posts/2025-01-10_model_function/model_function.html#demonstration",
    "title": "Models - Condensed code and output",
    "section": "Demonstration",
    "text": "Demonstration\n\nRunning the function\nWe can run our function using the code below:\n\nmod_bwt &lt;- dat_bwt %&gt;% \n  thekids_model(y = \"bwt\", x = \"smoke\", formula = \"age + ht\")\n\nIn this, we have passed (piped) the data into the first argument, specified the outcome variable in the next argument, the exposure of interest in the next argument, and finally the remainder of the models formula (that is, the covariates we are looking to have in our model).\nAlternatively, we could run our model in the ‘normal’ way and pass it to our output processing function - which is really the workhorse of the above function - and this would look something like the following:\n\nmy_model &lt;- lm(bwt ~ smoke + age + ht, data = dat_bwt)\nthekids_model_output(my_model, by = \"smoke\") # still need to specify our exposure of interest\n\nThe objects defined above (mod_bwt, my_model) are lists that contain outputs relating to our selected model type.\nNow, we see how the output would appear in an .html report.\n\n\nFunction output\n\nDesc statsModel diagModel outputModel effect\n\n\nThe table below shows summary statistics for all variables in the model by the primary exposure variable (maternal smoking status in pregnancy).\n\n\nsmokeNo  N = 1151Yes  N = 741p-value2bwt3,056 (753) [115]2,772 (660) [74]0.007age23 (5) [115]23 (5) [74]0.5ht7 (6.1%)5 (6.8%)&gt;0.91Mean (SD) [N Non-missing]; n (%)2Wilcoxon rank sum test; Fisher's exact test\n\n\nThe figure below shows the distribution of the primary outcome variable (infant birth weight) by the primary exposure variable (maternal smoking status in pregnancy).\n\n\n\n\n\n\n\n\n\n\n\nThe four panel plot below shows diagnostic plots that can aid in determining if the required assumptions of model are met.\nBased on the below, the model fit is deemed to be good.\n\n\n\n\n\n\n\n\n\nBroadly, what we are looking for (and why) in these plots are:\n\n(top-left) relatively even bands of points around a flat line at 0 as we move from left to right | linear relationship check,\n(top right) the points to fall close to the diagonal line | normal distribution of errors check,\n(bottom-right) the points to funnel close to 0 as we move from left to right with no extreme values in the top right or bottom right corners | influential observations check, and\n(bottom-left) checking residuals are relatively evenly spread across the range of predictions | homoscedasticity check.\n\n\n\nThe table below shows the model output for the linear regression model, including the beta coefficient (and 95% confidence interval) and p-value associated with each variable in the model.\n\n\nCharacteristicBetap-value(Intercept)2,824.1 (2,351.4, 3,296.7)0.000smokeNo—Yes-275.7 (-485.1, -66.2)0.010age11.0 (-8.4, 30.3)0.264htNo—Yes-424.2 (-843.0, -5.4)0.047\n\n\n\n\nThe figure and table below show the predicted value (also known as the estimated marginal mean) for the outcome variable (infant birth weight) for each level of the exposure of interest (here, no maternal smoking in pregnancy or maternal smoking in pregnancy), along with a 95% confidence interval. Note, see table footnote for the values used for the other variables in the model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        Predicted values of bwt\n              \n                smoke\n                Predicted\n                95% CI\n              \n        \n        Adjusted for: age = 23.00,  ht =    No\n        \n                \n                  No \n                  3076.84\n                  2943.20, 3210.47\n                \n                \n                  Yes\n                  2801.18\n                  2635.57, 2966.79\n                \n        \n      \n    \n\n\n\n\n\n\n\n\nSummary\n\nThere is evidence to suggest that smoking during pregnancy is associated with reduced birth weight.\nMaternal smoking during pregnancy is associated with a -275.66g lower birth weight (95% CI: -485.1,-66.2).\n\nThe code to generate this can be viewed below:\n\n\nCode\n# ::: panel-tabset\n# ## Desc stats\n# \n# The table below shows summary statistics for all variables in the model by the primary exposure variable (maternal smoking status in pregnancy).\n# \n# ```{r}\n# mod_bwt$mod_desc %&gt;% \n#   thekids_table(colour = \"DarkTeal\",\n#                 padding.left = 10, padding.right = 10)\n# ```\n# \n# The figure below shows the distribution of the primary outcome variable (infant birth weight) by the primary exposure variable (maternal smoking status in pregnancy).\n# \n# ```{r fig.height=5, fig.width=5, fig.align='center'}\n# mod_bwt$mod_desc_plot\n# ```\n# \n# ## Model diag\n# \n# The four panel plot below shows diagnostic plots that can aid in determining if the required assumptions of model are met.\n# \n# **Based on the below,** the model fit is deemed to be good. \n# \n# ```{r fig.height=6, fig.width=6, fig.align='center'}\n# mod_bwt$mod_diag\n# ```\n# \n# Broadly, what we are looking for (and why) in these plots are: \n# \n# + *(top-left)* relatively even bands of points around a flat line at 0 as we move from left to right | linear relationship check, \n# + *(top right)* the points to fall close to the diagonal line | normal distribution of errors check, \n# + *(bottom-right)* the points to funnel close to 0 as we move from left to right with no extreme values in the top right or bottom right corners | influential observations check, and \n# + *(bottom-left)* checking residuals are relatively evenly spread across the range of predictions | homoscedasticity check.\n# \n# ## Model output\n# \n# The table below shows the model output for the linear regression model, including the beta coefficient (and 95% confidence interval) and p-value associated with each variable in the model.\n# \n# ```{r}\n# mod_bwt$mod_output %&gt;% \n#   thekids_table(colour = \"DarkTeal\",\n#                 padding.left = 10, padding.right = 10)\n# ```\n# \n# ## Model effect\n# \n# The figure and table below show the predicted value (also known as the estimated marginal mean) for the outcome variable (infant birth weight) for each level of the exposure of interest (here, no maternal smoking in pregnancy or maternal smoking in pregnancy), along with a 95% confidence interval. Note, see table footnote for the values used for the other variables in the model.\n# \n# ```{r fig.height=4.5, fig.width=5, fig.align='center'}\n# mod_bwt$mod %&gt;% \n#   ggeffects::predict_response(\"smoke\") %&gt;% \n#   plot\n# ```\n# \n# ```{r}\n# mod_bwt$mod %&gt;% \n#   ggeffects::predict_response(\"smoke\") %&gt;% \n#   ggeffects::print_html()\n# ```\n# :::\n# \n# ::: column-margin\n# **Summary**\n# \n# + There is evidence to suggest that smoking during pregnancy is associated with reduced birth weight.\n# + Maternal smoking during pregnancy is associated with a `r round(coef(mod_bwt$mod)[2],2)`g lower birth weight (95% CI: `r round(confint(mod_bwt$mod)[2,1],1)`,`r round(confint(mod_bwt$mod)[2,2],1)`).\n# :::\n\n\n\n\nIf you copy all this code, you can then select all, and use ctrl+C (or command+c) to uncomment the code."
  },
  {
    "objectID": "posts/2025-01-10_model_function/model_function.html#pros-and-cons",
    "href": "posts/2025-01-10_model_function/model_function.html#pros-and-cons",
    "title": "Models - Condensed code and output",
    "section": "Pros and cons",
    "text": "Pros and cons\nThe motivation behind this was really to reduce the length of code in our Quarto reports and to reduce the scroll in our rendered .html reports, without having to hideaway important information (like diagnostics) in a supplementary report, along with forcing (is that too strong?) the reader to engage with the raw data (and it’s distribution) a little before just seeing the model output.\nWe acknowledge this is still a work in progress.\n\nPros\n\nConcise code and concise report output\nMaintain ready access to the model and the data (for additional processing, if required)\nReduce indecision (ad nauseam) over how to format your model output (which package(s) to use)\n\n\n\nCons\n\nMultiple copies of the dataset are created, which will be problematic with large data\nLimited functionality as it relates to different (more complex) model types\nYet another function to learn and engage with\n\nOur view, again as it speaks to the motivation here, is that we often run these sorts of models - which may not even be our final model - but may be informative on the pathway to specifying the final model. And, along the way, there is a lot value in having all of this output structured and readily available."
  },
  {
    "objectID": "posts/2025-01-10_model_function/model_function.html#accessing-the-function",
    "href": "posts/2025-01-10_model_function/model_function.html#accessing-the-function",
    "title": "Models - Condensed code and output",
    "section": "Accessing the function",
    "text": "Accessing the function\nThe above two functions (thekids_model and thekids_model_output) are now available in our package thekidsbiostats, which you can install by running the following code:\n\nremotes::install_github(\"The-Kids-Biostats/thekidsbiostats\")\n\nThe help documentation (?thekids_model) is there to walk you through the relevant arguments."
  },
  {
    "objectID": "posts/2025-01-10_model_function/model_function.html#reproducibility-information",
    "href": "posts/2025-01-10_model_function/model_function.html#reproducibility-information",
    "title": "Models - Condensed code and output",
    "section": "Reproducibility Information",
    "text": "Reproducibility Information\nTo access the .qmd (Quarto markdown) files as well as any R scripts or data that was used in this post, please visit our GitHub:\nhttps://github.com/The-Kids-Biostats/The-Kids-Biostats.github.io/tree/main/posts/\nThe session information can also be seen below.\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.1 (2024-06-14 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_Australia.utf8  LC_CTYPE=English_Australia.utf8   \n[3] LC_MONETARY=English_Australia.utf8 LC_NUMERIC=C                      \n[5] LC_TIME=English_Australia.utf8    \n\ntime zone: Australia/Perth\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] thekidsbiostats_0.0.1 flextable_0.9.7       gtsummary_2.0.4      \n [4] lubridate_1.9.4       forcats_1.0.0         stringr_1.5.1        \n [7] dplyr_1.1.4           purrr_1.0.2           readr_2.1.5          \n[10] tidyr_1.3.1           tibble_3.2.1          ggplot2_3.5.1        \n[13] tidyverse_2.0.0       extrafont_0.19       \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1        sjlabelled_1.2.0        viridisLite_0.4.2      \n [4] ggfortify_0.4.17        farver_2.1.2            fastmap_1.2.0          \n [7] fontquiver_0.2.1        janitor_2.2.1           broom.helpers_1.17.0   \n[10] labelled_2.14.0         digest_0.6.37           timechange_0.3.0       \n[13] lifecycle_1.0.4         magrittr_2.0.3          compiler_4.4.1         \n[16] rlang_1.1.4             tools_4.4.1             igraph_2.1.2           \n[19] yaml_2.3.10             data.table_1.16.4       knitr_1.49             \n[22] askpass_1.2.1           labeling_0.4.3          htmlwidgets_1.6.4      \n[25] xml2_1.3.6              tinytable_0.6.1         withr_3.0.2            \n[28] grid_4.4.1              datawizard_0.13.0       fansi_1.0.6            \n[31] gdtools_0.4.1           colorspace_2.1-1        extrafontdb_1.0        \n[34] scales_1.3.0            MASS_7.3-64             insight_1.0.0          \n[37] cli_3.6.3               rmarkdown_2.29          ragg_1.3.3             \n[40] generics_0.1.3          rstudioapi_0.17.1       tzdb_0.4.0             \n[43] vctrs_0.6.5             jsonlite_1.8.9          fontBitstreamVera_0.1.1\n[46] hms_1.1.3               systemfonts_1.1.0       glue_1.8.0             \n[49] stringi_1.8.4           gtable_0.3.6            ggeffects_2.0.0        \n[52] munsell_0.5.1           pillar_1.10.1           htmltools_0.5.8.1      \n[55] biometrics_1.2.4        openssl_2.2.2           R6_2.5.1               \n[58] textshaping_0.4.1       evaluate_1.0.1          kableExtra_1.4.0       \n[61] haven_2.5.4             backports_1.5.0         cards_0.4.0            \n[64] broom_1.0.7             snakecase_0.11.1        fontLiberation_0.1.0   \n[67] Rcpp_1.0.13-1           cardx_0.2.2             zip_2.3.1              \n[70] uuid_1.2-1              svglite_2.1.3           gridExtra_2.3          \n[73] Rttf2pt1_1.3.12         officer_0.6.7           xfun_0.49              \n[76] pkgconfig_2.0.3"
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#for-loop",
    "href": "posts/2024-07-01_parallel/parallel.html#for-loop",
    "title": "Parallel Computing in R",
    "section": "For-loop",
    "text": "For-loop\n\nLet’s begin by using a traditional for-loop. For each bootstrap sample, we:\n\nInitialise a bootstrap sample, ind.\nRun our linear regression on the bootstrap sample, results\nExtract the coefficients from results, and append this to an overall coefficient matrix bootstrap_coefs.\n\nSubsequently, we calculate a 95% confidence interval for each of our parameter estimates by taking the 2.5th and 97.5th percentiles from the bootstrap distribution and calling this bootstrap_cis.\n\n\n\nCode\nstart &lt;- proc.time() # Start our timer!\n\n# Initialise a matrix to store the coefficients from each bootstrap sample\nbootstrap_coefs &lt;- matrix(NA, nrow = trials, ncol = 4)\ncolnames(bootstrap_coefs) &lt;- names(coef(lm(mpg ~ hp + wt + am, data = mtcars)))\n\nfor (i in 1:trials){\n  \n  # Take bootstrap sample\n  ind &lt;- mtcars[sample(nrow(mtcars), \n                       replace = TRUE),\n                ]\n  \n  # Construct linear regression\n  result &lt;- lm(mpg ~ hp + wt + as_factor(am), \n               data = ind)\n  \n  # Extract coefficients and store to `bootstrap_coefs`\n  bootstrap_coefs[i, ] &lt;- coef(result)\n}\n\n# Calculate the 2.5th and 97.5th percentile for each variable's coefficient estimates from the bootstrap distribution.\nbootstrap_cis &lt;- apply(bootstrap_coefs, \n                       2, \n                       function(coefs){quantile(coefs, probs = c(0.025, 0.975))})\n\nend &lt;- proc.time() # End our timer!\ntime1 &lt;- end-start\n\n\n\nLet’s visualise the first few lines of the bootstrapped results\n\n\nCode\nhead(bootstrap_coefs)\n\n\n     (Intercept)          hp        wt         am\n[1,]    34.04596 -0.02741585 -3.158154  0.6330600\n[2,]    31.46670 -0.03097903 -2.375727  5.8142235\n[3,]    35.98084 -0.02535775 -3.763464 -0.2485866\n[4,]    33.47330 -0.04410244 -2.343210  2.6890689\n[5,]    32.21798 -0.04138935 -2.222471  1.2289610\n[6,]    32.78747 -0.02758182 -2.989311  1.1744731\n\n\nand associated 95% confidence interval\n\n\nCode\nbootstrap_cis\n\n\n      (Intercept)          hp        wt         am\n2.5%     29.07843 -0.05539954 -5.134682 -0.7627477\n97.5%    40.74463 -0.02161861 -1.057830  4.9428501\n\n\n\nThis had the following run-time (seconds):\n\n\nCode\nprint(time1)\n\n\n   user  system elapsed \n  4.201   0.014   4.309 \n\n\n\n\n\n\n\n\nproc.time components\n\n\n\n\nuser = time the CPU has spent executing the R process.\nsystem = time the CPU has spent on system-level operations that facilitate the R process (e.g., memory management, system calls).\nelapsed = real-world time that has elapsed."
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#do-loop-not-parallel",
    "href": "posts/2024-07-01_parallel/parallel.html#do-loop-not-parallel",
    "title": "Parallel Computing in R",
    "section": "%do% loop (not parallel)",
    "text": "%do% loop (not parallel)\nAs an alternative, let’s also use the %do% operator from the foreach package. Similar to a for-loop, each bootstrap sample is executed sequentially.\n\n\nCode\nstart &lt;- proc.time()\n\nbootstrap_coefs &lt;- foreach::foreach(i = 1:trials, .combine = rbind) %do% {\n  ind &lt;- mtcars[sample(nrow(mtcars), replace = TRUE), ]\n  result &lt;- lm(mpg ~ hp + wt + am, data = ind)\n  coef(result)\n}\n\n# Calculate the 2.5th and 97.5th percentile for each variable's coefficient estimates from the bootstrap distribution.\nbootstrap_cis &lt;- apply(bootstrap_coefs, \n                       2, \n                       function(coefs) {quantile(coefs, probs = c(0.025, 0.975))})\n\nend &lt;- proc.time()\ntime2 &lt;- end-start\n\n\n\nSimilarly, let’s visualise the first few lines of the bootstrapped results\n\n\nCode\nhead(bootstrap_coefs)\n\n\n         (Intercept)          hp        wt       am\nresult.1    32.28476 -0.02807558 -2.989010 2.404865\nresult.2    38.19523 -0.02740136 -4.576854 1.082726\nresult.3    32.87519 -0.06693216 -1.172027 3.879952\nresult.4    29.54068 -0.03806135 -1.468158 1.933494\nresult.5    31.54392 -0.02793433 -2.756060 1.347816\nresult.6    34.42623 -0.03900395 -2.865725 1.167583\n\n\nand associated 95% confidence interval\n\n\nCode\nbootstrap_cis\n\n\n      (Intercept)          hp        wt         am\n2.5%     29.03455 -0.05615444 -5.286909 -0.8228096\n97.5%    41.09610 -0.02124169 -1.055386  4.9293286\n\n\n\nThis had the following run-time (seconds):\n\n\nCode\nprint(time2)\n\n\n   user  system elapsed \n  3.624   0.009   3.672"
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#dopar-parallelisation",
    "href": "posts/2024-07-01_parallel/parallel.html#dopar-parallelisation",
    "title": "Parallel Computing in R",
    "section": "%dopar% parallelisation",
    "text": "%dopar% parallelisation\nNow, let’s run this in parallel across 6 cores. The %dopar% operator defines the for-loop in the parallel environment.\n\n\nCode\ndoParallel::registerDoParallel(cores = 6) # Initialise parallel cluster\n\nstart &lt;- proc.time()\nbootstrap_coefs &lt;- foreach(i = 1:trials, .combine = rbind, .packages = 'stats') %dopar% {\n  ind &lt;- mtcars[sample(nrow(mtcars), replace = TRUE), ]\n  result &lt;- lm(mpg ~ hp + wt + am, data = ind)\n  coef(result)\n}\n\n# Calculate the 2.5th and 97.5th percentile for each variable's coefficient estimates from the bootstrap distribution.\nbootstrap_cis &lt;- apply(bootstrap_coefs, \n                       2, \n                       function(coefs) {quantile(coefs, probs = c(0.025, 0.975))})\n\nend &lt;- proc.time()\ntime3 &lt;- end-start\n\ndoParallel::stopImplicitCluster() # De-register parallel cluster\n\n\nAs expected, the output of the bootstrapped coefficient distribution are identical before\n\n\nCode\nhead(bootstrap_coefs)\n\n\n         (Intercept)          hp        wt         am\nresult.1    40.28800 -0.02495526 -5.378558  1.0705520\nresult.2    35.76160 -0.03754365 -3.259903  2.3125147\nresult.3    34.22544 -0.04007194 -2.705909  2.4514809\nresult.4    33.92515 -0.03215035 -3.096706  1.1716197\nresult.5    35.84923 -0.01510045 -4.301475 -0.4492253\nresult.6    30.77413 -0.04316729 -1.616423  2.6139924\n\n\nas are the associated 95% confidence intervals.\n\n\nCode\nbootstrap_cis\n\n\n      (Intercept)          hp        wt         am\n2.5%     29.09744 -0.05537917 -5.151840 -0.8456313\n97.5%    40.98867 -0.02161646 -1.085433  4.8985324\n\n\nLastly, this had the following run-time (seconds)\n\n\nCode\nprint(time3)\n\n\n   user  system elapsed \n  4.555   0.273   1.318"
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#discussion",
    "href": "posts/2024-07-01_parallel/parallel.html#discussion",
    "title": "Parallel Computing in R",
    "section": "Discussion",
    "text": "Discussion\nImmediately, the syntax of the alternative for-loop structures are more readable and easier to construct than the traditional for-loop. Because the foreach::foreach function easily combines output in a list, we need not define an empty matrix to append output to.\nComputation time in the parallel environment is significantly faster — approximately 69% faster than the traditional for-loop! Across multiple analyses and data sets, these time savings certainly add up!"
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#data",
    "href": "posts/2024-07-01_parallel/parallel.html#data",
    "title": "Parallel Computing in R",
    "section": "Data",
    "text": "Data\nFirst, let’s simulate our data set and set some parameters:\n\n10,000 observations.\nIndependent variables temperature, precipitation and elevation sampled from a random normal distribution and vegetation type (categorical factor) randomly prescribed.\nSpecies presence (dichotomous) outcome variable is randomly prescribed.\n\n\n\nCode\nn &lt;- 10000 # Sample size\n\ndata &lt;- data.frame(temperature = rnorm(n, \n                                       mean = 15, \n                                       sd   = 40),\n                   precipitation = rnorm(n, \n                                         mean = 1000, \n                                         sd   = 300),\n                   elevation = rnorm(n, \n                                     mean = 500, \n                                     sd   = 200),\n                   vegetation_type = as_factor(sample(c(\"forest\", \n                                                        \"grassland\", \n                                                        \"wetland\", \n                                                        \"desert\"), \n                                                      n, \n                                                      replace = T)),\n                   species_presence = as_factor(sample(c(\"present\", \n                                                         \"absent\"), \n                                                       n, \n                                                       replace = T)))\n\n\nLet’s assign 70% of the data to our training set, and the remaining 30% to test data set and initialise a random forest model with 1000 trees.\n\n\nCode\ntrain_index &lt;- sample(1:n, 0.7*n)\n\ntrain_data &lt;- data[train_index, ]\ntest_data &lt;- data[-train_index, ]\n\nnum_trees &lt;- 1000\n\n\n\nInstead of running one random forest model comprising 1000 trees, let’s combine the results of 4 smaller random forest models models each comprising 250 trees. By doing this, we can return more reliable and robust output (smaller random forest models are less prone to overfitting) and better manage working memory (smaller models require less memory to train and store)."
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#for-loop-1",
    "href": "posts/2024-07-01_parallel/parallel.html#for-loop-1",
    "title": "Parallel Computing in R",
    "section": "For-loop",
    "text": "For-loop\n\n\nCode\nstart &lt;- proc.time()\nrf &lt;- list()\nfor (i in 1:(num_trees/250)){\n\n  rf[[i]] &lt;- randomForest::randomForest(species_presence ~ ., \n                                        data = train_data, \n                                        ntree = num_trees/4)\n}\n\ncombined_output &lt;- do.call(randomForest::combine, rf)\n\npredictions &lt;- predict(combined_output, test_data %&gt;% select(-species_presence))\n\nend &lt;- proc.time()\ntime1 &lt;- end - start\n\n\nLet’s print the confusion matrix of this output. Because this data was relatively simply simulated, we don’t expect the predictive power to be too great.\n\n\nCode\ntable(predictions, test_data$species_presence)\n\n\n           \npredictions absent present\n    absent     707     769\n    present    779     745\n\n\nThis has the following run-time (seconds):\n\n\nCode\nprint(time1)\n\n\n   user  system elapsed \n  5.929   0.159   6.203"
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#do-loop-not-parallel-1",
    "href": "posts/2024-07-01_parallel/parallel.html#do-loop-not-parallel-1",
    "title": "Parallel Computing in R",
    "section": "%do% loop (not parallel)",
    "text": "%do% loop (not parallel)\nSimilar to the traditional for-loop, we can sequentially execute this code using the %do% operator.\n\n\nCode\nstart &lt;- proc.time()\nrf &lt;- foreach::foreach(ntree = rep(num_trees/4, 4), \n                       .combine = randomForest::combine, \n                       .packages = 'randomForest') %do%\n  randomForest::randomForest(species_presence ~ ., \n                             data = train_data,\n                             ntree = ntree)\n\npredictions &lt;- predict(rf, test_data %&gt;% select(-species_presence))\n\nend &lt;- proc.time()\ntime2 &lt;- end - start\n\n\nLet’s print the confusion matrix of this output. Because this data was relatively simply simulated, we don’t expect the predictive power to be too great.\n\n\nCode\ntable(predictions, test_data$species_presence)\n\n\n           \npredictions absent present\n    absent     699     777\n    present    787     737\n\n\nThis has the following run-time (seconds):\n\n\nCode\nprint(time2)\n\n\n   user  system elapsed \n  5.768   0.219   6.057"
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#dopar-parallelisation-1",
    "href": "posts/2024-07-01_parallel/parallel.html#dopar-parallelisation-1",
    "title": "Parallel Computing in R",
    "section": "%dopar% parallelisation",
    "text": "%dopar% parallelisation\nFor simplicity, let’s allocate 4 cores to the computation and imagine that one core is responsible for processing one of the four random forest models simultaneously.\n\n\nCode\ndoParallel::registerDoParallel(cores = 4)\n\nstart &lt;- proc.time()\nrf &lt;- foreach::foreach(ntree = rep(num_trees/4, 4), \n                       .combine = randomForest::combine, \n                       .packages = 'randomForest') %dopar%\n  randomForest::randomForest(species_presence ~ ., \n                             data = train_data,\n                             ntree = ntree)\n\npredictions &lt;- predict(rf, test_data %&gt;% select(-species_presence))\n\nend &lt;- proc.time()\ndoParallel::stopImplicitCluster()\n\ntime3 &lt;- end-start\n\n\nLet’s print the confusion matrix of this output. Because this data was relatively simply simulated, we don’t expect the predictive power to be too great.\n\n\nCode\ntable(predictions, test_data$species_presence)\n\n\n           \npredictions absent present\n    absent     705     775\n    present    781     739\n\n\nThis now has the following run-time (seconds):\n\n\nCode\nprint(time3)\n\n\n   user  system elapsed \n  6.237   0.796   3.253"
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#discussion-1",
    "href": "posts/2024-07-01_parallel/parallel.html#discussion-1",
    "title": "Parallel Computing in R",
    "section": "Discussion",
    "text": "Discussion\nAgain, using easily adaptable and readable syntax, we leverage a parallel environment to significantly lessen the computation time of our large model. Relative to a standard for-loop, the parallelised computation is approximately 48% faster."
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#data-1",
    "href": "posts/2024-07-01_parallel/parallel.html#data-1",
    "title": "Parallel Computing in R",
    "section": "Data",
    "text": "Data\n\nLet’s use nycflights13::flights — a set of over 300,000 flight records that departed from all NYC airports in 2013.\nWe would like to explore how arrival delay (as a continuous and dichotomous (delayed = 1, not delayed = 0) outcome variable) may be influenced by a set of independent variables. We would like to stratify this by month.\n\n\n\nCode\nflights &lt;- nycflights13::flights\nflights &lt;- flights %&gt;%\n  select(year, day, dep_delay, arr_delay, air_time, distance) %&gt;%\n  mutate(arr_delay_bin = as.factor(case_when(arr_delay &gt;  15 ~ 1, TRUE ~ 0)))\n\nflights\n\n\n# A tibble: 336,776 × 7\n    year   day dep_delay arr_delay air_time distance arr_delay_bin\n   &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;        \n 1  2013     1         2        11      227     1400 0            \n 2  2013     1         4        20      227     1416 1            \n 3  2013     1         2        33      160     1089 1            \n 4  2013     1        -1       -18      183     1576 0            \n 5  2013     1        -6       -25      116      762 0            \n 6  2013     1        -4        12      150      719 0            \n 7  2013     1        -5        19      158     1065 1            \n 8  2013     1        -3       -14       53      229 0            \n 9  2013     1        -3        -8      140      944 0            \n10  2013     1        -2         8      138      733 0            \n# ℹ 336,766 more rows\n\n\n\nWe would like to specify a set of models which predict overall flight delay, as both continuous (arrival delay time) and dichotomous (delayed yes/no) outcomes.\n\nOutcome variables\n\nArrival delay (continuous)\nArrival delayed (dichotomous)\n\nIndependent variables\n\nFlight distance (distance)\nAir time (air_time)\nDeparture delay (dep_delay)\n\n\n\n\nCode\nindep_vars &lt;- c(\"distance\", \"air_time\", \"dep_delay\")\noutcome_vars &lt;- c(\"arr_delay\", \"arr_delay_bin\")\n\n\nFor each outcome variable, we run a model. If the outcome variable is continuous, we run a simple linear model; otherwise we run a basic logistic regression."
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#for-loop-2",
    "href": "posts/2024-07-01_parallel/parallel.html#for-loop-2",
    "title": "Parallel Computing in R",
    "section": "For-loop",
    "text": "For-loop\n\n\nCode\nstart &lt;- proc.time()\nmodels &lt;- list() # To store our model output\n\nfor (i in outcome_vars){\n  if (i == \"arr_delay\"){\n    model &lt;- lm(as.formula(paste(i,\"~\",paste(indep_vars, collapse = \"+\"))),\n                data = flights)\n    \n  } else if (i == \"arr_delay_bin\"){\n    model &lt;- glm(as.formula(paste(i,\"~\",paste(indep_vars, collapse = \"+\"))),\n                 family = binomial,\n                 data = flights)\n  }\n  models[[i]] &lt;- summary(model)\n}\n\nend &lt;- proc.time()\ntime1 &lt;- end-start\n\n\nThis returns a list with the model output summary for each of our models.\nThe for-loop has the following run-time (seconds):\n\n\nCode\nprint(time1)\n\n\n   user  system elapsed \n  0.495   0.071   0.657"
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#purrrmap",
    "href": "posts/2024-07-01_parallel/parallel.html#purrrmap",
    "title": "Parallel Computing in R",
    "section": "purrr::map",
    "text": "purrr::map\nMap functions apply a function to each element of a list/vector and return an object. In cases relying on multiple computations across different values, they often come in handy.\n\n\nCode\nstart &lt;- proc.time()\n\n\nmodels &lt;- map(outcome_vars, \n      ~ if (.x == \"arr_delay\"){\n        lm(as.formula(paste(.x, \"~\", \n                            paste(indep_vars, collapse = \" + \"))),\n           data = flights) %&gt;%\n          summary()\n        \n        } else if (.x == \"arr_delay_bin\"){\n          glm(as.formula(paste(.x, \"~\",\n                               paste(indep_vars, collapse = \" + \"))),\n              family = binomial,\n              data = flights) %&gt;%\n            summary()\n        }\n      )\nnames(models) &lt;- outcome_vars\n\nend &lt;- proc.time()\ntime2 &lt;- end-start\n\n\nThis has the following run-time (seconds):\n\n\nCode\nprint(time2)\n\n\n   user  system elapsed \n  0.476   0.055   0.544"
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#furrrfuture_map",
    "href": "posts/2024-07-01_parallel/parallel.html#furrrfuture_map",
    "title": "Parallel Computing in R",
    "section": "furrr::future_map",
    "text": "furrr::future_map\nThere is also a parallel implementation of the purrr::map function, offered by the furrr package. The syntax is (nicely) identical to above, but importantly relies on specifying a parallel (multisession) “plan” ahead of executing the code (similar to what we did in Example 1 and 2).\n\n\nCode\nlibrary(furrr)\n\nplan(multisession, workers = 6) # Initialise parallel environment using furrr\n\nstart &lt;- proc.time()\nmodels &lt;- furrr::future_map(outcome_vars, \n      ~ if (.x == \"arr_delay\"){\n        lm(as.formula(paste(.x, \"~\", \n                            paste(indep_vars, collapse = \" + \"))),\n           data = flights) %&gt;%\n          summary()\n        \n        } else if (.x == \"arr_delay_bin\"){\n          glm(as.formula(paste(.x, \"~\",\n                               paste(indep_vars, collapse = \" + \"))),\n              family = binomial,\n              data = flights) %&gt;%\n            summary()\n        }\n      )\nnames(models) &lt;- outcome_vars\n\nend &lt;- proc.time()\ntime3 &lt;- end-start\n\nplan(sequential) # Revert to sequential processing\n\n\nThis has the following run-time (seconds):\n\n\nCode\nprint(time3)\n\n\n   user  system elapsed \n  0.087   0.012   0.888"
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#dopar-parallelisation-2",
    "href": "posts/2024-07-01_parallel/parallel.html#dopar-parallelisation-2",
    "title": "Parallel Computing in R",
    "section": "%dopar% parallelisation",
    "text": "%dopar% parallelisation\nAlternatively, as done earlier, we can turn our non-parallel %do% code into parallel %dopar% code.\n\nWe use the %:% operator from the foreach package to nest a for-loop within a parallel environment.\nThe syntax does not differ too dramatically.\n\n\n\nCode\ndoParallel::registerDoParallel(cores = 6)\n\nstart &lt;- proc.time()\nmodels &lt;- foreach(j = outcome_vars, .combine = \"list\") %dopar% {\n  if (j == \"arr_delay\"){\n    model &lt;- lm(as.formula(paste(j,\"~\",paste(indep_vars, collapse = \"+\"))),\n                data = flights)\n  } else if (j == \"arr_delay_bin\"){\n    model &lt;- glm(as.formula(paste(j,\"~\",paste(indep_vars, collapse = \"+\"))),\n                 family = binomial,\n                 data = flights)\n  }\n}\nnames(models) &lt;- outcome_vars\n\nend &lt;- proc.time()\n\ntime4 &lt;- end - start\ndoParallel::stopImplicitCluster()\n\n\nThis has the following run-time (seconds):\n\n\nCode\nprint(time4)\n\n\n   user  system elapsed \n  0.124   0.136   0.701"
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#discussion-2",
    "href": "posts/2024-07-01_parallel/parallel.html#discussion-2",
    "title": "Parallel Computing in R",
    "section": "Discussion",
    "text": "Discussion\nWe now see that the parallel processing of these tasks takes far longer – about 6% so! The underlying set of operations — running a series of linear models — are already small and relatively fast, so the overhead of managing the task (splitting, computing and combining results) in a parallel environment far exceeds what what can easily be spun up using a for-loop (or purrr::map)."
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#acknowledgements",
    "href": "posts/2024-07-01_parallel/parallel.html#acknowledgements",
    "title": "Parallel Computing in R",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Wesley Billingham, Matt Cooper, and Elizabeth McKinnon for providing feedback on and reviewing this post.\nYou can look forward to seeing posts from these other team members here in the coming weeks and months."
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#reproducibility-information",
    "href": "posts/2024-07-01_parallel/parallel.html#reproducibility-information",
    "title": "Parallel Computing in R",
    "section": "Reproducibility Information",
    "text": "Reproducibility Information\nTo access the .qmd (Quarto markdown) files as well as any R scripts or data that was used in this post, please visit our GitHub:\nhttps://github.com/The-Kids-Biostats/The-Kids-Biostats.github.io/tree/main/posts/parallel\nThe session information can also be seen below.\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.0 (2024-04-24)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.5\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Australia/Perth\ntzcode source: internal\n\nattached base packages:\n[1] parallel  stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] furrr_0.3.1          future_1.33.2        randomForest_4.7-1.1\n [4] doParallel_1.0.17    iterators_1.0.14     foreach_1.5.2       \n [7] lubridate_1.9.3      forcats_1.0.0        stringr_1.5.1       \n[10] dplyr_1.1.4          purrr_1.0.2          readr_2.1.5         \n[13] tidyr_1.3.1          tibble_3.2.1         ggplot2_3.5.1       \n[16] tidyverse_2.0.0     \n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.4         generics_0.1.3     stringi_1.8.4      listenv_0.9.1     \n [5] hms_1.1.3          digest_0.6.36      magrittr_2.0.3     evaluate_0.24.0   \n [9] grid_4.4.0         timechange_0.3.0   fastmap_1.2.0      jsonlite_1.8.8    \n[13] fansi_1.0.6        scales_1.3.0       codetools_0.2-20   cli_3.6.3         \n[17] rlang_1.1.4        parallelly_1.37.1  munsell_0.5.1      withr_3.0.0       \n[21] yaml_2.3.9         tools_4.4.0        tzdb_0.4.0         colorspace_2.1-0  \n[25] globals_0.16.3     vctrs_0.6.5        R6_2.5.1           lifecycle_1.0.4   \n[29] htmlwidgets_1.6.4  pkgconfig_2.0.3    pillar_1.9.0       gtable_0.3.5      \n[33] glue_1.7.0         xfun_0.46          tidyselect_1.2.1   rstudioapi_0.16.0 \n[37] knitr_1.48         htmltools_0.5.8.1  rmarkdown_2.27     nycflights13_1.0.2\n[41] compiler_4.4.0"
  },
  {
    "objectID": "posts/2024-06-07_lmer-missing/missing_lmer.html",
    "href": "posts/2024-06-07_lmer-missing/missing_lmer.html",
    "title": "Handling Missing Data with LMER",
    "section": "",
    "text": "As consultant statisticians, we are often approached by people who have already carried out some preliminary data analysis and who are now looking to move onto something more complex. As missing data is generally present (or rather not present!) in health-related datasets, we find this is a question that is regularly raised:\n“Can we use mixed models, since they use all available data?”\nWorking through even the basics on this topic will mean one will also have to work through challenging and varied (often cryptic) statistical nomenclature. This post is a worked example and was motivated by reading this paper. It has an interesting example of ‘missing data and mixed models’ that we thought could benefit from some figures and commentary to aid in the understanding of what is achieved."
  },
  {
    "objectID": "posts/2024-06-07_lmer-missing/missing_lmer.html#research-question",
    "href": "posts/2024-06-07_lmer-missing/missing_lmer.html#research-question",
    "title": "Handling Missing Data with LMER",
    "section": "Research question",
    "text": "Research question\nAre there differences in the rate of wage growth between males and females over time (in this dataset)?\nThat question is quite straightforward to answer here, but the motivating commentary is really around how mixed effects model can be beneficial in the presence of systematic missing (follow-up) data - with a focus on parameter estimates and their graphical interpretation.\nMissing follow-up data (lost to follow, attrition, drop out) is often seen in health research datasets. The data might be Missing At Random, it my be Missing Completely At Random, the important nuances of these are largely out of scope for this post."
  },
  {
    "objectID": "posts/2024-06-07_lmer-missing/missing_lmer.html#erroneous-basic-linear-regression-model",
    "href": "posts/2024-06-07_lmer-missing/missing_lmer.html#erroneous-basic-linear-regression-model",
    "title": "Handling Missing Data with LMER",
    "section": "Erroneous basic linear regression model",
    "text": "Erroneous basic linear regression model\nTo address the question of “are there differences between genders in the rate of wage growth”, we are going to fit a gender by time interaction term which will give us an indication of if ‘as time changes’ whether the outcome (logged wage) changes at a different rate for each gender.\n\n\nCode\nmod1 &lt;- lm(lwage ~ gender * t, data = dat)\nexport_summs(mod1, error_format = \"[{conf.low}, {conf.high}]\",\n             error_pos = \"right\", digits = 3,\n             statistics = c(N = \"nobs\"))\n\n\n\n\nModel 1\n\n(Intercept)6.340 ***[6.312, 6.368]\n\ngenderFemale-0.456 ***[-0.540, -0.372]\n\nt0.097 ***[0.091, 0.104]\n\ngenderFemale:t-0.005    [-0.024, 0.014]\n\nN4165             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nWe can see there is an effect of gender present, and an effect of time (the growth overtime we saw in the original plot), but the very small beta coefficient (relative to the scale of data we are working with) and the p-value of 0.6 are suggestive that the rate of wage growth over time does not differ significantly between genders.\nTo view this, we’re not going to use geom_smooth or stat_summary as we might do when graphing on-the-fly, rather we will use the predict() function to create data for our line of best fit.\nTo set a coding framework which we’ll use again later in the post, we’ll create a new dataset and use predictions to draw our (straight) line.\n\n\nCode\nnewdat &lt;- expand.grid(t = 1:7, gender = c(\"Male\", \"Female\")) %&gt;% \n  mutate(gender = factor(gender),\n         gender = relevel(gender, \"Male\"))\n\np_mod1 &lt;- cbind(newdat, \n                lwage = predict(mod1, newdat, interval = \"prediction\"))\n\n\n\n\nCode\ndat %&gt;% \n  ggplot(aes(t, lwage, colour = gender)) +\n  geom_point(alpha = 0.15) +\n  geom_line(aes(group = id), alpha = 0.15) + \n  facet_wrap(~ gender) +\n  geom_line(data = p_mod1, aes(y = lwage.fit), colour = \"red\", linewidth = 1) +\n  scale_colour_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  theme(legend.position=\"none\") +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\") +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9))\n\n\n\n\n\n\n\n\n\nOkay, these red fitted lines look like quite good as a ‘line of best fit’; they pass the eye test of broadly representing the trends of the data well.\nFitted with a prediction confidence interval, we see.\n\n\nCode\np_mod1 %&gt;% \n  ggplot(aes(t, lwage.fit, ymin = lwage.lwr, ymax = lwage.upr, fill = gender)) +\n  geom_ribbon(alpha = 0.5) +\n  geom_line(colour = \"black\", linewidth = 1) + \n  facet_wrap(~ gender) +\n  scale_fill_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  theme(legend.position=\"none\") +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\")  +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9)) -&gt; p1\np1\n\n\n\n\n\n\n\n\n\nThese lines look parallel - suggestive of no difference in growth rates between the genders (in line with the non-significant interaction term we saw).\nOf course, we have not adjusted for the within person correlation present in the data. The model above is inappropriate as one of the main assumptions of the model is violated - the data points are not all independent (we know there are 7 from each individual)."
  },
  {
    "objectID": "posts/2024-06-07_lmer-missing/missing_lmer.html#mixed-effects-model",
    "href": "posts/2024-06-07_lmer-missing/missing_lmer.html#mixed-effects-model",
    "title": "Handling Missing Data with LMER",
    "section": "Mixed effects model",
    "text": "Mixed effects model\nHere we run a fairly basic linear mixed effects model, the model has the same fixed effects terms as above (the interaction term we are curious about) but also includes a random effect, that is, the intercept is allowed to varied for each individual.\n\n\nCode\nmod2 &lt;- lmer(lwage ~ gender * t + (1 | id), data = dat)\nexport_summs(mod2, error_format = \"[{conf.low}, {conf.high}]\",\n             error_pos = \"right\", digits = 3,\n             statistics = c(N = \"nobs\"))\n\n\n\n\nModel 1\n\n(Intercept)6.340 ***[6.307, 6.373]\n\ngenderFemale-0.456 ***[-0.553, -0.358]\n\nt0.097 ***[0.095, 0.100]\n\ngenderFemale:t-0.005    [-0.012, 0.003]\n\nN4165             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nLets also fit the predicted values from this model.\n\n\nCode\nnewdat &lt;- expand.grid(t = 1:7, gender = c(\"Male\", \"Female\")) %&gt;% \n  mutate(gender = factor(gender),\n         gender = relevel(gender, \"Male\"),\n         id = c(rep(1, 7), rep(2, 7)))\n\nstore &lt;- simulate(mod2, seed=1, newdata=newdat, re.form=NA,\n                  allow.new.levels=T, nsim = 500)\n\np_mod2 &lt;- cbind(newdat,\n                store %&gt;% \n                  rowwise() %&gt;% \n                  mutate(fit = mean(c_across(sim_1:sim_500)),\n                         lwr = quantile(c_across(sim_1:sim_500), 0.025),\n                         upr = quantile(c_across(sim_1:sim_500), 0.975)) %&gt;% \n                  select(fit, lwr, upr))\n\n\n\n– Slight segue\nWith standard linear regression model (first used), we used predict to generate a ‘prediction interval’. With linear mixed effects models, we do no have the same function available (that will incorporate the random effects variability into the prediction interval), so rather than calculating these with a formula, we simulate! Some extra content on this can be read here or (somewhat less so) here.\nThis use of simulation is part of the reason behind the confidence intervals not being parallel."
  },
  {
    "objectID": "posts/2024-06-07_lmer-missing/missing_lmer.html#back-to-our-mixed-effects-model",
    "href": "posts/2024-06-07_lmer-missing/missing_lmer.html#back-to-our-mixed-effects-model",
    "title": "Handling Missing Data with LMER",
    "section": "Back to our Mixed effects model",
    "text": "Back to our Mixed effects model\n\n\nCode\np_mod2 %&gt;% \n  ggplot(aes(t, fit, ymin = lwr, ymax = upr, fill = gender)) +\n  geom_ribbon(alpha = 0.5) +\n  geom_line(colour = \"black\", linewidth = 1) + \n  facet_wrap(~ gender) +\n  scale_fill_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  theme(legend.position=\"none\") +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\") +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9)) -&gt; p2\np2\n\n\n\n\n\n\n\n\n\nWhen we compare the output of this model with the earlier (erroneous) model, we see two expected things.\n\n\nCode\nexport_summs(mod1, mod2,\n             error_format = \"[{conf.low}, {conf.high}]\",\n             error_pos = \"right\", digits = 3,\n             statistics = c(N = \"nobs\"),\n             model.names = c(\"Erroneous model\", \"Mixed efects model\"))\n\n\n\n\nErroneous modelMixed efects model\n\n(Intercept)6.340 ***[6.312, 6.368]6.340 ***[6.307, 6.373]\n\ngenderFemale-0.456 ***[-0.540, -0.372]-0.456 ***[-0.553, -0.358]\n\nt0.097 ***[0.091, 0.104]0.097 ***[0.095, 0.100]\n\ngenderFemale:t-0.005    [-0.024, 0.014]-0.005    [-0.012, 0.003]\n\nN4165             4165             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\n\nThe coefficients have the same value in each model as these represent the fixed effect\nThe confidence intervals for those coefficients are slightly narrower in Model 2, this is because some of the variation present [within Model 1] is explained by the random effects [present in Model 2 and not Model 1].\n\nWe can see this when we plot the predicted values side by side.\n\n\nCode\n(p1 + labs(title = \"Erroneous model\")) / (p2 + labs(title = \"Mixed effects model\"))"
  },
  {
    "objectID": "posts/2024-06-07_lmer-missing/missing_lmer.html#missing---erroneous-basic-linear-regression-model",
    "href": "posts/2024-06-07_lmer-missing/missing_lmer.html#missing---erroneous-basic-linear-regression-model",
    "title": "Handling Missing Data with LMER",
    "section": "Missing - Erroneous basic linear regression model",
    "text": "Missing - Erroneous basic linear regression model\n\n\nCode\nmod3 &lt;- lm(mlwage ~ gender * t, data = mdat)\nexport_summs(mod3, error_format = \"[{conf.low}, {conf.high}]\",\n             error_pos = \"right\", digits = 3,\n             statistics = c(N = \"nobs\"))\n\n\n\n\nModel 1\n\n(Intercept)6.379 ***[6.354, 6.405]\n\ngenderFemale-0.462 ***[-0.533, -0.390]\n\nt0.047 ***[0.040, 0.053]\n\ngenderFemale:t0.032 ***[0.016, 0.049]\n\nN3101             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nNow the model is indicating that there is a strong interaction effect for gender by time. The coefficient of the interaction term implies that (logged) wages increase at a faster rate (over time) for females than they do for males.\nLet’s visual this alongside our modified dataset.\n\n\nCode\nnewdat &lt;- expand.grid(t = 1:7, gender = c(\"Male\", \"Female\"), id = 1) %&gt;% \n  mutate(gender = factor(gender),\n         gender = relevel(gender, \"Male\"))\n\np_mod3 &lt;- cbind(newdat, \n                lwage = predict(mod3, newdat, interval = \"prediction\"))\n\n\n\n\nCode\nmdat %&gt;% \n  ggplot(aes(t, mlwage, colour = gender)) +\n  geom_ribbon(data = p_mod3, aes(t, lwage.fit, ymin = lwage.lwr, ymax = lwage.upr, fill = gender), alpha = 0.5) +\n  geom_line(data = p_mod3, aes(t, lwage.fit), colour = \"black\", linewidth = 1) + \n  geom_point(data = mdat %&gt;% filter(is_zero_following == 1),\n             aes(y = lwage, group = id), alpha = 0.1, colour = \"red\") +\n  geom_line(data = mdat %&gt;% filter(is_zero_following == 1),\n            aes(y = lwage, group = id), alpha = 0.15, colour = \"red\") + \n  geom_point(alpha = 0.15) +\n  geom_line(aes(group = id), alpha = 0.15) + \n  facet_wrap(~ gender) +\n  scale_colour_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  theme(legend.position=\"none\",\n        plot.subtitle=element_text(colour = \"red\")) +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\",\n       subtitle = \"Missing data shown in red\")  +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9)) -&gt; p4\np4\n\n\n\n\n\n\n\n\n\nWe can see the predicted line and bands (95% prediction interval) represent the (non-missing) data well, and we can see the difference in slope between genders - the observed significant interaction term."
  },
  {
    "objectID": "posts/2024-06-07_lmer-missing/missing_lmer.html#missing---mixed-effects-model",
    "href": "posts/2024-06-07_lmer-missing/missing_lmer.html#missing---mixed-effects-model",
    "title": "Handling Missing Data with LMER",
    "section": "Missing - Mixed effects model",
    "text": "Missing - Mixed effects model\n\n\nCode\nmod4 &lt;- lmer(mlwage ~ gender * t + (1 | id), data = mdat)\nexport_summs(mod4, error_format = \"[{conf.low}, {conf.high}]\",\n             error_pos = \"right\", digits = 3,\n             statistics = c(N = \"nobs\"))\n\n\n\n\nModel 1\n\n(Intercept)6.341 ***[6.311, 6.372]\n\ngenderFemale-0.457 ***[-0.546, -0.368]\n\nt0.090 ***[0.087, 0.093]\n\ngenderFemale:t0.003    [-0.004, 0.011]\n\nN3101             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nWhen we run the mixed effects model on the dataset with missing data, we (correctly) do not see a significant interaction effect.\nIn fact:\n\n\nCode\nexport_summs(mod4, mod2,\n             error_format = \"[{conf.low}, {conf.high}]\",\n             error_pos = \"right\", digits = 3,\n             model.names = c(\"ME - Missing\", \"ME - Complete\"),\n             statistics = c(N = \"nobs\"))\n\n\n\n\nME - MissingME - Complete\n\n(Intercept)6.341 ***[6.311, 6.372]6.340 ***[6.307, 6.373]\n\ngenderFemale-0.457 ***[-0.546, -0.368]-0.456 ***[-0.553, -0.358]\n\nt0.090 ***[0.087, 0.093]0.097 ***[0.095, 0.100]\n\ngenderFemale:t0.003    [-0.004, 0.011]-0.005    [-0.012, 0.003]\n\nN3101             4165             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nOur mixed effects model on the dataset with a lot of missing data (ME - Missing) generates quite similar estimates to what we know the ‘truth’ to be from the model on the complete data (ME - Complete).\nTo comparatively visualise this.\n\n\nCode\nnewdat &lt;- expand.grid(t = 1:7, gender = c(\"Male\", \"Female\")) %&gt;% \n  mutate(gender = factor(gender),\n         gender = relevel(gender, \"Male\"),\n         id = c(rep(4, 7), rep(8, 7)))\n\nstore &lt;- simulate(mod4, seed=1, newdata=newdat, re.form=NA,\n                  allow.new.levels=T, nsim = 500)\n\np_mod4 &lt;- cbind(newdat,\n                store %&gt;% \n                  rowwise() %&gt;% \n                  mutate(fit = mean(c_across(sim_1:sim_500)),\n                         lwr = quantile(c_across(sim_1:sim_500), 0.025),\n                         upr = quantile(c_across(sim_1:sim_500), 0.975)) %&gt;% \n                  select(fit, lwr, upr))\n\n\n\n\nCode\nmdat %&gt;% \n  ggplot(aes(t, mlwage, colour = gender)) +\n  geom_ribbon(data = p_mod4, aes(t, fit, ymin = lwr, ymax = upr, fill = gender), alpha = 0.5) +\n  geom_line(data = p_mod4, aes(t, fit), colour = \"black\", linewidth = 1) + \n  geom_point(data = mdat %&gt;% filter(is_zero_following == 1),\n             aes(y = lwage, group = id), alpha = 0.1, colour = \"red\") +\n  geom_line(data = mdat %&gt;% filter(is_zero_following == 1),\n            aes(y = lwage, group = id), alpha = 0.15, colour = \"red\") + \n  geom_point(alpha = 0.15) +\n  geom_line(aes(group = id), alpha = 0.15) + \n  facet_wrap(~ gender) +\n  scale_colour_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  theme(legend.position=\"none\",\n        plot.subtitle=element_text(colour = \"red\")) +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\",\n       subtitle = \"Missing data shown in red\")  +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9)) -&gt; p4\np4\n\n\n\n\n\n\n\n\n\nThese lines (predicted lines; by gender) look parallel (as they should). Notably with the Males, we see the predicted line “pulled up” in the direction of the missing data even though that data was not available to the model - this is because the model has leveraged the slope of the data it did have access to, at the individual (person) level, when converging on its estimates.\nIf the erroneous interaction effect (non-parallel lines) was not obvious in the plot separated by gender, here we see the predicted lines on the same plot for each model.\n\n\nCode\np_mod3 %&gt;% \n  ggplot(aes(t, lwage.fit, ymin = lwage.lwr, ymax = lwage.upr, fill = gender)) +\n  geom_ribbon(alpha = 0.5) +\n  geom_line(colour = \"black\", linewidth = 1) + \n  scale_fill_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\",\n       title = \"Erroneous basic linear regression\",\n       subtitle = \"Missing data present\") +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9)) -&gt; p5\n\np_mod4 %&gt;% \n  ggplot(aes(t, fit, ymin = lwr, ymax = upr, fill = gender)) +\n  geom_ribbon(alpha = 0.5) +\n  geom_line(colour = \"black\", linewidth = 1) + \n  scale_fill_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\",\n       title = \"Mixed effects regression\",\n       subtitle = \"Missing data present\")  +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9)) -&gt; p6\n\np5 + p6 + plot_layout(guides = \"collect\") & theme(legend.position='bottom')"
  },
  {
    "objectID": "posts/2024-06-07_lmer-missing/missing_lmer.html#less-aggressive-missingness",
    "href": "posts/2024-06-07_lmer-missing/missing_lmer.html#less-aggressive-missingness",
    "title": "Handling Missing Data with LMER",
    "section": "Less aggressive missingness",
    "text": "Less aggressive missingness\nWhat if we whip through the same process and comparison, in a setting that ‘less aggressively’ has drop out with increasing wage and also some additional random missingness throughout.\n\n\nCode\nmdat &lt;- dat %&gt;% \n  group_by(id) %&gt;% \n  mutate(plwage = c(0, lwage[1:6]),\n         pt = 1 / (1+exp(-7.2 + plwage))) %&gt;% # Less aggressive dropout as a function of age\n  rowwise() %&gt;% \n  mutate(pt = case_when(pt &gt; 0.5 & runif(1) &lt; 0.10 & t &gt; 2 ~ 0, # Adding an underlying random component to dropout\n                        T ~ pt)) %&gt;% \n  group_by(id) %&gt;% \n  mutate(mlwage = case_when(pt &gt; 0.5 ~ lwage,\n                            pt &lt; 0.5 ~ 0),\n         is_zero_following = ifelse(mlwage == 0, 1, 0),\n         is_zero_following = cummax(is_zero_following),\n         mlwage = ifelse(is_zero_following == 1, NA, mlwage)) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nMale, N = 2,8041\nFemale, N = 3521\n\n\n\n\nt\n\n\n\n\n\n\n    1\n528\n67\n\n\n    2\n528\n67\n\n\n    3\n478\n57\n\n\n    4\n396\n49\n\n\n    5\n338\n43\n\n\n    6\n290\n38\n\n\n    7\n246\n31\n\n\n\n1 n\n\n\n\n\n\n\n\n\n\n\nCode\nmdat %&gt;% \n  ggplot(aes(t, mlwage, colour = gender)) +\n  geom_point(data = mdat %&gt;% filter(is_zero_following == 1),\n             aes(y = lwage, group = id), alpha = 0.1, colour = \"red\") +\n  geom_line(data = mdat %&gt;% filter(is_zero_following == 1),\n            aes(y = lwage, group = id), alpha = 0.15, colour = \"red\") + \n  geom_point(alpha = 0.15) +\n  geom_line(aes(group = id), alpha = 0.15) + \n  facet_wrap(~ gender) +\n  scale_colour_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  theme(legend.position=\"none\") +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\")  +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9))\n\n\n\n\n\n\n\n\n\n\n\nCode\nb1_mod1 &lt;- lm(mlwage ~ gender * t, data = mdat)\nb1_mod2 &lt;- lmer(mlwage ~ gender * t + (1 | id), data = mdat)\n\nexport_summs(mod2, b1_mod1, b1_mod2,\n             error_format = \"[{conf.low}, {conf.high}]\",\n             error_pos = \"right\", digits = 3,\n             model.names = c(\"ME - Complete\", \"Basic - Missing\", \"ME - Missing\"),\n             statistics = c(N = \"nobs\"))\n\n\n\n\nME - CompleteBasic - MissingME - Missing\n\n(Intercept)6.340 ***[6.307, 6.373]6.380 ***[6.352, 6.407]6.342 ***[6.311, 6.373]\n\ngenderFemale-0.456 ***[-0.553, -0.358]-0.491 ***[-0.573, -0.410]-0.465 ***[-0.557, -0.372]\n\nt0.097 ***[0.095, 0.100]0.072 ***[0.065, 0.079]0.094 ***[0.091, 0.097]\n\ngenderFemale:t-0.005    [-0.012, 0.003]0.024 *  [0.003, 0.044]0.003    [-0.006, 0.012]\n\nN4165             3156             3156             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nWe can see, the (erroneous) basic linear regression still inappropriately suggests a significant interaction effect is present, while the mixed effects models continues to perform well (relative to the model using the complete data)."
  },
  {
    "objectID": "posts/2024-06-07_lmer-missing/missing_lmer.html#completely-random-missingness",
    "href": "posts/2024-06-07_lmer-missing/missing_lmer.html#completely-random-missingness",
    "title": "Handling Missing Data with LMER",
    "section": "Completely random missingness",
    "text": "Completely random missingness\nWhat if the dropout is completely at random?\nNote, this is dropout at random, not sporadic missingness, these are two different things.\n\n\nCode\nmdat &lt;- dat %&gt;% \n  group_by(id) %&gt;% \n  rowwise() %&gt;% \n  mutate(pt = 1,\n         pt = case_when(runif(1) &lt; 0.10 & t &gt; 2 ~ 0, # Implementing completely random dropout\n                        T ~ pt)) %&gt;% \n  group_by(id) %&gt;% \n  mutate(mlwage = case_when(pt &gt; 0.5 ~ lwage,\n                            pt &lt; 0.5 ~ 0),\n         is_zero_following = ifelse(mlwage == 0, 1, 0),\n         is_zero_following = cummax(is_zero_following),\n         mlwage = ifelse(is_zero_following == 1, NA, mlwage)) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nME - CompleteBasic - MissingME - Missing\n\n(Intercept)6.340 ***[6.307, 6.373]6.330 ***[6.300, 6.359]6.333 ***[6.301, 6.366]\n\ngenderFemale-0.456 ***[-0.553, -0.358]-0.456 ***[-0.544, -0.368]-0.456 ***[-0.553, -0.359]\n\nt0.097 ***[0.095, 0.100]0.102 ***[0.095, 0.109]0.099 ***[0.096, 0.102]\n\ngenderFemale:t-0.005    [-0.012, 0.003]-0.007    [-0.029, 0.014]-0.003    [-0.012, 0.005]\n\nN4165             3379             3379             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nThe basic regression model is no long suggesting there is a significant gender by time interaction effect, and comparatively, all three models give similar estimates."
  },
  {
    "objectID": "posts/2024-06-07_lmer-missing/missing_lmer.html#acknowledgements",
    "href": "posts/2024-06-07_lmer-missing/missing_lmer.html#acknowledgements",
    "title": "Handling Missing Data with LMER",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Elizabeth McKinnon, Zac Dempsey, and Wesley Billingham for providing feedback on and reviewing this post.\nYou can look forward to seeing posts from these other team members here in the coming weeks and months."
  },
  {
    "objectID": "posts/2024-06-07_lmer-missing/missing_lmer.html#reproducibility-information",
    "href": "posts/2024-06-07_lmer-missing/missing_lmer.html#reproducibility-information",
    "title": "Handling Missing Data with LMER",
    "section": "Reproducibility Information",
    "text": "Reproducibility Information\nTo access the .qmd (Quarto markdown) files as well as any R scripts or data that was used in this post, please visit our GitHub:\nhttps://github.com/The-Kids-Biostats/The-Kids-Biostats.github.io/tree/main/posts/lmer-missingx\nThe session information can also be seen below.\n\n\nR version 4.4.0 (2024-04-24)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.5\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Australia/Perth\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n [5] purrr_1.0.2       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n [9] ggplot2_3.5.1     tidyverse_2.0.0   patchwork_1.2.0   parameters_0.22.0\n[13] jtools_2.2.2      gtsummary_1.7.2   AER_1.2-12        survival_3.5-8   \n[17] sandwich_3.1-0    lmtest_0.9-40     zoo_1.8-12        car_3.1-2        \n[21] carData_3.0-5     merTools_0.6.2    arm_1.14-4        MASS_7.3-60.2    \n[25] lme4_1.1-35.4     Matrix_1.7-0     \n\nloaded via a namespace (and not attached):\n [1] rlang_1.1.4          magrittr_2.0.3       furrr_0.3.1         \n [4] compiler_4.4.0       vctrs_0.6.5          pkgconfig_2.0.3     \n [7] crayon_1.5.3         fastmap_1.2.0        backports_1.5.0     \n[10] labeling_0.4.3       pander_0.6.5         utf8_1.2.4          \n[13] promises_1.3.0       rmarkdown_2.27       markdown_1.13       \n[16] tzdb_0.4.0           nloptr_2.1.0         xfun_0.46           \n[19] jsonlite_1.8.8       later_1.3.2          broom_1.0.6         \n[22] parallel_4.4.0       R6_2.5.1             stringi_1.8.4       \n[25] parallelly_1.37.1    boot_1.3-30          estimability_1.5.1  \n[28] assertthat_0.2.1     Rcpp_1.0.13          iterators_1.0.14    \n[31] knitr_1.48           httpuv_1.6.15        splines_4.4.0       \n[34] timechange_0.3.0     tidyselect_1.2.1     rstudioapi_0.16.0   \n[37] abind_1.4-5          yaml_2.3.9           codetools_0.2-20    \n[40] listenv_0.9.1        lattice_0.22-6       shiny_1.8.1.1       \n[43] withr_3.0.0          bayestestR_0.13.2    coda_0.19-4.1       \n[46] evaluate_0.24.0      future_1.33.2        huxtable_5.5.6      \n[49] xml2_1.3.6           pillar_1.9.0         foreach_1.5.2       \n[52] insight_0.20.1       generics_0.1.3       hms_1.1.3           \n[55] munsell_0.5.1        commonmark_1.9.1     scales_1.3.0        \n[58] minqa_1.2.7          globals_0.16.3       xtable_1.8-4        \n[61] glue_1.7.0           emmeans_1.10.2       tools_4.4.0         \n[64] mvtnorm_1.2-5        grid_4.4.0           datawizard_0.11.0   \n[67] colorspace_2.1-0     nlme_3.1-164         Formula_1.2-5       \n[70] cli_3.6.3            fansi_1.0.6          broom.helpers_1.15.0\n[73] viridisLite_0.4.2    gt_0.10.1            gtable_0.3.5        \n[76] broom.mixed_0.2.9.5  sass_0.4.9           digest_0.6.36       \n[79] pbkrtest_0.5.3       htmlwidgets_1.6.4    farver_2.1.2        \n[82] htmltools_0.5.8.1    lifecycle_1.0.4      mime_0.12           \n[85] blme_1.0-5"
  },
  {
    "objectID": "posts/2024-10-14_likert_visualisation/likert_visualisations.html",
    "href": "posts/2024-10-14_likert_visualisation/likert_visualisations.html",
    "title": "Visualisations for (ordinal) Likert scale data",
    "section": "",
    "text": "Likert scale variables (and hence data) are widely utilised in research—they are useful for getting participants to rate things, or to provide an average quantity as a response in situations where asking for the exact quantity may be problematic. How can asking for an exact quantity be problematic? Well, consider the example below that relates to how much water one drinks per day. Very few people drink the exact same amount of water each day, so asking participants “How many (250ml) glasses of water do you drink per day” and getting a response of “3” is typically pointless—there is likely substantial measurement error, and if they drank 3 glasses yesterday (anomaly or otherwise) but 4 glasses the day before, is the response of 3 not just outright incorrect?\nWhat is presented below is nothing ground breaking. We went in search of a concise, succinct, and accurate way to display (specifically) “pre/post” Likert data, and this is where we are currently at."
  },
  {
    "objectID": "posts/2024-10-14_likert_visualisation/likert_visualisations.html#definitions",
    "href": "posts/2024-10-14_likert_visualisation/likert_visualisations.html#definitions",
    "title": "Visualisations for (ordinal) Likert scale data",
    "section": "Definitions",
    "text": "Definitions\nFirstly, some definitions. There are two main types of Likert data. We are going to refer to them as “ordinal” and “bidirectional”.\nOrdinal Likert data (sometimes called unipolar Likert data, or interval Likert data) involves category responses that have some natural order (decreasing/increasing) to them, the width of categories and the distance between categories are not necessarily consistent, and the categories often represent a underlying continuous scale that has been ‘binned’ (into the Likert categories).\nAn example. “How many glasses of water do you typically drink per day?” with response options:\n\nLess than one glass/day\n1-2 glasses/day\n3-4 glasses/day\n5-6 glasses/day\nMore than six glasses/day\n\nBirectional Likert data (sometimes called bipolar Likert data) involves category responses that have a natural order with responses from two opposing directions—typically negative responses and positive responses—around a central (or neutral) point.\nAn example. “The amount of reading I do influences how much reading my child does?” with response options:\n\nStrongly disagree\nDisagree\nNeither agree nor disagree (the neutral midpoint)\nAgree\nStrongly agree\n\nWe’ll return to bidirectional Likert data in a future post, for now we will look at ordinal Likert data."
  },
  {
    "objectID": "posts/2024-10-14_likert_visualisation/likert_visualisations.html#demo-data",
    "href": "posts/2024-10-14_likert_visualisation/likert_visualisations.html#demo-data",
    "title": "Visualisations for (ordinal) Likert scale data",
    "section": "Demo data",
    "text": "Demo data\n\n\nCode\nlibrary(simstudy)\nlibrary(ggsankey); library(ggalluvial)\nlibrary(likert); library(patchwork)\nlibrary(gt); library(gtsummary)\nlibrary(flextable)\nlibrary(thekidsbiostats) # install with remotes::install_github(\"The-Kids-Biostats/thekidsbiostats\")\n\n\nWe’re going to use one of our favourite packages to create some synthetic data to use.\nSpecifically, we will simulate some pre and post response data, a group identifier (intervention or control), and then some labelled response columns.\n\n\nCode\nset.seed(123) # For reproducibility\n\n# dat_i is the intervention group\nn &lt;- 183 # Set the number of individuals\ndef &lt;- defData(varname = \"pre\", formula = \"1;5\", dist = \"uniformInt\") # Pre values: uniformly distributed between 1 and 5\ndat_i &lt;- genData(n, def)\ngroup_probs &lt;- c(0.45, 0.45, 0.10)\ndat_i$grp &lt;- sample(1:3, n, replace = TRUE, prob = group_probs)\ndat_i$post &lt;- dat_i$pre\n\ndat_i$post[dat_i$grp == 2] &lt;- pmin(dat_i$pre[dat_i$grp == 2] + (rbinom(sum(dat_i$grp == 2), 2, 0.2) + 1), 5) # Increase by 1, max 5\ndat_i$post[dat_i$grp == 3] &lt;- pmax(dat_i$pre[dat_i$grp == 3] - (rbinom(sum(dat_i$grp == 3), 2, 0.2) + 1), 1) # Decrease by 1, min 1\n\n# dat_c is the control group\nn &lt;- 154 # Set the number of individuals\ndef &lt;- defData(varname = \"pre\", formula = \"1;5\", dist = \"uniformInt\") # Pre values: uniformly distributed between 1 and 5\ndat_c &lt;- genData(n, def)\ngroup_probs &lt;- c(0.55, 0.25, 0.20)\ndat_c$grp &lt;- sample(1:3, n, replace = TRUE, prob = group_probs)\ndat_c$post &lt;- dat_c$pre\n\ndat_c$post[dat_c$grp == 2] &lt;- pmin(dat_c$pre[dat_c$grp == 2] + (rbinom(sum(dat_c$grp == 2), 2, 0.2) + 1), 5) # Increase by 1, max 5\ndat_c$post[dat_c$grp == 3] &lt;- pmax(dat_c$pre[dat_c$grp == 3] - (rbinom(sum(dat_c$grp == 3), 2, 0.2) + 1), 1) # Decrease by 1, min 1\n\n# Combine the control & intervention data into one dataframe\ndat &lt;- rbind(cbind(dat_i, group = \"Intervention\"), \n             cbind(dat_c, group = \"Control\")) %&gt;% \n  mutate(post = as.integer(post)) %&gt;% \n  select(-grp)\n\n# Add some factored labels\ndat &lt;- dat %&gt;% \n  mutate(pre_l = fct_case_when(pre == 1 ~ \"Less than one cup/day\",\n                               pre == 2 ~ \"About 1-2 cups/day\",\n                               pre == 3 ~ \"About 3-4 cups/day\",\n                               pre == 4 ~ \"About 5-6 cups/day\",\n                               pre == 5 ~ \"More than 6 cups/day\"),\n         post_l = fct_case_when(post == 1 ~ \"Less than one cup/day\",\n                                post == 2 ~ \"About 1-2 cups/day\",\n                                post == 3 ~ \"About 3-4 cups/day\",\n                                post == 4 ~ \"About 5-6 cups/day\",\n                                post == 5 ~ \"More than 6 cups/day\"))\n\n# Visualise the first few rows of data\nhead(dat, 5) %&gt;%\n  thekids_table(colour = \"Saffron\", padding = 3)\n\n\nidprepostgrouppre_lpost_l123InterventionAbout 1-2 cups/dayAbout 3-4 cups/day244InterventionAbout 5-6 cups/dayAbout 5-6 cups/day333InterventionAbout 3-4 cups/dayAbout 3-4 cups/day455InterventionMore than 6 cups/dayMore than 6 cups/day555InterventionMore than 6 cups/dayMore than 6 cups/day"
  },
  {
    "objectID": "posts/2024-10-14_likert_visualisation/likert_visualisations.html#the-visualisation",
    "href": "posts/2024-10-14_likert_visualisation/likert_visualisations.html#the-visualisation",
    "title": "Visualisations for (ordinal) Likert scale data",
    "section": "The visualisation",
    "text": "The visualisation\n\n\nCode\nmax_prop &lt;- dat %&gt;%\n  select(id, group, pre, post) %&gt;% \n  pivot_longer(cols = c(pre, post)) %&gt;% \n  group_by(group, name) %&gt;%\n  count(value) %&gt;%\n  mutate(freq = n / sum(n)) %&gt;% \n  .$freq %&gt;%\n  max\n\nmax_prop &lt;- plyr::round_any(max_prop, 0.05, f = ceiling)\n\np1 &lt;- dat %&gt;% \n  filter(group == \"Intervention\") %&gt;% \n  group_by(pre) %&gt;% \n  tally() %&gt;% \n  mutate(freq = n / sum(n),\n         res = str_c(n, \"\\n(\", round(freq*100, 1), \"%)\")) %&gt;% \n  ggplot(aes(x = as.factor(pre), y = freq)) +\n  geom_bar(aes(fill = as.factor(pre)), stat=\"identity\", alpha = 0.8,\n           colour = \"black\") +\n  theme_institute(base_size = 14) +\n  theme(legend.position = \"none\",\n        panel.grid.major.x = element_blank(),\n        plot.title = element_text(hjust = 0.5)) +\n  scale_y_continuous(labels = scales::percent_format(),\n                     breaks = seq(0, max_prop, by = 0.05),\n                     expand = expansion(mult = c(0, 0.1))) +\n  coord_cartesian(ylim = c(0, max_prop)) +\n  scale_fill_viridis_d(option = \"plasma\", end = 0.85, direction = -1) +\n  labs(title = \"Pre\",\n       fill = \"Response\",\n       x = \"Response\", y = \"\") +\n  geom_text(aes(label = res), vjust = -0.1,\n            family = \"Barlow Semi Condensed\") +\n  guides(fill = guide_legend(nrow = 1))\n\n\np2 &lt;- dat %&gt;% \n  filter(group == \"Intervention\") %&gt;% \n  rename(Pre = pre,\n         Post = post) %&gt;% \n  make_long(Pre, Post) %&gt;% \n  mutate(node = factor(node, levels = c(7,6,5,4,3,2,1)),\n         next_node = factor(next_node, levels = c(7,6,5,4,3,2,1))) %&gt;% \n  ggplot(aes(x = x, \n             next_x = next_x, \n             node = node, \n             next_node = next_node,\n             fill = factor(node))) +\n  geom_sankey(alpha = 0.7,\n              node.color = 'black') +\n  geom_sankey_label(aes(label = node), alpha = 0.75,\n                    size = 3, color = \"black\", fill = \"gray80\") +\n  scale_x_discrete(expand = c(0.05,0.05)) +\n  theme_institute(base_size = 14) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks=element_blank(),\n        legend.position = \"bottom\",\n        plot.title = element_text(hjust = 0.5)) +\n  guides(fill = guide_legend(reverse = T, nrow = 1)) +\n  labs(title = \"Pre-post\",\n       fill = \"Response\",\n       x = \"\")\n\np3 &lt;- dat %&gt;% \n  filter(group == \"Intervention\") %&gt;% \n  group_by(post) %&gt;% \n  tally() %&gt;% \n  mutate(freq = n / sum(n),\n         res = str_c(n, \"\\n(\", round(freq*100, 1), \"%)\")) %&gt;% \n  ggplot(aes(x = as.factor(post), y = freq)) +\n  geom_bar(aes(fill = as.factor(post)), stat=\"identity\", alpha = 0.8,\n           colour = \"black\") +\n  theme_institute(base_size = 14) +\n  theme(legend.position = \"none\",\n        panel.grid.major.x = element_blank(),\n        plot.background = element_blank(),\n        plot.title = element_text(hjust = 0.5)) +\n  scale_y_continuous(labels = scales::percent_format(),\n                     breaks = seq(0, max_prop, by = 0.05),\n                     expand = expansion(mult = c(0, 0.1))) +\n  coord_cartesian(ylim = c(0, max_prop)) +\n  scale_fill_viridis_d(option = \"plasma\", end = 0.85, direction = -1) +\n  labs(title = \"Post\",\n       fill = \"Response\",\n       x = \"Response\", y = \"\") +\n  geom_text(aes(label = res), vjust = -0.1,\n            family = \"Barlow Semi Condensed\") +\n  guides(fill = guide_legend(nrow = 1))\n\n\nWe create the plot as three panels, and then use patchwork to control the joining together of the panels into one image.\n\n\nCode\np1 + p2 + p3 + theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nI know what you’re thinking: i) that looks great, ii) slow down, you had two groups.\nCorrect on both accounts. This is just the intervention group data.\n\n\nCode\nq1 &lt;- dat %&gt;% \n  filter(group == \"Control\") %&gt;% \n  group_by(pre) %&gt;% \n  tally() %&gt;% \n  mutate(freq = n / sum(n),\n         res = str_c(n, \"\\n(\", round(freq*100, 1), \"%)\")) %&gt;% \n  ggplot(aes(x = as.factor(pre), y = freq)) +\n  geom_bar(aes(fill = as.factor(pre)), stat=\"identity\", alpha = 0.8,\n           colour = \"black\") +\n  theme_institute(base_size = 14) +\n  theme(legend.position = \"none\",\n        panel.grid.major.x = element_blank(),\n        plot.title = element_text(hjust = 0.5)) +\n  scale_y_continuous(labels = scales::percent_format(),\n                     breaks = seq(0, max_prop, by = 0.05),\n                     expand = expansion(mult = c(0, 0.1))) +\n  coord_cartesian(ylim = c(0, max_prop)) +\n  scale_fill_viridis_d(option = \"plasma\", end = 0.85, direction = -1) +\n  labs(title = \"Pre\",\n       fill = \"Response\",\n       x = \"Response\", y = \"\") +\n  geom_text(aes(label = res), vjust = -0.1,\n            family = \"Barlow Semi Condensed\") +\n  guides(fill = guide_legend(nrow = 1))\n\n\nq2 &lt;- dat %&gt;% \n  filter(group == \"Control\") %&gt;% \n  rename(Pre = pre,\n         Post = post) %&gt;% \n  make_long(Pre, Post) %&gt;% \n  mutate(node = factor(node, levels = c(7,6,5,4,3,2,1)),\n         next_node = factor(next_node, levels = c(7,6,5,4,3,2,1))) %&gt;% \n  ggplot(aes(x = x, \n             next_x = next_x, \n             node = node, \n             next_node = next_node,\n             fill = factor(node))) +\n  geom_sankey(alpha = 0.7,\n              node.color = 'black') +\n  geom_sankey_label(aes(label = node), alpha = 0.75,\n                    size = 3, color = \"black\", fill = \"gray80\") +\n  scale_x_discrete(expand = c(0.05,0.05)) +\n  theme_institute(base_size = 14) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks=element_blank(),\n        legend.position = \"bottom\",\n        plot.title = element_text(hjust = 0.5)) +\n  guides(fill = guide_legend(reverse = T, nrow = 1)) +\n  labs(title = \"Pre-post\",\n       fill = \"Response\",\n       x = \"\")\n\nq3 &lt;- dat %&gt;% \n  filter(group == \"Control\") %&gt;% \n  group_by(post) %&gt;% \n  tally() %&gt;% \n  mutate(freq = n / sum(n),\n         res = str_c(n, \"\\n(\", round(freq*100, 1), \"%)\")) %&gt;% \n  ggplot(aes(x = as.factor(post), y = freq)) +\n  geom_bar(aes(fill = as.factor(post)), stat=\"identity\", alpha = 0.8,\n           colour = \"black\") +\n  theme_institute(base_size = 14) +\n  theme(legend.position = \"none\",\n        panel.grid.major.x = element_blank(),\n        plot.background = element_blank(),\n        plot.title = element_text(hjust = 0.5)) +\n  scale_y_continuous(labels = scales::percent_format(),\n                     breaks = seq(0, max_prop, by = 0.05),\n                     expand = expansion(mult = c(0, 0.1))) +\n  coord_cartesian(ylim = c(0, max_prop)) +\n  scale_fill_viridis_d(option = \"plasma\", end = 0.85, direction = -1) +\n  labs(title = \"Post\",\n       fill = \"Response\",\n       x = \"Response\", y = \"\") +\n  geom_text(aes(label = res), vjust = -0.1,\n            family = \"Barlow Semi Condensed\") +\n  guides(fill = guide_legend(nrow = 1))\n\n\nWe can double up the plot, again using patchwork, to show both groups.\n\n\nCode\npatchwork &lt;- (p1 + p2 + p3) / (q1 + q2 + q3)\n\npatchwork + \n  plot_annotation(tag_levels = list(c('Intervention', '', '', \n                                      'Control', '', ''))) &\n  theme(plot.tag.position = c(0, 1),\n        plot.tag = element_text(face = \"bold\", hjust = 0, vjust = 0))\n\n\n\n\n\n\n\n\n\nAnd, we might also like to table some of the ‘change’ data that this plot is based on—using our favourite package (to battle with) gtsummary.\n\n\nCode\ndat %&gt;% \n  mutate(Change = fct_case_when(post &lt; pre ~ \"Decrease\",\n                                pre == post ~ \"No change\",\n                                post &gt; pre ~ \"Increase\")) %&gt;% \n  select(group, pre_l, Change) %&gt;% \n  tbl_strata(\n    strata = group,\n    ~.x %&gt;%\n      tbl_summary(\n        by = pre_l) %&gt;%\n      modify_header(all_stat_cols() ~ \"**{level}**\"),\n    .combine_with = \"tbl_stack\"\n  ) %&gt;% \n  thekids_table(colour = \"Saffron\")\n\n\nGroupCharacteristicLess than one cup/day1About 1-2 cups/day1About 3-4 cups/day1About 5-6 cups/day1More than 6 cups/day1ControlChangeDecrease0 (0%)9 (29%)5 (19%)3 (12%)8 (20%)No change24 (80%)15 (48%)14 (52%)13 (50%)32 (80%)Increase6 (20%)7 (23%)8 (30%)10 (38%)0 (0%)InterventionChangeDecrease0 (0%)6 (14%)6 (15%)3 (7.7%)3 (9.1%)No change15 (50%)20 (48%)10 (26%)20 (51%)30 (91%)Increase15 (50%)16 (38%)23 (59%)16 (41%)0 (0%)1n (%)\n\n\nOr perhaps just this will suffice:\n\n\nCode\ntbl_merge(tbls = list(dat %&gt;% \n                        mutate(\"Change in water intake\" = fct_case_when(post &lt; pre ~ \"Decrease\",\n                                                                        pre == post ~ \"No change\",\n                                                                        post &gt; pre ~ \"Increase\")) %&gt;% \n                        filter(group == \"Control\") %&gt;% \n                        select(\"Change in water intake\") %&gt;% \n                        tbl_summary(), \n                      dat %&gt;% \n                        mutate(\"Change in water intake\" = fct_case_when(post &lt; pre ~ \"Decrease\",\n                                                                        pre == post ~ \"No change\",\n                                                                        post &gt; pre ~ \"Increase\")) %&gt;% \n                        filter(group == \"Intervention\") %&gt;% \n                        select(\"Change in water intake\") %&gt;% \n                        tbl_summary()),\n          tab_spanner = c(\"**Control**\", \"**Intervention**\")) %&gt;% \n  thekids_table(colour = \"Saffron\")\n\n\n ControlInterventionCharacteristicN = 1541N = 1831Change in water intakeDecrease25 (16%)18 (9.8%)No change98 (64%)95 (52%)Increase31 (20%)70 (38%)1n (%)"
  },
  {
    "objectID": "posts/2024-10-14_likert_visualisation/likert_visualisations.html#reproducibility-information",
    "href": "posts/2024-10-14_likert_visualisation/likert_visualisations.html#reproducibility-information",
    "title": "Visualisations for (ordinal) Likert scale data",
    "section": "Reproducibility Information",
    "text": "Reproducibility Information\nTo access the .qmd (Quarto markdown) files as well as any R scripts or data that was used in this post, please visit our GitHub:\nhttps://github.com/The-Kids-Biostats/The-Kids-Biostats.github.io/tree/main/posts/\nThe session information can also be seen below.\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.1 (2024-06-14 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_Australia.utf8  LC_CTYPE=English_Australia.utf8   \n[3] LC_MONETARY=English_Australia.utf8 LC_NUMERIC=C                      \n[5] LC_TIME=English_Australia.utf8    \n\ntime zone: Australia/Perth\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] thekidsbiostats_0.0.1 lubridate_1.9.3       forcats_1.0.0        \n [4] stringr_1.5.1         dplyr_1.1.4           purrr_1.0.2          \n [7] readr_2.1.5           tidyr_1.3.1           tibble_3.2.1         \n[10] tidyverse_2.0.0       extrafont_0.19        flextable_0.9.6      \n[13] gtsummary_2.0.2       gt_0.11.0             patchwork_1.3.0      \n[16] likert_1.3.5          xtable_1.8-4          ggalluvial_0.12.5    \n[19] ggplot2_3.5.1         ggsankey_0.0.99999    simstudy_0.8.1       \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1        psych_2.4.6.26          viridisLite_0.4.2      \n [4] farver_2.1.2            fastmap_1.2.0           fontquiver_0.2.1       \n [7] janitor_2.2.0           labelled_2.13.0         digest_0.6.37          \n[10] timechange_0.3.0        lifecycle_1.0.4         magrittr_2.0.3         \n[13] compiler_4.4.1          rlang_1.1.4             tools_4.4.1            \n[16] igraph_2.0.3            utf8_1.2.4              yaml_2.3.10            \n[19] data.table_1.16.0       knitr_1.48              labeling_0.4.3         \n[22] askpass_1.2.0           htmlwidgets_1.6.4       mnormt_2.1.1           \n[25] plyr_1.8.9              xml2_1.3.6              withr_3.0.1            \n[28] grid_4.4.1              fansi_1.0.6             gdtools_0.4.0          \n[31] colorspace_2.1-1        extrafontdb_1.0         scales_1.3.0           \n[34] cli_3.6.3               rmarkdown_2.28          ragg_1.3.3             \n[37] generics_0.1.3          rstudioapi_0.16.0       bigmemory.sri_0.1.8    \n[40] reshape2_1.4.4          tzdb_0.4.0              parallel_4.4.1         \n[43] fastglm_0.0.3           vctrs_0.6.5             jsonlite_1.8.9         \n[46] fontBitstreamVera_0.1.1 hms_1.1.3               systemfonts_1.1.0      \n[49] glue_1.7.0              bigmemory_4.6.4         stringi_1.8.4          \n[52] gtable_0.3.5            munsell_0.5.1           pillar_1.9.0           \n[55] htmltools_0.5.8.1       biometrics_1.2.4        openssl_2.2.2          \n[58] R6_2.5.1                textshaping_0.4.0       kableExtra_1.4.0       \n[61] evaluate_1.0.0          lattice_0.22-6          haven_2.5.4            \n[64] cards_0.2.2             backports_1.5.0         broom_1.0.7            \n[67] snakecase_0.11.1        fontLiberation_0.1.0    Rcpp_1.0.13            \n[70] zip_2.3.1               uuid_1.2-1              svglite_2.1.3          \n[73] gridExtra_2.3           nlme_3.1-164            Rttf2pt1_1.3.12        \n[76] officer_0.6.6           xfun_0.47               pkgconfig_2.0.3"
  },
  {
    "objectID": "posts/2025-07-03_large_scale_project/large_scale_project.html",
    "href": "posts/2025-07-03_large_scale_project/large_scale_project.html",
    "title": "Large-scale projects",
    "section": "",
    "text": "A recent collaboration of ours, led by Dr Melissa Licari of The Kids, involved producing a summary report detailing the challenges faced by individuals (and the families that care for them) living with Tourette Syndrome (TS) and other tic disorders. The full report can be found here.\n\n\n\n\n\n\nFigure 1: Report cover\n\n\n\nThe report presented the analysis of a national survey undertaken in 2023/2024. The data analysis and preparation of figures for publication was undertaken by the Biostatistics team at The Kids.\nTo support production of the final publication there was considerable behind-the-scenes work, including resolving various challenges common to a large-scale project like this, where there are many variables and outcomes of interest. For example:\n\nSurvey coverage of 7 impact domains, each comprised of existing validated questionnaires as well as bespoke questions with a variety of response types (e.g. multiple choice, radio buttons, free-text fields)\nSynthesis of data from 3 different distinct surveys (adults with tics, parents of children with tics, carers of adults with tics) that had a large overlap in terms of item/theme content but not necessarily identically matched response choices\nOnly partial survey completion by many respondents\n\nUltimately, the collaborators required the preparation of high-quality PDF output that met the needs of the report production team whilst conforming to The Kids style guidelines.\nWith a target endpoint that was rather different to the journal articles we Biostatisticians are typically preparing output for. Our experience with this TS collaboration prompted us to share our approach to tackling larger-scale projects like this, documenting elements of the processes followed in the hope they provide some insight to others (or ourselves in the future)!\nWhile this post leans heavily into the use of our package, the outlined principles and insights should have much broader appeal."
  },
  {
    "objectID": "posts/2025-07-03_large_scale_project/large_scale_project.html#ongoing-challenges",
    "href": "posts/2025-07-03_large_scale_project/large_scale_project.html#ongoing-challenges",
    "title": "Large-scale projects",
    "section": "Ongoing Challenges",
    "text": "Ongoing Challenges\nThere is always a balance of time investment and where to draw the line when attempting to further one’s technical prowess, particularly with R coding!\nWhile often times a quick google or ChatGPT will help you find a solution relatively quickly, we found here that there seemed to be a lack of clear (complete) online guidance around PDF exporting options and resolving output discrepancies (embedding issues, default fonts dependency on machine/OS, CMYK for printing vs HTML colour) - fodder for another post!"
  },
  {
    "objectID": "posts/2025-07-03_large_scale_project/large_scale_project.html#acknowledgements",
    "href": "posts/2025-07-03_large_scale_project/large_scale_project.html#acknowledgements",
    "title": "Large-scale projects",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Wes Billingham, Zac Dempsey, and Robin Cook for providing feedback on and reviewing this post."
  },
  {
    "objectID": "posts/2025-07-03_large_scale_project/large_scale_project.html#reproducibility-information",
    "href": "posts/2025-07-03_large_scale_project/large_scale_project.html#reproducibility-information",
    "title": "Large-scale projects",
    "section": "Reproducibility Information",
    "text": "Reproducibility Information\nThe session information can also be seen below.\n\n\nR version 4.5.0 (2025-04-11)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.6.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Australia/Perth\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] thekidsbiostats_1.4.1 extrafont_0.20        flextable_0.9.10     \n [4] gtsummary_2.4.0       lubridate_1.9.4       forcats_1.0.1        \n [7] stringr_1.5.2         dplyr_1.1.4           purrr_1.1.0          \n[10] readr_2.1.5           tidyr_1.3.1           tibble_3.3.0         \n[13] ggplot2_4.0.0         tidyverse_2.0.0      \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6            xfun_0.53               htmlwidgets_1.6.4      \n [4] tzdb_0.5.0              vctrs_0.6.5             tools_4.5.0            \n [7] generics_0.1.4          pkgconfig_2.0.3         data.table_1.17.8      \n[10] RColorBrewer_1.1-3      S7_0.2.0                uuid_1.2-1             \n[13] lifecycle_1.0.4         compiler_4.5.0          farver_2.1.2           \n[16] textshaping_1.0.3       janitor_2.2.1           snakecase_0.11.1       \n[19] httpuv_1.6.16           fontquiver_0.2.1        fontLiberation_0.1.0   \n[22] htmltools_0.5.8.1       yaml_2.3.10             Rttf2pt1_1.3.14        \n[25] extrafontdb_1.1         later_1.4.4             pillar_1.11.1          \n[28] openssl_2.3.4           mime_0.13               fontBitstreamVera_0.1.1\n[31] tidyselect_1.2.1        zip_2.3.3               digest_0.6.37          \n[34] stringi_1.8.7           labelled_2.15.0         fastmap_1.2.0          \n[37] grid_4.5.0              cli_3.6.5               magrittr_2.0.4         \n[40] patchwork_1.3.2         withr_3.0.2             gdtools_0.4.4          \n[43] scales_1.4.0            promises_1.3.3          timechange_0.3.0       \n[46] rmarkdown_2.30          officer_0.7.0           askpass_1.2.1          \n[49] ragg_1.5.0              hms_1.1.3               shiny_1.11.1           \n[52] evaluate_1.0.5          knitr_1.50              haven_2.5.5            \n[55] rlang_1.1.6             Rcpp_1.1.0              xtable_1.8-4           \n[58] glue_1.8.0              xml2_1.4.0              rstudioapi_0.17.1      \n[61] jsonlite_2.0.0          R6_2.6.1                systemfonts_1.3.1      \n[64] fs_1.6.6                shinyFiles_0.9.3"
  },
  {
    "objectID": "posts/2025-01-23_thekids_package/thekids_package.html",
    "href": "posts/2025-01-23_thekids_package/thekids_package.html",
    "title": "An Overview of our R Package: ‘thekidsbiostats’",
    "section": "",
    "text": "We created a package!\nA major attraction of the R programming language is its vast library of free packages, which can be found on both CRAN (the Comprehensive R Archive Network) or hosted on other sites such as GitHub. These packages can provide ready-made functions, datasets and/or templates for all kinds of applications (some staggeringly niche). In the field of statistics and data analysis, it is rare to encounter any methodology, model and/or algorithm which has not also been implemented somewhere in some R package.\nPackages can also be easily developed for use within teams or individual use, to automate or simplify tasks which appear often.\n\n\nIn summary, the package provides:\n\nFunctions to automate and/or simplify regular/routine tasks in our workflow.\nFunctions and Quarto templates which apply consistent formatting and theming to documents, ggplots, tables, etc.\n\nA complete list of functions, with brief descriptions, is found in the table below. The rest of this post will provide some worked examples for many of the functions in this list.\n\n\nFunctionDescriptionclean_REDCapCleans REDCap data exports by applying factor levels and converting column classes per a REDCap data dictionary.create_projectCreates a folder structure for a new project, as well as an R Project file and basic R scripts. Example.create_templateCreates a Quarto document from one of several template (extention) choices. Example.fct_case_whenLike dplyr::case_when but the output variable is of class factor, ordered based on the order entered into the case_when statement. Example.round_dfRounds all numeric columns in a data frame or tibble, preserving trailing zeroes. Example.round_vecRounds all values in a vector, preserving trailing zeroes. Example.scale_color_thekidsggplot scale_color theme for applying “The Kids”-themed colours.scale_fill_thekidsggplot scale_fill theme for applying “The Kids”-themed colours.thekids_modelGenerates a collection of informative model output in a structured list when provided data and model specification. Example.thekids_model_outputGenerates a collection of informative model output in a structured list when provided with an existing model object.thekids_tableCreates a formatted flextable with “The Kids” themed colours and fonts. Example.thekids_themeApplies “The Kids” theme to a ggplot object, including colours, fonts and changes to other elements for a cleaner look. Example.theme_instituteLike thekids_theme, but for historical projects completed using the Telethon Kids Institute style guide.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile the formatting functions and templates are designed for The Kids Research Institute Australia (TKRIA) within the guidelines of its style guide, they should be readily adjustable for use in other contexts with changes to specific fonts, colours, etc.\n\n\n\n\n\nThe package (and all of its code!) are available here: https://github.com/The-Kids-Biostats/thekidsbiostats.\nIf you would like to follow along with the examples below, simply run the code below:\n\nremotes::install_github(\"https://github.com/The-Kids-Biostats/thekidsbiostats\")\n\nlibrary(thekidsbiostats)"
  },
  {
    "objectID": "posts/2025-01-23_thekids_package/thekids_package.html#what-does-thekidsbiostats-do",
    "href": "posts/2025-01-23_thekids_package/thekids_package.html#what-does-thekidsbiostats-do",
    "title": "An Overview of our R Package: ‘thekidsbiostats’",
    "section": "",
    "text": "In summary, the package provides:\n\nFunctions to automate and/or simplify regular/routine tasks in our workflow.\nFunctions and Quarto templates which apply consistent formatting and theming to documents, ggplots, tables, etc.\n\nA complete list of functions, with brief descriptions, is found in the table below. The rest of this post will provide some worked examples for many of the functions in this list.\n\n\nFunctionDescriptionclean_REDCapCleans REDCap data exports by applying factor levels and converting column classes per a REDCap data dictionary.create_projectCreates a folder structure for a new project, as well as an R Project file and basic R scripts. Example.create_templateCreates a Quarto document from one of several template (extention) choices. Example.fct_case_whenLike dplyr::case_when but the output variable is of class factor, ordered based on the order entered into the case_when statement. Example.round_dfRounds all numeric columns in a data frame or tibble, preserving trailing zeroes. Example.round_vecRounds all values in a vector, preserving trailing zeroes. Example.scale_color_thekidsggplot scale_color theme for applying “The Kids”-themed colours.scale_fill_thekidsggplot scale_fill theme for applying “The Kids”-themed colours.thekids_modelGenerates a collection of informative model output in a structured list when provided data and model specification. Example.thekids_model_outputGenerates a collection of informative model output in a structured list when provided with an existing model object.thekids_tableCreates a formatted flextable with “The Kids” themed colours and fonts. Example.thekids_themeApplies “The Kids” theme to a ggplot object, including colours, fonts and changes to other elements for a cleaner look. Example.theme_instituteLike thekids_theme, but for historical projects completed using the Telethon Kids Institute style guide.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile the formatting functions and templates are designed for The Kids Research Institute Australia (TKRIA) within the guidelines of its style guide, they should be readily adjustable for use in other contexts with changes to specific fonts, colours, etc."
  },
  {
    "objectID": "posts/2025-01-23_thekids_package/thekids_package.html#how-to-install-the-package",
    "href": "posts/2025-01-23_thekids_package/thekids_package.html#how-to-install-the-package",
    "title": "An Overview of our R Package: ‘thekidsbiostats’",
    "section": "",
    "text": "The package (and all of its code!) are available here: https://github.com/The-Kids-Biostats/thekidsbiostats.\nIf you would like to follow along with the examples below, simply run the code below:\n\nremotes::install_github(\"https://github.com/The-Kids-Biostats/thekidsbiostats\")\n\nlibrary(thekidsbiostats)"
  },
  {
    "objectID": "posts/2025-01-23_thekids_package/thekids_package.html#helper-functions",
    "href": "posts/2025-01-23_thekids_package/thekids_package.html#helper-functions",
    "title": "An Overview of our R Package: ‘thekidsbiostats’",
    "section": "Helper Functions",
    "text": "Helper Functions\n\ncreate_project\nThis is the first function called when we begin a new project. It creates a folder structure based on parameters it is given, and an R Project file. The real power of this function comes from the ext_name parameter, which allows various templates (or ‘extensions’) to be used for the project. So far, we have a ‘basic’ extension and a ‘targets’ extension (which adds files and folders used by the targets package). Other extensions are in the works for specific types of projects.\nUpon calling create_project(), a pop-up will appear allowing the user to choose where the new project should be located:\n\ncreate_project(project_name = \"package_demo\", ext_name = \"targets\",\n               docs = F)\n\n\nIn this instance, we create a project called “package_demo”. Both the project directory and the R project (.Rproj) file take this name. We choose the “targets” extension, which means our directory starts with a “_targets.R” file and a “scripts” folder (had we chosen the “basic” extension, neither of these objects would be created).\n\nAdditionally, the function has the data, data-raw, admin, reports and docs parameters. These can be set to either TRUE or FALSE to include these folders in the project directory or not (default for all is TRUE). Note that we set “docs” to FALSE and so it does not appear in our directory above.\nThe “reports” directory created by create_project() is the default location for Quarto templates to be placed when calling create_template(), which we look at now…\n\n\ncreate_template\nThe purpose of create_template() is to generate ready-to-populate Quarto templates with “The Kids styling” already applied. Two templates are possible using the ext_name parameter: html and word, corresponding to the report format that we desire.\nWe can call it below, telling it to create the document in the “package_demo/reports” folder we created using create_project() above.\n\ncreate_template(file_name = \"demo_report\", directory = \"package_demo/reports/\")\n\nThe resulting quarto document looks like this:\n \nAnd rendered:\n\n\n\nthekids_model\nWe have a dedicated post exploring the use of thekids_model.\nBriefly, this function provides a standardised list of outputs for commonly used models in R. This includes the model itself, diagnostics, marginal means, and clean tables of typical model output (coefficients, confidence intervals, p-values etc). The above blog post details exactly how to display these nicely in a Quarto html document.\n\n\nround_vec and round_df\nThese two functions are examples where the sole purpose is keep code concise and to the point. Often when we present figures whether it is in plots, tables or in written form, we wish to preserve trailing zeroes when rounding numbers to a x decimal places.\nBelow we demonstrate the difference between the base R round() and round_vec() when rounding to decimal places.\n\n\noriginalroundround_vec1.80031.81.801.999822.002.58122.582.58\n\n\nround_vec() consistently has 2 decimal places, whereas round() drops any trailing zeroes, which is often undesirable for presentation.\nround_df() can be called on a data frame or tibble to round every numeric column in that object at once.\n\n\nfct_case_when\nIn a similar vein to the above function, fct_case_when is essentially dplyr::case_when but with one handy addition: the resulting vector is a factor where the levels are ordered in the same order they are defined within the case_when statement.\nThe factor levels are the result of the default case_when() function combined with as.factor(), and are simply ordered alphabetically.\n\nx &lt;- 1:50\ncase_when(\n  x %% 35 == 0 ~ \"fizz buzz\",\n  x %% 5 == 0 ~ \"fizz\",\n  x %% 7 == 0 ~ \"buzz\",\n  TRUE ~ \"everything else\"\n) %&gt;% as.factor %&gt;% levels\n\n[1] \"buzz\"            \"everything else\" \"fizz\"            \"fizz buzz\"      \n\n\nIn comparision, fct_case_when() orders the factor levels based on the order of their appearance in the argument:\n\nx &lt;- 1:50\nthekidsbiostats::fct_case_when(\n  x %% 35 == 0 ~ \"fizz buzz\",\n  x %% 5 == 0 ~ \"fizz\",\n  x %% 7 == 0 ~ \"buzz\",\n  TRUE ~ \"everything else\"\n) %&gt;% levels\n\n[1] \"fizz buzz\"       \"fizz\"            \"buzz\"            \"everything else\"\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is really handy for ensuring variables appear in the desired order rather than alphabetical, without the need for double handling!"
  },
  {
    "objectID": "posts/2025-01-23_thekids_package/thekids_package.html#theming",
    "href": "posts/2025-01-23_thekids_package/thekids_package.html#theming",
    "title": "An Overview of our R Package: ‘thekidsbiostats’",
    "section": "Theming",
    "text": "Theming\nUsing functions that automatically apply a set of formatting options to plots and tables saves time, allowing us to focus on the analysis and interpretation. Code also looks a lot cleaner when those 10 lines of ggplot per plot are automated away. Importantly, these functions also ensure a polished and consistent appearance across our team, so that output looks the same irrespective of who generated it.\n\nthekids_theme\nThe thekids_theme() function applies consistent theming to ggplot2 visualizations. It uses a clean, minimal aesthetic with fonts and colors that align with The Kids branding. Here’s an example of a plot before themeing:\n\nggplot(mtcars, aes(x = mpg, y = wt, col = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(x = \"Miles per Gallon\", y = \"Weight\", col = \"Cylinders\")\n\n\n\n\n\n\n\n\n\nggplot(mtcars, aes(x = mpg, y = wt, col = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(x = \"Miles per Gallon\", y = \"Weight\", col = \"Cylinders\") +\n  thekids_theme()\n\n\n\n\n\n\n\n\n\n\nthekids_table\nthekids_table() produces tables styled with The Kids branding and is powered by the flextable package. This includes by default applying the Barlow font, compact formatting (our preference!), and zebra-striping for readability, though these can all be altered/disabled via parameters.\nFor example, a raw table output from the mtcars dataset looks like this:\n\nhead(mtcars, 5)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n\nNow, applying thekids_table() transforms it into a clean, visually appealing format with branding elements:\n\nhead(mtcars, 5) %&gt;%\n  thekids_table(colour = \"Saffron\", font.size = 10)\n\nmpgcyldisphpdratwtqsecvsamgearcarb21.061601103.902.62016.46014421.061601103.902.87517.02014422.84108933.852.32018.61114121.462581103.083.21519.44103118.783601753.153.44017.020032\n\n\nThis outputs a compact, zebra-striped table ready for inclusion in Word or HTML reports. However, the padding and striped options can be changed if we would prefer some more space without any stripes:\n\nhead(mtcars, 5) %&gt;%\n  thekids_table(colour = \"Saffron\", font.size = 10, padding = 4, striped = F)\n\nmpgcyldisphpdratwtqsecvsamgearcarb21.061601103.902.62016.46014421.061601103.902.87517.02014422.84108933.852.32018.61114121.462581103.083.21519.44103118.783601753.153.44017.020032\n\n\nBoth thekids_theme() and thekids_table provide a host of options via parameters given to the functions.\n\n\n\n\n\n\nNote\n\n\n\nIf we wish to use only the colours for any reason (such as some non-ggplot visualisation that accepts a colour argument) we can access them directly:\n\nthekidsbiostats::thekids_colours\n\n        Saffron         Pumpkin            Teal        DarkTeal   CelestialBlue \n      \"#F1B434\"       \"#F56B00\"       \"#00A39C\"       \"#00807A\"       \"#4A99DE\" \n      AzureBlue    MidnightBlue       Saffron50       Pumpkin50          Teal50 \n      \"#426EA8\"       \"#1F3B73\"       \"#F8DA9A\"       \"#FAB580\"       \"#80D1CE\" \nCelestialBlue50  MidnightBlue50        CoolGrey      CoolGrey50      CoolGrey20 \n      \"#A5CCEF\"       \"#8F9DB9\"       \"#565F5F\"       \"#ABAFAF\"       \"#EEEFEF\""
  },
  {
    "objectID": "posts/2025-01-23_thekids_package/thekids_package.html#acknowledgements",
    "href": "posts/2025-01-23_thekids_package/thekids_package.html#acknowledgements",
    "title": "An Overview of our R Package: ‘thekidsbiostats’",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Matt Cooper, Zac Dempsey and Elizabeth McKinnon for providing feedback on and reviewing this post.\n\nAI Usage Note\nThe majority of this post and code were produced by the author. AI tools were used to refine the structure and wording."
  },
  {
    "objectID": "posts/2025-01-23_thekids_package/thekids_package.html#reproducibility-information",
    "href": "posts/2025-01-23_thekids_package/thekids_package.html#reproducibility-information",
    "title": "An Overview of our R Package: ‘thekidsbiostats’",
    "section": "Reproducibility Information",
    "text": "Reproducibility Information\nTo access the .qmd (Quarto markdown) files as well as any R scripts or data that was used in this post, please visit our GitHub:\nhttps://github.com/The-Kids-Biostats/The-Kids-Biostats.github.io/tree/main/posts/\nThe session information can also be seen below.\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Australia/Perth\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] kableExtra_1.4.0      thekidsbiostats_0.0.1 flextable_0.9.7      \n [4] gtsummary_2.0.4       extrafont_0.19        Hmisc_5.2-1          \n [7] lubridate_1.9.4       forcats_1.0.0         stringr_1.5.1        \n[10] dplyr_1.1.4           purrr_1.0.2           readr_2.1.5          \n[13] tidyr_1.3.1           tibble_3.2.1          ggplot2_3.5.1        \n[16] tidyverse_2.0.0      \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6            xfun_0.50               htmlwidgets_1.6.4      \n [4] tzdb_0.4.0              vctrs_0.6.5             tools_4.3.3            \n [7] generics_0.1.3          cluster_2.1.8           pkgconfig_2.0.3        \n[10] data.table_1.16.4       checkmate_2.3.2         uuid_1.2-1             \n[13] lifecycle_1.0.4         farver_2.1.2            compiler_4.3.3         \n[16] textshaping_0.4.1       munsell_0.5.1           janitor_2.2.1          \n[19] snakecase_0.11.1        fontquiver_0.2.1        fontLiberation_0.1.0   \n[22] htmltools_0.5.8.1       yaml_2.3.10             Rttf2pt1_1.3.12        \n[25] htmlTable_2.4.3         Formula_1.2-5           pillar_1.10.1          \n[28] extrafontdb_1.0         openssl_2.3.1           rpart_4.1.24           \n[31] fontBitstreamVera_0.1.1 zip_2.3.1               tidyselect_1.2.1       \n[34] digest_0.6.37           stringi_1.8.4           labeling_0.4.3         \n[37] labelled_2.14.0         fastmap_1.2.0           grid_4.3.3             \n[40] ftExtra_0.6.4           colorspace_2.1-1        cli_3.6.3              \n[43] magrittr_2.0.3          base64enc_0.1-3         foreign_0.8-87         \n[46] withr_3.0.2             gdtools_0.4.1           scales_1.3.0           \n[49] backports_1.5.0         timechange_0.3.0        rmarkdown_2.29         \n[52] officer_0.6.7           nnet_7.3-20             gridExtra_2.3          \n[55] ragg_1.3.3              askpass_1.2.1           hms_1.1.3              \n[58] evaluate_1.0.3          haven_2.5.4             knitr_1.49             \n[61] viridisLite_0.4.2       rlang_1.1.4             Rcpp_1.0.14            \n[64] glue_1.8.0              xml2_1.3.6              svglite_2.1.3          \n[67] rstudioapi_0.17.1       jsonlite_1.8.9          R6_2.5.1               \n[70] systemfonts_1.1.0"
  },
  {
    "objectID": "posts/2024-12-16_bidirectional_likert_visualisation/bidirectional_likert_visualisations.html",
    "href": "posts/2024-12-16_bidirectional_likert_visualisation/bidirectional_likert_visualisations.html",
    "title": "Visualisations for (bidirectional) Likert scale data",
    "section": "",
    "text": "Bidirectional Likert-style questions are widely employed as a survey instrument within research. Unlike ordinal Likert-style responses, which are often used to broadly obtain quantities (e.g., “glasses of water per day”) or ratings (e.g., “rate the food between 1 and 10”), bidirectional Likert-style responses are useful for assessing attitudes, perceptions or satisfaction with a particular policy or program.\nIn this post, similar to our previous blog post on ordinal Likert-style data, we are interested in “pre/post” data (collected before and after some intervention) and we are aiming to find a concise way to visualise and summarise the responses at each time point - as well as the change in responses between time points."
  },
  {
    "objectID": "posts/2024-12-16_bidirectional_likert_visualisation/bidirectional_likert_visualisations.html#definitions",
    "href": "posts/2024-12-16_bidirectional_likert_visualisation/bidirectional_likert_visualisations.html#definitions",
    "title": "Visualisations for (bidirectional) Likert scale data",
    "section": "Definitions",
    "text": "Definitions\nBidirectional (bipolar) Likert data involves category responses with a natural ordering of responses from two opposing directions—typically both negative and positive responses—around a central (or neutral) point.\nAn example. “The amount of reading I do influences how much reading my child does?” with response options:\n\nStrongly disagree\nDisagree\nNeither agree nor disagree (the neutral midpoint)\nAgree\nStrongly agree"
  },
  {
    "objectID": "posts/2024-12-16_bidirectional_likert_visualisation/bidirectional_likert_visualisations.html#demo-data",
    "href": "posts/2024-12-16_bidirectional_likert_visualisation/bidirectional_likert_visualisations.html#demo-data",
    "title": "Visualisations for (bidirectional) Likert scale data",
    "section": "Demo data",
    "text": "Demo data\nLet’s start with simulating a single survey question, before (pre) and after (post) an intervention, for participants in either an intervention or control program.\nWe would like to visualise and summarise the responses both before and after the program (separately), as well as the change in responses between time periods (increase/no change/decrease), across both the intervention and control programs.\n\n\nCode\nlibrary(thekidsbiostats) # install with remotes::install_github(\"The-Kids-Biostats/thekidsbiostats\")\n\nset.seed(123) # For reproducibility\n\n# Parameters\nq   &lt;- 1          # Number of bidirectional Likert questions to simulate\nn_i &lt;- 200        # Number of intervention respondents\nn_c &lt;- 150        # Number of control respondents\nn   &lt;- n_i + n_c  # Total number of participants\n\n# Define Likert scale labels\nlikert_labels &lt;- c(\"Strongly disagree\", \"Disagree\", \"Neutral\", \"Agree\", \"Strongly agree\")\n\n# Simulate pre-survey responses (randomly sampled from the Likert scale 1-5)\npre &lt;- matrix(sample(1:5, n * q, replace = TRUE), \n              nrow = n, ncol = q)\n\n# Function to simulate post-survey responses with both improvement and decline possibilities\nsimulate_post &lt;- function(pre_response) {\n  # Define possible changes with corresponding probabilities\n  possible_changes &lt;- c(-3, -2, -1, 0, 1, 2, 3)\n  probabilities &lt;- c(0.05, 0.1, 0.15, 0.4, 0.15, 0.1, 0.05)  # \"No change\" is most likely\n  \n  post_response &lt;- pre_response + sample(possible_changes, \n                                         1, \n                                         prob = probabilities)\n  \n  # Ensure post_response remains within the bounds of the Likert scale (1 to 5)\n  post_response &lt;- max(1, min(5, post_response))\n  \n  return(post_response)\n}\n\n# Simulate post-survey responses\npost &lt;- apply(pre, 2, function(x) sapply(x, simulate_post))\n\n# Convert numeric values to factors with labels\npre  &lt;- apply(pre,  2, factor, levels = 1:5, labels = likert_labels)\npost &lt;- apply(post, 2, factor, levels = 1:5, labels = likert_labels)\n\n# Combine the pre and post data\ndat_likert &lt;- data.frame(id = 1:n,\n                         mode = c(rep(\"Intervention\", n_i), rep(\"Control\", n_c)),\n                         pre,\n                         post)\n\n# Rename the columns to reflect pre and post data\ncolnames(dat_likert)[3:(2 + q)] &lt;- paste0(\"Q\", 1:q, \"_pre\")\ncolnames(dat_likert)[(3 + q):(2 + 2 * q)] &lt;- paste0(\"Q\", 1:q, \"_post\")\n\ndat_likert &lt;- dat_likert %&gt;%\n  as_tibble %&gt;%\n  pivot_longer(cols = contains(\"Q1\"), names_to = \"time\", names_prefix = \"Q1_\", values_to = \"Q1\") %&gt;%\n  mutate(time = factor(time, c(\"pre\", \"post\")),\n         Q1 = factor(Q1, likert_labels))\n\n\nVisualising the first few rows of data:\n\nhead(dat_likert) %&gt;%\n  thekids_table(colour = \"Saffron\", padding = 3)\n\nidmodetimeQ11InterventionpreNeutral1InterventionpostStrongly disagree2InterventionpreNeutral2InterventionpostAgree3InterventionpreDisagree3InterventionpostStrongly disagree"
  },
  {
    "objectID": "posts/2024-12-16_bidirectional_likert_visualisation/bidirectional_likert_visualisations.html#the-visualisation",
    "href": "posts/2024-12-16_bidirectional_likert_visualisation/bidirectional_likert_visualisations.html#the-visualisation",
    "title": "Visualisations for (bidirectional) Likert scale data",
    "section": "The visualisation",
    "text": "The visualisation\nWe have created a function to visualise these data at each period, separately for the intervention and controls groups. As we presented in another previous blog), functions can be a useful way to compartmentalise and generalise code, and to save on “copy and pasting” (potentially lengthy) chunks that are otherwise largely very similar.\nThis function is bidirect_plot().\nSome of the arguments for this function are:\n\ngrouping_var – Variable that defines how to group data, within a single plot, on the y-axis.\nfacet_var – Variable that defines separate facets (panels) in the plot.\noutcome_var – Variable that defines the question of interest.\nlabel_threshold – Minimum proportion threshold (as a percentage) for displaying labels on the bars.\nfill_colour – Vector of colours used for the segments of the stacked bars.\nhline – A logical flag (TRUE/FALSE) to determine whether a horizontal dashed line should be added to the plot.\nhline_threshold – If hline is TRUE, the x-axis position of the horizontal line.\n... – Additional arguments passed to thekidsbiostats::theme_institute(), allowing customization of the plot theme.\n\n\n\nNote:\nWe called this hline even though the line is vertical. That is because within the code, we use coord_flip() to turn the originally vertical plot into a horizontal plot.\nAll other arguments merely control other formatting options such as text size, label rounding, legend positioning, etc.\nClick below to expand and review the code within this function.\n\n\nCode\nbidirect_plot &lt;- function(data,\n                          grouping_var,\n                          facet_var,\n                          outcome_var,\n                          base_size = 14,\n                          legend_size = base_size - 4,\n                          axis_y_size = base_size,\n                          label_size = 3,\n                          label_threshold = 5,\n                          label_round = 1,\n                          fill_colour = c(\"#f56b00\", \"#fab580\", \"#eeefef\",\"#a5ccef\", \"#4a99de\"),\n                          axis_y_label_lineheight = 0.6,\n                          legend.position = \"bottom\",\n                          legend.title.position = \"left\",\n                          hline = TRUE,\n                          hline_threshold = 50,\n                          ...){\n\n  # Reverse the `grouping_var` factors for consistent colouring\n  data &lt;- data %&gt;%\n    mutate(!!sym(grouping_var) := fct_rev(!!sym(grouping_var)))\n  \n  # Calculate the counts and proportions for each group\n  plot_dat &lt;- data %&gt;%\n    count(!!sym(outcome_var), \n          !!sym(facet_var), \n          !!sym(grouping_var)) %&gt;%\n    group_by(!!sym(facet_var), \n             !!sym(grouping_var)) %&gt;%\n    mutate(prop = 100*n/sum(n),\n           pos = cumsum(prop) - 0.5*prop) %&gt;%\n    ungroup %&gt;%\n    mutate(!!sym(outcome_var) := fct_rev(!!sym(outcome_var)))\n  \n  # Create the plot!\n  suppressMessages({\n    plot &lt;- plot_dat %&gt;%\n      ggplot(aes(x = !!sym(grouping_var), \n                 y = prop, \n                 fill = !!sym(outcome_var))) +\n      geom_bar(stat = \"identity\", \n               position = \"stack\",\n               width = 0.7) +\n      geom_text(aes(label = ifelse(prop &gt;= label_threshold, \n                                   paste0(n, \" (\", round_vec(prop, label_round), \"%)\"), \"\"), \n                    y = pos),\n                size = label_size,\n                family = \"Barlow\") +\n      facet_wrap(as.formula(paste(\"~\", facet_var)), \n                 ncol = 1) +\n      coord_flip() +\n      thekidsbiostats::theme_institute(base_size = base_size, \n                                       ...) +\n      scale_fill_manual(values = rev(fill_colour)) +\n      scale_y_continuous(expand = c(0, 0)) + # y axis in-line with plot labels (no spacing)\n      scale_x_discrete(expand = expansion(add = c(0.5, 0.5))) + # Spacing between bar and plot area (bottom, top) \n      \n      guides(fill = guide_legend(title = \"Response\",\n                                 reverse = TRUE)) +\n      theme(# x axis formatting\n            axis.title.x = element_blank(),\n            axis.ticks.x = element_blank(),\n            axis.text.x = element_blank(),\n            \n            # y axis formatting\n            axis.title.y = element_blank(),\n            axis.text.y = element_text(size = axis_y_size, \n                                       lineheight = axis_y_label_lineheight),\n            \n            # Legend formatting\n            legend.position = legend.position,\n            legend.title.position = legend.title.position,\n            legend.text = element_text(size = legend_size),\n            \n            # Miscellaneous formatting\n            panel.grid.major.x = element_blank(),\n            plot.margin = margin(t = 10, r = 15, b = 5, l = 15))\n    })\n  \n  if (hline == TRUE){\n    plot &lt;- plot +\n      geom_hline(yintercept = hline_threshold, \n                 linetype = \"dashed\", \n                 colour = \"red\")\n  } else if (hline == FALSE){\n    plot &lt;- plot\n  }\n  \n  return(plot)\n  }\n\n\nAnd then, simply calling this function and passing data into it returns our plot:\n\nbidirect_plot(data = dat_likert,\n              outcome_var = \"Q1\",\n              grouping_var = \"time\",\n              facet_var = \"mode\")"
  },
  {
    "objectID": "posts/2024-12-16_bidirectional_likert_visualisation/bidirectional_likert_visualisations.html#change-between-time-periods",
    "href": "posts/2024-12-16_bidirectional_likert_visualisation/bidirectional_likert_visualisations.html#change-between-time-periods",
    "title": "Visualisations for (bidirectional) Likert scale data",
    "section": "Change between time periods",
    "text": "Change between time periods\nLet’s define the change in agreement between time periods as follows:\n\nIncrease – The individual agrees more after the program compared to before the program (e.g., a response moving from “disagree” to “strongly agree”).\nNo change – No change in agreement before and after the program (e.g., responding “neutral” at both time periods).\nDecrease – The individual agrees less after the program compared to before the program (e.g., a response moving from “strongly agree” to “agree”).\n\nTo implement this, we will construct another function, bidirect_change.\n\n\nCode\nbidirect_change &lt;- function(data, \n                            id_var = \"id\",\n                            mode_var = \"mode\",\n                            time_var = \"time\", \n                            pre_label = \"pre\",\n                            post_label = \"post\",\n                            outcome_var){\n  \n  data %&gt;%\n    pivot_wider(id_cols = c(!!rlang::sym(id_var), \n                            !!rlang::sym(mode_var)),\n                names_from = !!rlang::sym(time_var),\n                values_from = !!rlang::sym(outcome_var)) %&gt;%\n    mutate(Change = thekidsbiostats::fct_case_when(as.numeric(!!rlang::sym(pre_label)) &lt;  as.numeric(!!rlang::sym(post_label)) ~ \"Increase\",\n                                                   as.numeric(!!rlang::sym(pre_label)) == as.numeric(!!rlang::sym(post_label)) ~ \"No change\",\n                                                   as.numeric(!!rlang::sym(pre_label)) &gt;  as.numeric(!!rlang::sym(post_label)) ~ \"Decrease\"))\n  }\n\n\n\ndat_likert_change &lt;- bidirect_change(data = dat_likert, \n                                     outcome_var = \"Q1\")\n\nNow, summarising the change in responses for the “intervention” and “control” groups:\n\n\nCode\ndat_likert_change %&gt;%\n  select(mode, Change) %&gt;%\n  tbl_summary(by = mode) %&gt;%\n  thekids_table(colour = \"Saffron\") \n\n\nCharacteristicControl  N = 1501Intervention  N = 2001ChangeIncrease35 (23%)47 (24%)No change78 (52%)103 (52%)Decrease37 (25%)50 (25%)1n (%)\n\n\nWe can further contextualise these results by stratifying the change in responses by response selected in the pre time period. This can be really useful to see which pre groups are the biggest movers.\nTo stick with the “function-writing” theme of this blog, let’s define another stratified summary table function, bidirect_table.\n\n\nCode\nbidirect_table &lt;- function(data,\n                           labels = labels,\n                           strata_var = \"mode\",\n                           by_var = \"pre\",\n                           overall_last = F,\n                           overall_label = \"Overall\",\n                           spanning_header_label = paste0(\"Pre response\"),\n                           ...){\n  \n    tab &lt;- data %&gt;%\n      tbl_strata(strata = !!rlang::sym(strata_var),\n                 .tbl_fun = ~.x %&gt;%\n                   tbl_summary(by = !!rlang::sym(by_var)) %&gt;%\n                   modify_header(all_stat_cols() ~ \"**{level}**\") %&gt;%\n                   add_overall(last = overall_last,\n                               col_label = paste0(\"**\", overall_label, \"**\")),\n                 .header= \"{strata} \\nN={n}\",\n                 .combine_with = \"tbl_stack\")\n    \n    tab &lt;- tab %&gt;%  \n      thekidsbiostats::thekids_table(...)\n\n    overall_ncol &lt;- length(tab$col_keys)\n    stat_ncol &lt;- length(tab$col_keys[str_detect(tab$col_keys, pattern = \"stat_\")]) - 1\n      \n    tab &lt;- tab %&gt;%\n      flextable::add_header_row(values = c(\"\", spanning_header_label), \n                                colwidths = c(overall_ncol - stat_ncol, \n                                              stat_ncol)) %&gt;%\n      flextable::align(align = \"left\", part = \"header\", i = 1)  # Set alignment to left\n    \n    return(tab)\n  }\n\n\nThis returns:\n\ndat_likert_change %&gt;%\n  select(-c(id, post)) %&gt;%\n  bidirect_table(data = .,\n                 by_var = \"pre\",\n                 colour = \"Saffron\")\n\nPre responseGroupCharacteristicOverall1Strongly disagree1Disagree1Neutral1Agree1Strongly agree1Control N=150ChangeIncrease35 (23%)6 (21%)13 (36%)9 (32%)7 (27%)0 (0%)No change78 (52%)22 (79%)12 (33%)13 (46%)9 (35%)22 (69%)Decrease37 (25%)0 (0%)11 (31%)6 (21%)10 (38%)10 (31%)Intervention N=200ChangeIncrease47 (24%)13 (28%)11 (28%)14 (38%)9 (26%)0 (0%)No change103 (52%)33 (72%)19 (48%)8 (22%)14 (41%)29 (67%)Decrease50 (25%)0 (0%)10 (25%)15 (41%)11 (32%)14 (33%)1n (%)"
  },
  {
    "objectID": "posts/2024-12-16_bidirectional_likert_visualisation/bidirectional_likert_visualisations.html#reproducibility-information",
    "href": "posts/2024-12-16_bidirectional_likert_visualisation/bidirectional_likert_visualisations.html#reproducibility-information",
    "title": "Visualisations for (bidirectional) Likert scale data",
    "section": "Reproducibility Information",
    "text": "Reproducibility Information\nTo access the .qmd (Quarto markdown) files as well as any R scripts or data that was used in this post, please visit our GitHub:\nhttps://github.com/The-Kids-Biostats/The-Kids-Biostats.github.io/tree/main/posts/\nThe session information can also be seen below.\n\n\nCode\nsessionInfo()\n\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Australia/Perth\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] thekidsbiostats_0.0.1 flextable_0.9.7       gtsummary_2.0.4      \n [4] lubridate_1.9.3       forcats_1.0.0         stringr_1.5.1        \n [7] dplyr_1.1.4           purrr_1.0.2           readr_2.1.5          \n[10] tidyr_1.3.1           tibble_3.2.1          ggplot2_3.5.1        \n[13] tidyverse_2.0.0       extrafont_0.19       \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6            xfun_0.49               htmlwidgets_1.6.4      \n [4] tzdb_0.4.0              vctrs_0.6.5             tools_4.3.3            \n [7] generics_0.1.3          fansi_1.0.6             pkgconfig_2.0.3        \n[10] data.table_1.16.2       uuid_1.2-1              lifecycle_1.0.4        \n[13] farver_2.1.2            compiler_4.3.3          biometrics_1.2.4       \n[16] textshaping_0.4.0       munsell_0.5.1           janitor_2.2.0          \n[19] snakecase_0.11.1        fontquiver_0.2.1        fontLiberation_0.1.0   \n[22] htmltools_0.5.8.1       yaml_2.3.10             Rttf2pt1_1.3.12        \n[25] pillar_1.9.0            extrafontdb_1.0         openssl_2.2.2          \n[28] fontBitstreamVera_0.1.1 tidyselect_1.2.1        zip_2.3.1              \n[31] digest_0.6.37           stringi_1.8.4           labeling_0.4.3         \n[34] labelled_2.13.0         fastmap_1.2.0           grid_4.3.3             \n[37] colorspace_2.1-1        cli_3.6.3               magrittr_2.0.3         \n[40] cards_0.4.0             utf8_1.2.4              broom_1.0.7            \n[43] withr_3.0.2             backports_1.5.0         gdtools_0.4.1          \n[46] scales_1.3.0            timechange_0.3.0        rmarkdown_2.29         \n[49] officer_0.6.7           igraph_2.1.1            askpass_1.2.1          \n[52] ragg_1.3.3              hms_1.1.3               kableExtra_1.4.0       \n[55] evaluate_1.0.1          knitr_1.49              haven_2.5.4            \n[58] viridisLite_0.4.2       rlang_1.1.4             Rcpp_1.0.13-1          \n[61] glue_1.8.0              xml2_1.3.6              svglite_2.1.3          \n[64] rstudioapi_0.17.1       jsonlite_1.8.9          R6_2.5.1               \n[67] systemfonts_1.1.0"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Kids Biostats",
    "section": "",
    "text": "Accessing national health data via API\n\n\n\n\n\n\nR\n\n\nAIHW\n\n\nGuess that plot\n\n\n\n\n\n\n\n\n\nOct 20, 2025\n\n\nDr Matt Cooper and Dr Haileab Wolde\n\n\n\n\n\n\n\n\n\n\n\n\nLarge-scale projects\n\n\n\n\n\n\nR\n\n\nOutput\n\n\nQuarto\n\n\n\n\n\n\n\n\n\nJul 3, 2025\n\n\nDr Bethy McKinnon and Dr Matt Cooper\n\n\n\n\n\n\n\n\n\n\n\n\nManaging RStudio addins using the rsam package\n\n\n\n\n\n\nR\n\n\nRStudio\n\n\naddins\n\n\n\n\n\n\n\n\n\nApr 14, 2025\n\n\nZac Dempsey and Dr Matt Cooper\n\n\n\n\n\n\n\n\n\n\n\n\nAn Overview of our R Package: ‘thekidsbiostats’\n\n\n\n\n\n\nR\n\n\nSummary\n\n\n\n\n\n\n\n\n\nJan 23, 2025\n\n\nWesley Billingham\n\n\n\n\n\n\n\n\n\n\n\n\nModels - Condensed code and output\n\n\n\n\n\n\nR\n\n\nModelling\n\n\nQuarto\n\n\n\n\n\n\n\n\n\nJan 10, 2025\n\n\nDr Matt Cooper and Wesley Billingham\n\n\n\n\n\n\n\n\n\n\n\n\nVisualisations for (bidirectional) Likert scale data\n\n\n\n\n\n\nR\n\n\nVisualisations\n\n\nLikert\n\n\nggplot\n\n\n\n\n\n\n\n\n\nDec 16, 2024\n\n\nZac Dempsey and Dr Matt Cooper\n\n\n\n\n\n\n\n\n\n\n\n\nSharing - Code v Functions\n\n\n\n\n\n\nR\n\n\nVisualisations\n\n\nLikert\n\n\nggplot\n\n\n\n\n\n\n\n\n\nNov 19, 2024\n\n\nDr Matt Cooper and Zac Dempsey\n\n\n\n\n\n\n\n\n\n\n\n\nVisualisations for (ordinal) Likert scale data\n\n\n\n\n\n\nR\n\n\nVisualisations\n\n\nLikert\n\n\nggplot\n\n\n\n\n\n\n\n\n\nOct 14, 2024\n\n\nDr Matt Cooper and Zac Dempsey\n\n\n\n\n\n\n\n\n\n\n\n\nSmall-sample analysis\n\n\n\n\n\n\nSmall Samples\n\n\nR\n\n\n\n\n\n\n\n\n\nSep 10, 2024\n\n\nDr Bethy McKinnon, Dr Matt Cooper\n\n\n\n\n\n\n\n\n\n\n\n\nAI in Biostatistics - Part 2\n\n\n\n\n\n\nAI\n\n\nR\n\n\n\n\n\n\n\n\n\nAug 9, 2024\n\n\nWesley Billingham, Dr Matthew Cooper\n\n\n\n\n\n\n\n\n\n\n\n\nAI in Biostatistics - Part 1\n\n\n\n\n\n\nAI\n\n\nR\n\n\n\n\n\n\n\n\n\nAug 8, 2024\n\n\nWesley Billingham, Dr Matthew Cooper\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysing pre-post data\n\n\n\n\n\n\nPre-post\n\n\nMixed Models\n\n\nR\n\n\n\n\n\n\n\n\n\nJul 22, 2024\n\n\nDr Matthew Cooper\n\n\n\n\n\n\n\n\n\n\n\n\nData Anonymiser\n\n\n\n\n\n\nShiny\n\n\nR\n\n\n\n\n\n\n\n\n\nJul 19, 2024\n\n\nDr Matthew Cooper, Wesley Billingham\n\n\n\n\n\n\n\n\n\n\n\n\nParallel Computing in R\n\n\n\n\n\n\nR\n\n\nParallel\n\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\nZac Dempsey\n\n\n\n\n\n\n\n\n\n\n\n\nHandling Missing Data with LMER\n\n\n\n\n\n\nMissing Data\n\n\nMixed Models\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 7, 2024\n\n\nDr Matthew Cooper\n\n\n\n\n\n\nNo matching items"
  }
]