[
  {
    "objectID": "posts/lmer-missing/index.html",
    "href": "posts/lmer-missing/index.html",
    "title": "Handling Missing Data with LMER",
    "section": "",
    "text": "As consultant statisticians, we are often approached by people who have already carried out some preliminary data analysis and who are now looking to move onto something more complex. As missing data is generally present (or rather not present!) in health-related datasets, we find this is a question that is regularly raised:\n“Can we use mixed models, since they use all available data?”\nWorking through even the basics on this topic will mean one will also have to work through challenging and varied (often cryptic) statistical nomenclature. This post is a worked example and was motivated by reading this paper. It has an interesting example of ‘missing data and mixed models’ that we thought could benefit from some figures and commentary to aid in the understanding of what is achieved."
  },
  {
    "objectID": "posts/lmer-missing/index.html#research-question",
    "href": "posts/lmer-missing/index.html#research-question",
    "title": "Handling Missing Data with LMER",
    "section": "Research question",
    "text": "Research question\nAre there differences in the rate of wage growth between males and females over time (in this dataset)?\nThat question is quite straightforward to answer here, but the motivating commentary is really around how mixed effects model can be beneficial in the presence of systematic missing (follow-up) data - with a focus on parameter estimates and their graphical interpretation.\nMissing follow-up data (lost to follow, attrition, drop out) is often seen in health research datasets. The data might be Missing At Random, it my be Missing Completely At Random, the important nuances of these are largely out of scope for this post."
  },
  {
    "objectID": "posts/lmer-missing/index.html#erroneous-basic-linear-regression-model",
    "href": "posts/lmer-missing/index.html#erroneous-basic-linear-regression-model",
    "title": "Handling Missing Data with LMER",
    "section": "Erroneous basic linear regression model",
    "text": "Erroneous basic linear regression model\nTo address the question of “are there differences between genders in the rate of wage growth”, we are going to fit a gender by time interaction term which will give us an indication of if ‘as time changes’ whether the outcome (logged wage) changes at a different rate for each gender.\n\n\nCode\nmod1 &lt;- lm(lwage ~ gender * t, data = dat)\nexport_summs(mod1, error_format = \"[{conf.low}, {conf.high}]\",\n             error_pos = \"right\", digits = 3,\n             statistics = c(N = \"nobs\"))\n\n\n\n\nModel 1\n\n(Intercept)6.340 ***[6.312, 6.368]\n\ngenderFemale-0.456 ***[-0.540, -0.372]\n\nt0.097 ***[0.091, 0.104]\n\ngenderFemale:t-0.005    [-0.024, 0.014]\n\nN4165             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nWe can see there is an effect of gender present, and an effect of time (the growth overtime we saw in the original plot), but the very small beta coefficient (relative to the scale of data we are working with) and the p-value of 0.6 are suggestive that the rate of wage growth over time does not differ significantly between genders.\nTo view this, we’re not going to use geom_smooth or stat_summary as we might do when graphing on-the-fly, rather we will use the predict() function to create data for our line of best fit.\nTo set a coding framework which we’ll use again later in the post, we’ll create a new dataset and use predictions to draw our (straight) line.\n\n\nCode\nnewdat &lt;- expand.grid(t = 1:7, gender = c(\"Male\", \"Female\")) %&gt;% \n  mutate(gender = factor(gender),\n         gender = relevel(gender, \"Male\"))\n\np_mod1 &lt;- cbind(newdat, \n                lwage = predict(mod1, newdat, interval = \"prediction\"))\n\n\n\n\nCode\ndat %&gt;% \n  ggplot(aes(t, lwage, colour = gender)) +\n  geom_point(alpha = 0.15) +\n  geom_line(aes(group = id), alpha = 0.15) + \n  facet_wrap(~ gender) +\n  geom_line(data = p_mod1, aes(y = lwage.fit), colour = \"red\", linewidth = 1) +\n  scale_colour_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  theme(legend.position=\"none\") +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\") +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9))\n\n\n\n\n\n\n\n\n\nOkay, these red fitted lines look like quite good as a ‘line of best fit’; they pass the eye test of broadly representing the trends of the data well.\nFitted with a prediction confidence interval, we see.\n\n\nCode\np_mod1 %&gt;% \n  ggplot(aes(t, lwage.fit, ymin = lwage.lwr, ymax = lwage.upr, fill = gender)) +\n  geom_ribbon(alpha = 0.5) +\n  geom_line(colour = \"black\", linewidth = 1) + \n  facet_wrap(~ gender) +\n  scale_fill_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  theme(legend.position=\"none\") +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\")  +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9)) -&gt; p1\np1\n\n\n\n\n\n\n\n\n\nThese lines look parallel - suggestive of no difference in growth rates between the genders (in line with the non-significant interaction term we saw).\nOf course, we have not adjusted for the within person correlation present in the data. The model above is inappropriate as one of the main assumptions of the model is violated - the data points are not all independent (we know there are 7 from each individual)."
  },
  {
    "objectID": "posts/lmer-missing/index.html#mixed-effects-model",
    "href": "posts/lmer-missing/index.html#mixed-effects-model",
    "title": "Handling Missing Data with LMER",
    "section": "Mixed effects model",
    "text": "Mixed effects model\nHere we run a fairly basic linear mixed effects model, the model has the same fixed effects terms as above (the interaction term we are curious about) but also includes a random effect, that is, the intercept is allowed to varied for each individual.\n\n\nCode\nmod2 &lt;- lmer(lwage ~ gender * t + (1 | id), data = dat)\nexport_summs(mod2, error_format = \"[{conf.low}, {conf.high}]\",\n             error_pos = \"right\", digits = 3,\n             statistics = c(N = \"nobs\"))\n\n\n\n\nModel 1\n\n(Intercept)6.340 ***[6.307, 6.373]\n\ngenderFemale-0.456 ***[-0.553, -0.358]\n\nt0.097 ***[0.095, 0.100]\n\ngenderFemale:t-0.005    [-0.012, 0.003]\n\nN4165             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nLets also fit the predicted values from this model.\n\n\nCode\nnewdat &lt;- expand.grid(t = 1:7, gender = c(\"Male\", \"Female\")) %&gt;% \n  mutate(gender = factor(gender),\n         gender = relevel(gender, \"Male\"),\n         id = c(rep(1, 7), rep(2, 7)))\n\nstore &lt;- simulate(mod2, seed=1, newdata=newdat, re.form=NA,\n                  allow.new.levels=T, nsim = 500)\n\np_mod2 &lt;- cbind(newdat,\n                store %&gt;% \n                  rowwise() %&gt;% \n                  mutate(fit = mean(c_across(sim_1:sim_500)),\n                         lwr = quantile(c_across(sim_1:sim_500), 0.025),\n                         upr = quantile(c_across(sim_1:sim_500), 0.975)) %&gt;% \n                  select(fit, lwr, upr))\n\n\n\n– Slight segue\nWith standard linear regression model (first used), we used predict to generate a ‘prediction interval’. With linear mixed effects models, we do no have the same function available (that will incorporate the random effects variability into the prediction interval), so rather than calculating these with a formula, we simulate! Some extra content on this can be read here or (somewhat less so) here.\nThis use of simulation is part of the reason behind the confidence intervals not being parallel."
  },
  {
    "objectID": "posts/lmer-missing/index.html#back-to-our-mixed-effects-model",
    "href": "posts/lmer-missing/index.html#back-to-our-mixed-effects-model",
    "title": "Handling Missing Data with LMER",
    "section": "Back to our Mixed effects model",
    "text": "Back to our Mixed effects model\n\n\nCode\np_mod2 %&gt;% \n  ggplot(aes(t, fit, ymin = lwr, ymax = upr, fill = gender)) +\n  geom_ribbon(alpha = 0.5) +\n  geom_line(colour = \"black\", linewidth = 1) + \n  facet_wrap(~ gender) +\n  scale_fill_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  theme(legend.position=\"none\") +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\") +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9)) -&gt; p2\np2\n\n\n\n\n\n\n\n\n\nWhen we compare the output of this model with the earlier (erroneous) model, we see two expected things.\n\n\nCode\nexport_summs(mod1, mod2,\n             error_format = \"[{conf.low}, {conf.high}]\",\n             error_pos = \"right\", digits = 3,\n             statistics = c(N = \"nobs\"),\n             model.names = c(\"Erroneous model\", \"Mixed efects model\"))\n\n\n\n\nErroneous modelMixed efects model\n\n(Intercept)6.340 ***[6.312, 6.368]6.340 ***[6.307, 6.373]\n\ngenderFemale-0.456 ***[-0.540, -0.372]-0.456 ***[-0.553, -0.358]\n\nt0.097 ***[0.091, 0.104]0.097 ***[0.095, 0.100]\n\ngenderFemale:t-0.005    [-0.024, 0.014]-0.005    [-0.012, 0.003]\n\nN4165             4165             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\n\nThe coefficients have the same value in each model as these represent the fixed effect\nThe confidence intervals for those coefficients are slightly narrower in Model 2, this is because some of the variation present [within Model 1] is explained by the random effects [present in Model 2 and not Model 1].\n\nWe can see this when we plot the predicted values side by side.\n\n\nCode\n(p1 + labs(title = \"Erroneous model\")) / (p2 + labs(title = \"Mixed effects model\"))"
  },
  {
    "objectID": "posts/lmer-missing/index.html#missing---erroneous-basic-linear-regression-model",
    "href": "posts/lmer-missing/index.html#missing---erroneous-basic-linear-regression-model",
    "title": "Handling Missing Data with LMER",
    "section": "Missing - Erroneous basic linear regression model",
    "text": "Missing - Erroneous basic linear regression model\n\n\nCode\nmod3 &lt;- lm(mlwage ~ gender * t, data = mdat)\nexport_summs(mod3, error_format = \"[{conf.low}, {conf.high}]\",\n             error_pos = \"right\", digits = 3,\n             statistics = c(N = \"nobs\"))\n\n\n\n\nModel 1\n\n(Intercept)6.379 ***[6.354, 6.405]\n\ngenderFemale-0.462 ***[-0.533, -0.390]\n\nt0.047 ***[0.040, 0.053]\n\ngenderFemale:t0.032 ***[0.016, 0.049]\n\nN3101             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nNow the model is indicating that there is a strong interaction effect for gender by time. The coefficient of the interaction term implies that (logged) wages increase at a faster rate (over time) for females than they do for males.\nLet’s visual this alongside our modified dataset.\n\n\nCode\nnewdat &lt;- expand.grid(t = 1:7, gender = c(\"Male\", \"Female\"), id = 1) %&gt;% \n  mutate(gender = factor(gender),\n         gender = relevel(gender, \"Male\"))\n\np_mod3 &lt;- cbind(newdat, \n                lwage = predict(mod3, newdat, interval = \"prediction\"))\n\n\n\n\nCode\nmdat %&gt;% \n  ggplot(aes(t, mlwage, colour = gender)) +\n  geom_ribbon(data = p_mod3, aes(t, lwage.fit, ymin = lwage.lwr, ymax = lwage.upr, fill = gender), alpha = 0.5) +\n  geom_line(data = p_mod3, aes(t, lwage.fit), colour = \"black\", linewidth = 1) + \n  geom_point(data = mdat %&gt;% filter(is_zero_following == 1),\n             aes(y = lwage, group = id), alpha = 0.1, colour = \"red\") +\n  geom_line(data = mdat %&gt;% filter(is_zero_following == 1),\n            aes(y = lwage, group = id), alpha = 0.15, colour = \"red\") + \n  geom_point(alpha = 0.15) +\n  geom_line(aes(group = id), alpha = 0.15) + \n  facet_wrap(~ gender) +\n  scale_colour_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  theme(legend.position=\"none\",\n        plot.subtitle=element_text(colour = \"red\")) +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\",\n       subtitle = \"Missing data shown in red\")  +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9)) -&gt; p4\np4\n\n\n\n\n\n\n\n\n\nWe can see the predicted line and bands (95% prediction interval) represent the (non-missing) data well, and we can see the difference in slope between genders - the observed significant interaction term."
  },
  {
    "objectID": "posts/lmer-missing/index.html#missing---mixed-effects-model",
    "href": "posts/lmer-missing/index.html#missing---mixed-effects-model",
    "title": "Handling Missing Data with LMER",
    "section": "Missing - Mixed effects model",
    "text": "Missing - Mixed effects model\n\n\nCode\nmod4 &lt;- lmer(mlwage ~ gender * t + (1 | id), data = mdat)\nexport_summs(mod4, error_format = \"[{conf.low}, {conf.high}]\",\n             error_pos = \"right\", digits = 3,\n             statistics = c(N = \"nobs\"))\n\n\n\n\nModel 1\n\n(Intercept)6.341 ***[6.311, 6.372]\n\ngenderFemale-0.457 ***[-0.546, -0.368]\n\nt0.090 ***[0.087, 0.093]\n\ngenderFemale:t0.003    [-0.004, 0.011]\n\nN3101             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nWhen we run the mixed effects model on the dataset with missing data, we (correctly) do not see a significant interaction effect.\nIn fact:\n\n\nCode\nexport_summs(mod4, mod2,\n             error_format = \"[{conf.low}, {conf.high}]\",\n             error_pos = \"right\", digits = 3,\n             model.names = c(\"ME - Missing\", \"ME - Complete\"),\n             statistics = c(N = \"nobs\"))\n\n\n\n\nME - MissingME - Complete\n\n(Intercept)6.341 ***[6.311, 6.372]6.340 ***[6.307, 6.373]\n\ngenderFemale-0.457 ***[-0.546, -0.368]-0.456 ***[-0.553, -0.358]\n\nt0.090 ***[0.087, 0.093]0.097 ***[0.095, 0.100]\n\ngenderFemale:t0.003    [-0.004, 0.011]-0.005    [-0.012, 0.003]\n\nN3101             4165             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nOur mixed effects model on the dataset with a lot of missing data (ME - Missing) generates quite similar estimates to what we know the ‘truth’ to be from the model on the complete data (ME - Complete).\nTo comparatively visualise this.\n\n\nCode\nnewdat &lt;- expand.grid(t = 1:7, gender = c(\"Male\", \"Female\")) %&gt;% \n  mutate(gender = factor(gender),\n         gender = relevel(gender, \"Male\"),\n         id = c(rep(4, 7), rep(8, 7)))\n\nstore &lt;- simulate(mod4, seed=1, newdata=newdat, re.form=NA,\n                  allow.new.levels=T, nsim = 500)\n\np_mod4 &lt;- cbind(newdat,\n                store %&gt;% \n                  rowwise() %&gt;% \n                  mutate(fit = mean(c_across(sim_1:sim_500)),\n                         lwr = quantile(c_across(sim_1:sim_500), 0.025),\n                         upr = quantile(c_across(sim_1:sim_500), 0.975)) %&gt;% \n                  select(fit, lwr, upr))\n\n\n\n\nCode\nmdat %&gt;% \n  ggplot(aes(t, mlwage, colour = gender)) +\n  geom_ribbon(data = p_mod4, aes(t, fit, ymin = lwr, ymax = upr, fill = gender), alpha = 0.5) +\n  geom_line(data = p_mod4, aes(t, fit), colour = \"black\", linewidth = 1) + \n  geom_point(data = mdat %&gt;% filter(is_zero_following == 1),\n             aes(y = lwage, group = id), alpha = 0.1, colour = \"red\") +\n  geom_line(data = mdat %&gt;% filter(is_zero_following == 1),\n            aes(y = lwage, group = id), alpha = 0.15, colour = \"red\") + \n  geom_point(alpha = 0.15) +\n  geom_line(aes(group = id), alpha = 0.15) + \n  facet_wrap(~ gender) +\n  scale_colour_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  theme(legend.position=\"none\",\n        plot.subtitle=element_text(colour = \"red\")) +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\",\n       subtitle = \"Missing data shown in red\")  +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9)) -&gt; p4\np4\n\n\n\n\n\n\n\n\n\nThese lines (predicted lines; by gender) look parallel (as they should). Notably with the Males, we see the predicted line “pulled up” in the direction of the missing data even though that data was not available to the model - this is because the model has leveraged the slope of the data it did have access to, at the individual (person) level, when converging on its estimates.\nIf the erroneous interaction effect (non-parallel lines) was not obvious in the plot separated by gender, here we see the predicted lines on the same plot for each model.\n\n\nCode\np_mod3 %&gt;% \n  ggplot(aes(t, lwage.fit, ymin = lwage.lwr, ymax = lwage.upr, fill = gender)) +\n  geom_ribbon(alpha = 0.5) +\n  geom_line(colour = \"black\", linewidth = 1) + \n  scale_fill_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\",\n       title = \"Erroneous basic linear regression\",\n       subtitle = \"Missing data present\") +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9)) -&gt; p5\n\np_mod4 %&gt;% \n  ggplot(aes(t, fit, ymin = lwr, ymax = upr, fill = gender)) +\n  geom_ribbon(alpha = 0.5) +\n  geom_line(colour = \"black\", linewidth = 1) + \n  scale_fill_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\",\n       title = \"Mixed effects regression\",\n       subtitle = \"Missing data present\")  +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9)) -&gt; p6\n\np5 + p6 + plot_layout(guides = \"collect\") & theme(legend.position='bottom')"
  },
  {
    "objectID": "posts/lmer-missing/index.html#less-aggressive-missingness",
    "href": "posts/lmer-missing/index.html#less-aggressive-missingness",
    "title": "Handling Missing Data with LMER",
    "section": "Less aggressive missingness",
    "text": "Less aggressive missingness\nWhat if we whip through the same process and comparison, in a setting that ‘less aggressively’ has drop out with increasing wage and also some additional random missingness throughout.\n\n\nCode\nmdat &lt;- dat %&gt;% \n  group_by(id) %&gt;% \n  mutate(plwage = c(0, lwage[1:6]),\n         pt = 1 / (1+exp(-7.2 + plwage))) %&gt;% # Less aggressive dropout as a function of age\n  rowwise() %&gt;% \n  mutate(pt = case_when(pt &gt; 0.5 & runif(1) &lt; 0.10 & t &gt; 2 ~ 0, # Adding an underlying random component to dropout\n                        T ~ pt)) %&gt;% \n  group_by(id) %&gt;% \n  mutate(mlwage = case_when(pt &gt; 0.5 ~ lwage,\n                            pt &lt; 0.5 ~ 0),\n         is_zero_following = ifelse(mlwage == 0, 1, 0),\n         is_zero_following = cummax(is_zero_following),\n         mlwage = ifelse(is_zero_following == 1, NA, mlwage)) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nMale, N = 2,8041\nFemale, N = 3521\n\n\n\n\nt\n\n\n\n\n\n\n    1\n528\n67\n\n\n    2\n528\n67\n\n\n    3\n478\n57\n\n\n    4\n396\n49\n\n\n    5\n338\n43\n\n\n    6\n290\n38\n\n\n    7\n246\n31\n\n\n\n1 n\n\n\n\n\n\n\n\n\n\n\n\nCode\nmdat %&gt;% \n  ggplot(aes(t, mlwage, colour = gender)) +\n  geom_point(data = mdat %&gt;% filter(is_zero_following == 1),\n             aes(y = lwage, group = id), alpha = 0.1, colour = \"red\") +\n  geom_line(data = mdat %&gt;% filter(is_zero_following == 1),\n            aes(y = lwage, group = id), alpha = 0.15, colour = \"red\") + \n  geom_point(alpha = 0.15) +\n  geom_line(aes(group = id), alpha = 0.15) + \n  facet_wrap(~ gender) +\n  scale_colour_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  theme(legend.position=\"none\") +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\")  +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9))\n\n\n\n\n\n\n\n\n\n\n\nCode\nb1_mod1 &lt;- lm(mlwage ~ gender * t, data = mdat)\nb1_mod2 &lt;- lmer(mlwage ~ gender * t + (1 | id), data = mdat)\n\nexport_summs(mod2, b1_mod1, b1_mod2,\n             error_format = \"[{conf.low}, {conf.high}]\",\n             error_pos = \"right\", digits = 3,\n             model.names = c(\"ME - Complete\", \"Basic - Missing\", \"ME - Missing\"),\n             statistics = c(N = \"nobs\"))\n\n\n\n\nME - CompleteBasic - MissingME - Missing\n\n(Intercept)6.340 ***[6.307, 6.373]6.380 ***[6.352, 6.407]6.342 ***[6.311, 6.373]\n\ngenderFemale-0.456 ***[-0.553, -0.358]-0.491 ***[-0.573, -0.410]-0.465 ***[-0.557, -0.372]\n\nt0.097 ***[0.095, 0.100]0.072 ***[0.065, 0.079]0.094 ***[0.091, 0.097]\n\ngenderFemale:t-0.005    [-0.012, 0.003]0.024 *  [0.003, 0.044]0.003    [-0.006, 0.012]\n\nN4165             3156             3156             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nWe can see, the (erroneous) basic linear regression still inappropriately suggests a significant interaction effect is present, while the mixed effects models continues to perform well (relative to the model using the complete data)."
  },
  {
    "objectID": "posts/lmer-missing/index.html#completely-random-missingness",
    "href": "posts/lmer-missing/index.html#completely-random-missingness",
    "title": "Handling Missing Data with LMER",
    "section": "Completely random missingness",
    "text": "Completely random missingness\nWhat if the dropout is completely at random?\nNote, this is dropout at random, not sporadic missingness, these are two different things.\n\n\nCode\nmdat &lt;- dat %&gt;% \n  group_by(id) %&gt;% \n  rowwise() %&gt;% \n  mutate(pt = 1,\n         pt = case_when(runif(1) &lt; 0.10 & t &gt; 2 ~ 0, # Implementing completely random dropout\n                        T ~ pt)) %&gt;% \n  group_by(id) %&gt;% \n  mutate(mlwage = case_when(pt &gt; 0.5 ~ lwage,\n                            pt &lt; 0.5 ~ 0),\n         is_zero_following = ifelse(mlwage == 0, 1, 0),\n         is_zero_following = cummax(is_zero_following),\n         mlwage = ifelse(is_zero_following == 1, NA, mlwage)) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nME - CompleteBasic - MissingME - Missing\n\n(Intercept)6.340 ***[6.307, 6.373]6.330 ***[6.300, 6.359]6.333 ***[6.301, 6.366]\n\ngenderFemale-0.456 ***[-0.553, -0.358]-0.456 ***[-0.544, -0.368]-0.456 ***[-0.553, -0.359]\n\nt0.097 ***[0.095, 0.100]0.102 ***[0.095, 0.109]0.099 ***[0.096, 0.102]\n\ngenderFemale:t-0.005    [-0.012, 0.003]-0.007    [-0.029, 0.014]-0.003    [-0.012, 0.005]\n\nN4165             3379             3379             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nThe basic regression model is no long suggesting there is a significant gender by time interaction effect, and comparatively, all three models give similar estimates."
  },
  {
    "objectID": "posts/lmer-missing/index.html#acknowledgements",
    "href": "posts/lmer-missing/index.html#acknowledgements",
    "title": "Handling Missing Data with LMER",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Elizabeth McKinnon, Zac Dempsey, and Wesley Billingham for providing feedback on and reviewing this post.\nYou can look forward to seeing posts from these other team members here in the coming weeks and months."
  },
  {
    "objectID": "posts/lmer-missing/index.html#reproducibility-information",
    "href": "posts/lmer-missing/index.html#reproducibility-information",
    "title": "Handling Missing Data with LMER",
    "section": "Reproducibility Information",
    "text": "Reproducibility Information\nTo access the .qmd (Quarto markdown) files as well as any R scripts or data that was used in this post, please visit our GitHub:\nhttps://github.com/The-Kids-Biostats/The-Kids-Biostats.github.io/tree/main/posts/lmer-missingx\nThe session information can also be seen below.\n\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Australia/Perth\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n [5] purrr_1.0.2       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n [9] ggplot2_3.5.1     tidyverse_2.0.0   patchwork_1.2.0   parameters_0.21.7\n[13] jtools_2.2.2      gtsummary_1.7.2   AER_1.2-12        survival_3.6-4   \n[17] sandwich_3.1-0    lmtest_0.9-40     zoo_1.8-12        car_3.1-2        \n[21] carData_3.0-5     merTools_0.6.2    arm_1.14-4        MASS_7.3-60.0.1  \n[25] lme4_1.1-35.3     Matrix_1.6-5     \n\nloaded via a namespace (and not attached):\n [1] rlang_1.1.3          magrittr_2.0.3       multcomp_1.4-25     \n [4] furrr_0.3.1          compiler_4.3.3       vctrs_0.6.5         \n [7] pkgconfig_2.0.3      crayon_1.5.2         fastmap_1.2.0       \n[10] backports_1.5.0      labeling_0.4.3       pander_0.6.5        \n[13] utf8_1.2.4           promises_1.3.0       rmarkdown_2.27      \n[16] markdown_1.12        tzdb_0.4.0           nloptr_2.0.3        \n[19] xfun_0.44            jsonlite_1.8.8       later_1.3.2         \n[22] broom_1.0.6          parallel_4.3.3       R6_2.5.1            \n[25] stringi_1.8.4        parallelly_1.37.1    boot_1.3-30         \n[28] numDeriv_2016.8-1.1  estimability_1.5.1   assertthat_0.2.1    \n[31] Rcpp_1.0.12          iterators_1.0.14     knitr_1.45          \n[34] httpuv_1.6.15        splines_4.3.3        timechange_0.3.0    \n[37] tidyselect_1.2.1     rstudioapi_0.16.0    abind_1.4-5         \n[40] yaml_2.3.8           codetools_0.2-20     listenv_0.9.1       \n[43] lmerTest_3.1-3       lattice_0.22-6       shiny_1.8.1.1       \n[46] withr_3.0.0          bayestestR_0.13.2    coda_0.19-4.1       \n[49] evaluate_0.23        future_1.33.2        huxtable_5.5.6      \n[52] xml2_1.3.6           pillar_1.9.0         foreach_1.5.2       \n[55] insight_0.19.11      generics_0.1.3       hms_1.1.3           \n[58] munsell_0.5.1        commonmark_1.9.1     scales_1.3.0        \n[61] minqa_1.2.7          globals_0.16.3       xtable_1.8-4        \n[64] glue_1.7.0           emmeans_1.10.2       tools_4.3.3         \n[67] mvtnorm_1.2-5        grid_4.3.3           datawizard_0.10.0   \n[70] colorspace_2.1-0     nlme_3.1-164         Formula_1.2-5       \n[73] cli_3.6.2            fansi_1.0.6          viridisLite_0.4.2   \n[76] broom.helpers_1.15.0 gt_0.10.1            gtable_0.3.5        \n[79] broom.mixed_0.2.9.5  sass_0.4.9           digest_0.6.35       \n[82] TH.data_1.1-2        farver_2.1.2         htmlwidgets_1.6.4   \n[85] htmltools_0.5.8.1    lifecycle_1.0.4      mime_0.12           \n[88] blme_1.0-5"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Kids Biostats",
    "section": "",
    "text": "Parallel Computing in R\n\n\n\n\n\n\nR\n\n\nParallel\n\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\nZac Dempsey\n\n\n\n\n\n\n\n\n\n\n\n\nHandling Missing Data with LMER\n\n\n\n\n\n\nMissing Data\n\n\nMixed Models\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 7, 2024\n\n\nDr Matthew Cooper\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "We are members of the biostatistics team at Telethon Kids Institute in Perth, Western Australia. Our work involves providing statistical consultation and collaboration to child health researchers at all stages of the scientific pipeline, from design to analysis to publication.\n\nPrimarily R users, we encounter particular coding, methodology, or analysis challenges - often that have recurring or related themes. The idea of this site is to be a centralised repository of our solutions to or narration of these recurrent challenges, for both our reference and yours!\nWe hope you’ll find something useful here, and are always open to feedback and conversation on these topics."
  },
  {
    "objectID": "posts/parallel/index.html#for-loop",
    "href": "posts/parallel/index.html#for-loop",
    "title": "Parallel Computing in R",
    "section": "For-loop",
    "text": "For-loop\n\nLet’s begin by using a traditional for-loop. For each bootstrap sample, we:\n\nInitialise a bootstrap sample, ind.\nRun our linear regression on the bootstrap sample, results\nExtract the coefficients from results, and append this to an overall coefficient matrix bootstrap_coefs.\n\nSubsequently, we calculate a 95% confidence interval for each of our parameter estimates by taking the 2.5th and 97.5th percentiles from the bootstrap distribution and calling this bootstrap_cis.\n\n\n\nCode\nstart &lt;- proc.time() # Start our timer!\n\n# Initialise a matrix to store the coefficients from each bootstrap sample\nbootstrap_coefs &lt;- matrix(NA, nrow = trials, ncol = 4)\ncolnames(bootstrap_coefs) &lt;- names(coef(lm(mpg ~ hp + wt + am, data = mtcars)))\n\nfor (i in 1:trials){\n  \n  # Take bootstrap sample\n  ind &lt;- mtcars[sample(nrow(mtcars), \n                       replace = TRUE),\n                ]\n  \n  # Construct linear regression\n  result &lt;- lm(mpg ~ hp + wt + as_factor(am), \n               data = ind)\n  \n  # Extract coefficients and store to `bootstrap_coefs`\n  bootstrap_coefs[i, ] &lt;- coef(result)\n}\n\n# Calculate the 2.5th and 97.5th percentile for each variable's coefficient estimates from the bootstrap distribution.\nbootstrap_cis &lt;- apply(bootstrap_coefs, \n                       2, \n                       function(coefs){quantile(coefs, probs = c(0.025, 0.975))})\n\nend &lt;- proc.time() # End our timer!\ntime1 &lt;- end-start\n\n\n\nLet’s visualise the first few lines of the bootstrapped results\n\n\nCode\nhead(bootstrap_coefs)\n\n\n     (Intercept)          hp        wt         am\n[1,]    34.04596 -0.02741585 -3.158154  0.6330600\n[2,]    31.46670 -0.03097903 -2.375727  5.8142235\n[3,]    35.98084 -0.02535775 -3.763464 -0.2485866\n[4,]    33.47330 -0.04410244 -2.343210  2.6890689\n[5,]    32.21798 -0.04138935 -2.222471  1.2289610\n[6,]    32.78747 -0.02758182 -2.989311  1.1744731\n\n\nand associated 95% confidence interval\n\n\nCode\nbootstrap_cis\n\n\n      (Intercept)          hp        wt         am\n2.5%     29.07843 -0.05539954 -5.134682 -0.7627477\n97.5%    40.74463 -0.02161861 -1.057830  4.9428501\n\n\n\nThis had the following run-time (seconds):\n\n\nCode\nprint(time1)\n\n\n   user  system elapsed \n   8.13    0.11    8.25 \n\n\n\n\n\n\n\n\nproc.time components\n\n\n\n\nuser = time the CPU has spent executing the R process.\nsystem = time the CPU has spent on system-level operations that facilitate the R process (e.g., memory management, system calls).\nelapsed = real-world time that has elapsed."
  },
  {
    "objectID": "posts/parallel/index.html#do-loop-not-parallel",
    "href": "posts/parallel/index.html#do-loop-not-parallel",
    "title": "Parallel Computing in R",
    "section": "%do% loop (not parallel)",
    "text": "%do% loop (not parallel)\nAs an alternative, let’s also use the %do% operator from the foreach package. Similar to a for-loop, each bootstrap sample is executed sequentially.\n\n\nCode\nstart &lt;- proc.time()\n\nbootstrap_coefs &lt;- foreach::foreach(i = 1:trials, .combine = rbind) %do% {\n  ind &lt;- mtcars[sample(nrow(mtcars), replace = TRUE), ]\n  result &lt;- lm(mpg ~ hp + wt + am, data = ind)\n  coef(result)\n}\n\n# Calculate the 2.5th and 97.5th percentile for each variable's coefficient estimates from the bootstrap distribution.\nbootstrap_cis &lt;- apply(bootstrap_coefs, \n                       2, \n                       function(coefs) {quantile(coefs, probs = c(0.025, 0.975))})\n\nend &lt;- proc.time()\ntime2 &lt;- end-start\n\n\n\nSimilarly, let’s visualise the first few lines of the bootstrapped results\n\n\nCode\nhead(bootstrap_coefs)\n\n\n         (Intercept)          hp        wt       am\nresult.1    32.28476 -0.02807558 -2.989010 2.404865\nresult.2    38.19523 -0.02740136 -4.576854 1.082726\nresult.3    32.87519 -0.06693216 -1.172027 3.879952\nresult.4    29.54068 -0.03806135 -1.468158 1.933494\nresult.5    31.54392 -0.02793433 -2.756060 1.347816\nresult.6    34.42623 -0.03900395 -2.865725 1.167583\n\n\nand associated 95% confidence interval\n\n\nCode\nbootstrap_cis\n\n\n      (Intercept)          hp        wt         am\n2.5%     29.03455 -0.05615444 -5.286909 -0.8228096\n97.5%    41.09610 -0.02124169 -1.055386  4.9293286\n\n\n\nThis had the following run-time (seconds):\n\n\nCode\nprint(time2)\n\n\n   user  system elapsed \n   7.33    0.09    7.46"
  },
  {
    "objectID": "posts/parallel/index.html#dopar-parallelisation",
    "href": "posts/parallel/index.html#dopar-parallelisation",
    "title": "Parallel Computing in R",
    "section": "%dopar% parallelisation",
    "text": "%dopar% parallelisation\nNow, let’s run this in parallel across 6 cores. The %dopar% operator defines the for-loop in the parallel environment.\n\n\nCode\ndoParallel::registerDoParallel(cores = 6) # Initialise parallel cluster\n\nstart &lt;- proc.time()\nbootstrap_coefs &lt;- foreach(i = 1:trials, .combine = rbind, .packages = 'stats') %dopar% {\n  ind &lt;- mtcars[sample(nrow(mtcars), replace = TRUE), ]\n  result &lt;- lm(mpg ~ hp + wt + am, data = ind)\n  coef(result)\n}\n\n# Calculate the 2.5th and 97.5th percentile for each variable's coefficient estimates from the bootstrap distribution.\nbootstrap_cis &lt;- apply(bootstrap_coefs, \n                       2, \n                       function(coefs) {quantile(coefs, probs = c(0.025, 0.975))})\n\nend &lt;- proc.time()\ntime3 &lt;- end-start\n\ndoParallel::stopImplicitCluster() # De-register parallel cluster\n\n\nAs expected, the output of the bootstrapped coefficient distribution are identical before\n\n\nCode\nhead(bootstrap_coefs)\n\n\n         (Intercept)          hp         wt        am\nresult.1    29.01023 -0.03144924 -1.7011668 1.9710101\nresult.2    34.64419 -0.02981554 -3.3686345 2.2630996\nresult.3    29.30763 -0.05186821 -0.7765918 5.3702237\nresult.4    32.78371 -0.05571083 -1.8368810 4.7410181\nresult.5    34.58447 -0.02907062 -3.3133915 0.1025598\nresult.6    33.63623 -0.03984186 -2.6517603 2.7519106\n\n\nas are the associated 95% confidence intervals.\n\n\nCode\nbootstrap_cis\n\n\n      (Intercept)          hp        wt         am\n2.5%     29.00517 -0.05536575 -5.181523 -0.7919252\n97.5%    40.89882 -0.02190769 -1.076947  4.9375445\n\n\nLastly, this had the following run-time (seconds)\n\n\nCode\nprint(time3)\n\n\n   user  system elapsed \n   2.81    0.45    4.63"
  },
  {
    "objectID": "posts/parallel/index.html#discussion",
    "href": "posts/parallel/index.html#discussion",
    "title": "Parallel Computing in R",
    "section": "Discussion",
    "text": "Discussion\nImmediately, the syntax of the alternative for-loop structures are more readable and easier to construct than the traditional for-loop. Because the foreach::foreach function easily combines output in a list, we need not define an empty matrix to append output to.\nComputation time in the parallel environment is significantly faster — approximately 44% faster than the traditional for-loop! Across multiple analyses and data sets, these time savings certainly add up!"
  },
  {
    "objectID": "posts/parallel/index.html#data",
    "href": "posts/parallel/index.html#data",
    "title": "Parallel Computing in R",
    "section": "Data",
    "text": "Data\nFirst, let’s simulate our data set and set some parameters:\n\n10,000 observations.\nIndependent variables temperature, precipitation and elevation sampled from a random normal distribution and vegetation type (categorical factor) randomly prescribed.\nSpecies presence (dichotomous) outcome variable is randomly prescribed.\n\n\n\nCode\nn &lt;- 10000 # Sample size\n\ndata &lt;- data.frame(temperature = rnorm(n, \n                                       mean = 15, \n                                       sd   = 40),\n                   precipitation = rnorm(n, \n                                         mean = 1000, \n                                         sd   = 300),\n                   elevation = rnorm(n, \n                                     mean = 500, \n                                     sd   = 200),\n                   vegetation_type = as_factor(sample(c(\"forest\", \n                                                        \"grassland\", \n                                                        \"wetland\", \n                                                        \"desert\"), \n                                                      n, \n                                                      replace = T)),\n                   species_presence = as_factor(sample(c(\"present\", \n                                                         \"absent\"), \n                                                       n, \n                                                       replace = T)))\n\n\nLet’s assign 70% of the data to our training set, and the remaining 30% to test data set and initialise a random forest model with 1000 trees.\n\n\nCode\ntrain_index &lt;- sample(1:n, 0.7*n)\n\ntrain_data &lt;- data[train_index, ]\ntest_data &lt;- data[-train_index, ]\n\nnum_trees &lt;- 1000\n\n\n\nInstead of running one random forest model comprising 1000 trees, let’s combine the results of 4 smaller random forest models models each comprising 250 trees. By doing this, we can return more reliable and robust output (smaller random forest models are less prone to overfitting) and better manage working memory (smaller models require less memory to train and store)."
  },
  {
    "objectID": "posts/parallel/index.html#for-loop-1",
    "href": "posts/parallel/index.html#for-loop-1",
    "title": "Parallel Computing in R",
    "section": "For-loop",
    "text": "For-loop\n\n\nCode\nstart &lt;- proc.time()\nrf &lt;- list()\nfor (i in 1:(num_trees/250)){\n\n  rf[[i]] &lt;- randomForest::randomForest(species_presence ~ ., \n                                        data = train_data, \n                                        ntree = num_trees/4)\n}\n\ncombined_output &lt;- do.call(randomForest::combine, rf)\n\npredictions &lt;- predict(combined_output, test_data %&gt;% select(-species_presence))\n\nend &lt;- proc.time()\ntime1 &lt;- end - start\n\n\nLet’s print the confusion matrix of this output. Because this data was relatively simply simulated, we don’t expect the predictive power to be too great.\n\n\nCode\ntable(predictions, test_data$species_presence)\n\n\n           \npredictions absent present\n    absent     705     777\n    present    781     737\n\n\nThis has the following run-time (seconds):\n\n\nCode\nprint(time1)\n\n\n   user  system elapsed \n   5.47    0.26    5.77"
  },
  {
    "objectID": "posts/parallel/index.html#do-loop-not-parallel-1",
    "href": "posts/parallel/index.html#do-loop-not-parallel-1",
    "title": "Parallel Computing in R",
    "section": "%do% loop (not parallel)",
    "text": "%do% loop (not parallel)\nSimilar to the traditional for-loop, we can sequentially execute this code using the %do% operator.\n\n\nCode\nstart &lt;- proc.time()\nrf &lt;- foreach::foreach(ntree = rep(num_trees/4, 4), \n                       .combine = randomForest::combine, \n                       .packages = 'randomForest') %do%\n  randomForest::randomForest(species_presence ~ ., \n                             data = train_data,\n                             ntree = ntree)\n\npredictions &lt;- predict(rf, test_data %&gt;% select(-species_presence))\n\nend &lt;- proc.time()\ntime2 &lt;- end - start\n\n\nLet’s print the confusion matrix of this output. Because this data was relatively simply simulated, we don’t expect the predictive power to be too great.\n\n\nCode\ntable(predictions, test_data$species_presence)\n\n\n           \npredictions absent present\n    absent     706     778\n    present    780     736\n\n\nThis has the following run-time (seconds):\n\n\nCode\nprint(time2)\n\n\n   user  system elapsed \n   5.31    0.48    5.81"
  },
  {
    "objectID": "posts/parallel/index.html#dopar-parallelisation-1",
    "href": "posts/parallel/index.html#dopar-parallelisation-1",
    "title": "Parallel Computing in R",
    "section": "%dopar% parallelisation",
    "text": "%dopar% parallelisation\nFor simplicity, let’s allocate 4 cores to the computation and imagine that one core is responsible for processing one of the four random forest models simultaneously.\n\n\nCode\ndoParallel::registerDoParallel(cores = 4)\n\nstart &lt;- proc.time()\nrf &lt;- foreach::foreach(ntree = rep(num_trees/4, 4), \n                       .combine = randomForest::combine, \n                       .packages = 'randomForest') %dopar%\n  randomForest::randomForest(species_presence ~ ., \n                             data = train_data,\n                             ntree = ntree)\n\npredictions &lt;- predict(rf, test_data %&gt;% select(-species_presence))\n\nend &lt;- proc.time()\ndoParallel::stopImplicitCluster()\n\ntime3 &lt;- end-start\n\n\nLet’s print the confusion matrix of this output. Because this data was relatively simply simulated, we don’t expect the predictive power to be too great.\n\n\nCode\ntable(predictions, test_data$species_presence)\n\n\n           \npredictions absent present\n    absent     704     767\n    present    782     747\n\n\nThis now has the following run-time (seconds):\n\n\nCode\nprint(time3)\n\n\n   user  system elapsed \n   1.08    0.55    3.81"
  },
  {
    "objectID": "posts/parallel/index.html#discussion-1",
    "href": "posts/parallel/index.html#discussion-1",
    "title": "Parallel Computing in R",
    "section": "Discussion",
    "text": "Discussion\nAgain, using easily adaptable and readable syntax, we leverage a parallel environment to significantly lessen the computation time of our large model. Relative to a standard for-loop, the parallelised computation is approximately 34% faster."
  },
  {
    "objectID": "posts/parallel/index.html#data-1",
    "href": "posts/parallel/index.html#data-1",
    "title": "Parallel Computing in R",
    "section": "Data",
    "text": "Data\n\nLet’s use nycflights13::flights — a set of over 300,000 flight records that departed from all NYC airports in 2013.\nWe would like to explore how arrival delay (as a continuous and dichotomous (delayed = 1, not delayed = 0) outcome variable) may be influenced by a set of independent variables. We would like to stratify this by month.\n\n\n\nCode\nflights &lt;- nycflights13::flights\nflights &lt;- flights %&gt;%\n  select(year, day, dep_delay, arr_delay, air_time, distance) %&gt;%\n  mutate(arr_delay_bin = as.factor(case_when(arr_delay &gt;  15 ~ 1, TRUE ~ 0)))\n\nflights\n\n\n# A tibble: 336,776 × 7\n    year   day dep_delay arr_delay air_time distance arr_delay_bin\n   &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;        \n 1  2013     1         2        11      227     1400 0            \n 2  2013     1         4        20      227     1416 1            \n 3  2013     1         2        33      160     1089 1            \n 4  2013     1        -1       -18      183     1576 0            \n 5  2013     1        -6       -25      116      762 0            \n 6  2013     1        -4        12      150      719 0            \n 7  2013     1        -5        19      158     1065 1            \n 8  2013     1        -3       -14       53      229 0            \n 9  2013     1        -3        -8      140      944 0            \n10  2013     1        -2         8      138      733 0            \n# ℹ 336,766 more rows\n\n\n\nWe would like to specify a set of models which predict overall flight delay, as both continuous (arrival delay time) and dichotomous (delayed yes/no) outcomes.\n\nOutcome variables\n\nArrival delay (continuous)\nArrival delayed (dichotomous)\n\nIndependent variables\n\nFlight distance (distance)\nAir time (air_time)\nDeparture delay (dep_delay)\n\n\n\n\nCode\nindep_vars &lt;- c(\"distance\", \"air_time\", \"dep_delay\")\noutcome_vars &lt;- c(\"arr_delay\", \"arr_delay_bin\")\n\n\nFor each outcome variable, we run a model. If the outcome variable is continuous, we run a simple linear model; otherwise we run a basic logistic regression."
  },
  {
    "objectID": "posts/parallel/index.html#for-loop-2",
    "href": "posts/parallel/index.html#for-loop-2",
    "title": "Parallel Computing in R",
    "section": "For-loop",
    "text": "For-loop\n\n\nCode\nstart &lt;- proc.time()\nmodels &lt;- list() # To store our model output\n\nfor (i in outcome_vars){\n  if (i == \"arr_delay\"){\n    model &lt;- lm(as.formula(paste(i,\"~\",paste(indep_vars, collapse = \"+\"))),\n                data = flights)\n    \n  } else if (i == \"arr_delay_bin\"){\n    model &lt;- glm(as.formula(paste(i,\"~\",paste(indep_vars, collapse = \"+\"))),\n                 family = binomial,\n                 data = flights)\n  }\n  models[[i]] &lt;- summary(model)\n}\n\nend &lt;- proc.time()\ntime1 &lt;- end-start\n\n\nThis returns a list with the model output summary for each of our models.\nThe for-loop has the following run-time (seconds):\n\n\nCode\nprint(time1)\n\n\n   user  system elapsed \n   1.00    0.23    1.25"
  },
  {
    "objectID": "posts/parallel/index.html#purrrmap",
    "href": "posts/parallel/index.html#purrrmap",
    "title": "Parallel Computing in R",
    "section": "purrr::map",
    "text": "purrr::map\nMap functions apply a function to each element of a list/vector and return an object. In cases relying on multiple computations across different values, they often come in handy.\n\n\nCode\nstart &lt;- proc.time()\n\n\nmodels &lt;- map(outcome_vars, \n      ~ if (.x == \"arr_delay\"){\n        lm(as.formula(paste(.x, \"~\", \n                            paste(indep_vars, collapse = \" + \"))),\n           data = flights) %&gt;%\n          summary()\n        \n        } else if (.x == \"arr_delay_bin\"){\n          glm(as.formula(paste(.x, \"~\",\n                               paste(indep_vars, collapse = \" + \"))),\n              family = binomial,\n              data = flights) %&gt;%\n            summary()\n        }\n      )\nnames(models) &lt;- outcome_vars\n\nend &lt;- proc.time()\ntime2 &lt;- end-start\n\n\nThis has the following run-time (seconds):\n\n\nCode\nprint(time2)\n\n\n   user  system elapsed \n   1.11    0.15    1.28"
  },
  {
    "objectID": "posts/parallel/index.html#furrrfuture_map",
    "href": "posts/parallel/index.html#furrrfuture_map",
    "title": "Parallel Computing in R",
    "section": "furrr::future_map",
    "text": "furrr::future_map\nThere is also a parallel implementation of the purrr::map function, offered by the furrr package. The syntax is (nicely) identical to above, but importantly relies on specifying a parallel (multisession) “plan” ahead of executing the code (similar to what we did in Example 1 and 2).\n\n\nCode\nlibrary(furrr)\n\nplan(multisession, workers = 6) # Initialise parallel environment using furrr\n\nstart &lt;- proc.time()\nmodels &lt;- furrr::future_map(outcome_vars, \n      ~ if (.x == \"arr_delay\"){\n        lm(as.formula(paste(.x, \"~\", \n                            paste(indep_vars, collapse = \" + \"))),\n           data = flights) %&gt;%\n          summary()\n        \n        } else if (.x == \"arr_delay_bin\"){\n          glm(as.formula(paste(.x, \"~\",\n                               paste(indep_vars, collapse = \" + \"))),\n              family = binomial,\n              data = flights) %&gt;%\n            summary()\n        }\n      )\nnames(models) &lt;- outcome_vars\n\nend &lt;- proc.time()\ntime3 &lt;- end-start\n\nplan(sequential) # Revert to sequential processing\n\n\nThis has the following run-time (seconds):\n\n\nCode\nprint(time3)\n\n\n   user  system elapsed \n   0.14    0.01    2.39"
  },
  {
    "objectID": "posts/parallel/index.html#dopar-parallelisation-2",
    "href": "posts/parallel/index.html#dopar-parallelisation-2",
    "title": "Parallel Computing in R",
    "section": "%dopar% parallelisation",
    "text": "%dopar% parallelisation\nAlternatively, as done earlier, we can turn our non-parallel %do% code into parallel %dopar% code.\n\nWe use the %:% operator from the foreach package to nest a for-loop within a parallel environment.\nThe syntax does not differ too dramatically.\n\n\n\nCode\ndoParallel::registerDoParallel(cores = 6)\n\nstart &lt;- proc.time()\nmodels &lt;- foreach(j = outcome_vars, .combine = \"list\") %dopar% {\n  if (j == \"arr_delay\"){\n    model &lt;- lm(as.formula(paste(j,\"~\",paste(indep_vars, collapse = \"+\"))),\n                data = flights)\n  } else if (j == \"arr_delay_bin\"){\n    model &lt;- glm(as.formula(paste(j,\"~\",paste(indep_vars, collapse = \"+\"))),\n                 family = binomial,\n                 data = flights)\n  }\n}\nnames(models) &lt;- outcome_vars\n\nend &lt;- proc.time()\n\ntime4 &lt;- end - start\ndoParallel::stopImplicitCluster()\n\n\nThis has the following run-time (seconds):\n\n\nCode\nprint(time4)\n\n\n   user  system elapsed \n   1.23    0.68    4.80"
  },
  {
    "objectID": "posts/parallel/index.html#discussion-2",
    "href": "posts/parallel/index.html#discussion-2",
    "title": "Parallel Computing in R",
    "section": "Discussion",
    "text": "Discussion\nWe now see that the parallel processing of these tasks takes far longer – about 74% so! The underlying set of operations — running a series of linear models — are already small and relatively fast, so the overhead of managing the task (splitting, computing and combining results) in a parallel environment far exceeds what what can easily be spun up using a for-loop (or purrr::map)."
  },
  {
    "objectID": "posts/parallel/index.html#acknowledgements",
    "href": "posts/parallel/index.html#acknowledgements",
    "title": "Parallel Computing in R",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Wesley Billingham, Matt Cooper, and Elizabeth McKinnon for providing feedback on and reviewing this post.\nYou can look forward to seeing posts from these other team members here in the coming weeks and months."
  },
  {
    "objectID": "posts/parallel/index.html#reproducibility-information",
    "href": "posts/parallel/index.html#reproducibility-information",
    "title": "Parallel Computing in R",
    "section": "Reproducibility Information",
    "text": "Reproducibility Information\nTo access the .qmd (Quarto markdown) files as well as any R scripts or data that was used in this post, please visit our GitHub:\nhttps://github.com/The-Kids-Biostats/The-Kids-Biostats.github.io/tree/main/posts/parallel\nThe session information can also be seen below.\n\n\nCode\nsessionInfo()\n\n\nR version 4.3.2 (2023-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_Australia.utf8  LC_CTYPE=English_Australia.utf8   \n[3] LC_MONETARY=English_Australia.utf8 LC_NUMERIC=C                      \n[5] LC_TIME=English_Australia.utf8    \n\ntime zone: Australia/Perth\ntzcode source: internal\n\nattached base packages:\n[1] parallel  stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] furrr_0.3.1          future_1.33.2        randomForest_4.7-1.1\n [4] doParallel_1.0.17    iterators_1.0.14     foreach_1.5.2       \n [7] lubridate_1.9.3      forcats_1.0.0        stringr_1.5.1       \n[10] dplyr_1.1.4          purrr_1.0.2          readr_2.1.5         \n[13] tidyr_1.3.1          tibble_3.2.1         ggplot2_3.5.1       \n[16] tidyverse_2.0.0     \n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.4         generics_0.1.3     stringi_1.8.3      listenv_0.9.1     \n [5] hms_1.1.3          digest_0.6.35      magrittr_2.0.3     evaluate_0.23     \n [9] grid_4.3.2         timechange_0.3.0   fastmap_1.1.1      jsonlite_1.8.8    \n[13] fansi_1.0.6        scales_1.3.0       codetools_0.2-20   cli_3.6.2         \n[17] rlang_1.1.3        parallelly_1.37.1  munsell_0.5.1      withr_3.0.0       \n[21] yaml_2.3.8         tools_4.3.2        tzdb_0.4.0         colorspace_2.1-0  \n[25] globals_0.16.3     vctrs_0.6.5        R6_2.5.1           lifecycle_1.0.4   \n[29] htmlwidgets_1.6.4  pkgconfig_2.0.3    pillar_1.9.0       gtable_0.3.5      \n[33] glue_1.7.0         xfun_0.43          tidyselect_1.2.1   rstudioapi_0.16.0 \n[37] knitr_1.46         htmltools_0.5.8.1  rmarkdown_2.26     nycflights13_1.0.2\n[41] compiler_4.3.2"
  },
  {
    "objectID": "posts/table_one/archive/Summary Tables.html",
    "href": "posts/table_one/archive/Summary Tables.html",
    "title": "Summary Tables",
    "section": "",
    "text": "Analysing data will normally always involve generating tables, whether they are the final goal of the analysis or used as an intermediary step to guide further analysis. Here, we’re going to focus on the former while keeping the latter in the back of our minds. A common use case here might be a “Table 1” from a randomised control trial where we are examining (not testing and generating p-values) a range of summary statistics for each arm of the trial, or perhaps data are collected on a group over three time points and we want to examine a range of summary statistics for each time point.\nThere are many R packages and functions dedicated to this task. In the post below, we specify a range of criteria that we deem important for our most common use cases (which include continuous data, dichotomous variables, and both ordered and unordered categorical data). We then use these criteria to evaluate a number of the more commonly used functions, showing examples of the code and output along the way."
  },
  {
    "objectID": "posts/table_one/archive/Summary Tables.html#the-dataset",
    "href": "posts/table_one/archive/Summary Tables.html#the-dataset",
    "title": "Summary Tables",
    "section": "The dataset",
    "text": "The dataset\nTo evaluate the summary table functions, we select a data set and make a few minor changes to ensure it has the right variety of variables types to test all of our required scenarios.\nWe will use the flchain (Assay of serum free light chain for 7874 subjects) dataset from the survival package.\n\nhead(flchain)\n\n\n\nagesexsample.yrkappalambdaflc.grpcreatininemgusfutimedeathchapter\n\n97F2e+035.7 4.86 101.70851Circulatory\n\n92F2e+030.870.68310.9012811Neoplasms\n\n94F2e+034.363.85 101.40691Circulatory\n\n92F2e+032.422.22 91  01151Circulatory\n\n93F2e+031.321.69 61.1010391Circulatory\n\n90F2e+032.011.86 91  013551Mental\n\n\n\nstr(flchain)\n\n'data.frame':   7874 obs. of  11 variables:\n $ age       : num  97 92 94 92 93 90 90 90 93 91 ...\n $ sex       : Factor w/ 2 levels \"F\",\"M\": 1 1 1 1 1 1 1 1 1 1 ...\n $ sample.yr : num  1997 2000 1997 1996 1996 ...\n $ kappa     : num  5.7 0.87 4.36 2.42 1.32 2.01 0.43 2.47 1.91 0.791 ...\n $ lambda    : num  4.86 0.683 3.85 2.22 1.69 1.86 0.88 2.7 2.18 2.22 ...\n $ flc.grp   : num  10 1 10 9 6 9 1 10 9 6 ...\n $ creatinine: num  1.7 0.9 1.4 1 1.1 1 0.8 1.2 1.2 0.8 ...\n $ mgus      : num  0 0 0 0 0 0 0 0 0 0 ...\n $ futime    : int  85 1281 69 115 1039 1355 2851 372 3309 1326 ...\n $ death     : num  1 1 1 1 1 1 1 1 1 1 ...\n $ chapter   : Factor w/ 16 levels \"Blood\",\"Circulatory\",..: 2 13 2 2 2 11 11 14 15 2 ...\n\n\nThe modifications we will make are:\n\n\n\n\nObjectives:\n\nStratify summarys statistics for all variables by death"
  },
  {
    "objectID": "posts/table_one/archive/Summary Tables.html#method-1---gtsummary",
    "href": "posts/table_one/archive/Summary Tables.html#method-1---gtsummary",
    "title": "Summary Tables",
    "section": "Method 1 - gtsummary",
    "text": "Method 1 - gtsummary\n\nRequired data structure:\n\nIn general, tbl_summary requires data in long format. This is consistent across both the tbl_summary and tbl_strata functions.\n“Columns” in the output are defined by use of the by = argument.\n“Rows” in the output are defined by columns in the input data set.\nAll variables included in the input data are, by default, included in the output. Therefore, the user must select/un-select columns pursuant to what is required.\n\n\n\n#tf &lt;- tempfile(\"example\", fileext = \".docx\")\n\nflchain %&gt;%\n  gtsummary::tbl_summary(by = \"death\", # Levels to separate data by\n                         missing = \"ifany\", # Show missing values for a variable only if they exist\n                         digits = list(all_categorical() ~ c(0, 2), # Categorical counts 0dp; relative percentage 2dp\n                                       all_continuous() ~ 2, # Continuous summary stats 2dp\n                                       lambda ~ 1), # Report stats for `lambda` to 1dp\n                         type = all_continuous() ~ \"continuous2\", # Define multi-level summary stats for all continuous variables\n                         statistic = list(all_continuous() ~ c(\"{mean} ({sd})\",            # Display mean & standard deviation\n                                                               \"{median} ({p25}, {p75})\"), # Display median & IQR\n                                          all_categorical() ~ \"{n} ({p}%)\"),               # Display count & percentage\n                         label = list(sample.yr ~ \"Sample Year\")) %&gt;% # Amend label of `sample.yr`\n  \n  add_overall(last = TRUE) %&gt;% # Add overall counts, set to `last` column of output\n  italicize_labels() %&gt;% # Italicize variable labels\n  \n  add_p(test = list(all_categorical() ~ \"chisq.test\", # Apply chi square test to all categorical variables\n                    c(kappa, lambda) ~ \"t.test\", # Apply t.test to `kappa`, `lambda`\n                    c(flc.grp, creatinine) ~ \"kruskal.test\"), # Apply Kruskal-Wallis test to `flc.grp` and `creatinine`\n        test.args = list(all_tests(\"t.test\") ~ list(var.equal = F, # Equal variance assumption for t-test FALSE\n                                                       paired = F), # Paired t-test FALSE\n                         all_tests(\"chisq.test\") ~ list(correct = F)), # Continuity correction = FALSE to chi-square test                                                   \n        pvalue_fun = function(x) style_pvalue(x, digits = 3)) # Set p-value rounding (`gtsummary::style_pvalue` supports only values 1, 2, 3)\n\nThere was an error in 'add_p()/add_difference()' for variable 'chapter', p-value omitted:\nError in stats::chisq.test(x = structure(c(2L, 13L, 2L, 2L, 2L, 11L, 11L, : 'x' and 'y' must have at least 2 levels\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n0, N = 5,7051\n1, N = 2,1691\nOverall, N = 7,8741\np-value2\n\n\n\n\nage\n\n\n\n\n\n\n&lt;0.001\n\n\n    Mean (SD)\n60.84 (8.20)\n73.39 (10.32)\n64.29 (10.46)\n\n\n\n\n    Median (IQR)\n59.00 (54.00, 66.00)\n74.00 (67.00, 81.00)\n63.00 (55.00, 72.00)\n\n\n\n\nsex\n\n\n\n\n\n\n0.091\n\n\n    F\n3,185 (55.83%)\n1,165 (53.71%)\n4,350 (55.25%)\n\n\n\n\n    M\n2,520 (44.17%)\n1,004 (46.29%)\n3,524 (44.75%)\n\n\n\n\nSample Year\n\n\n\n\n\n\n&lt;0.001\n\n\n    1995\n861 (15.09%)\n414 (19.09%)\n1,275 (16.19%)\n\n\n\n\n    1996\n2,435 (42.68%)\n1,056 (48.69%)\n3,491 (44.34%)\n\n\n\n\n    1997\n1,012 (17.74%)\n369 (17.01%)\n1,381 (17.54%)\n\n\n\n\n    1998\n526 (9.22%)\n161 (7.42%)\n687 (8.72%)\n\n\n\n\n    1999\n283 (4.96%)\n67 (3.09%)\n350 (4.45%)\n\n\n\n\n    2000\n193 (3.38%)\n52 (2.40%)\n245 (3.11%)\n\n\n\n\n    2001\n137 (2.40%)\n38 (1.75%)\n175 (2.22%)\n\n\n\n\n    2002\n47 (0.82%)\n1 (0.05%)\n48 (0.61%)\n\n\n\n\n    2003\n211 (3.70%)\n11 (0.51%)\n222 (2.82%)\n\n\n\n\nkappa\n\n\n\n\n\n\n&lt;0.001\n\n\n    Mean (SD)\n1.27 (0.65)\n1.85 (1.26)\n1.43 (0.90)\n\n\n\n\n    Median (IQR)\n1.19 (0.91, 1.53)\n1.57 (1.16, 2.14)\n1.27 (0.96, 1.68)\n\n\n\n\nlambda\n\n\n\n\n\n\n&lt;0.001\n\n\n    Mean (SD)\n1.5 (0.7)\n2.1 (1.5)\n1.7 (1.0)\n\n\n\n\n    Median (IQR)\n1.4 (1.2, 1.8)\n1.8 (1.4, 2.4)\n1.5 (1.2, 1.9)\n\n\n\n\nflc.grp\n\n\n\n\n\n\n&lt;0.001\n\n\n    Mean (SD)\n4.98 (2.72)\n6.77 (2.84)\n5.47 (2.86)\n\n\n\n\n    Median (IQR)\n5.00 (3.00, 7.00)\n7.00 (5.00, 9.00)\n5.00 (3.00, 8.00)\n\n\n\n\ncreatinine\n\n\n\n\n\n\n&lt;0.001\n\n\n    Mean (SD)\n1.05 (0.31)\n1.19 (0.59)\n1.09 (0.42)\n\n\n\n\n    Median (IQR)\n1.00 (0.90, 1.20)\n1.10 (0.90, 1.30)\n1.00 (0.90, 1.20)\n\n\n\n\n    Unknown\n1,143\n207\n1,350\n\n\n\n\nmgus\n99 (1.74%)\n16 (0.74%)\n115 (1.46%)\n&lt;0.001\n\n\nfutime\n\n\n\n\n\n\n&lt;0.001\n\n\n    Mean (SD)\n4,226.20 (974.25)\n2,174.54 (1,380.34)\n3,661.04 (1,432.68)\n\n\n\n\n    Median (IQR)\n4,607.00 (4,033.00, 4,852.00)\n2,165.00 (945.00, 3,282.00)\n4,302.00 (2,852.00, 4,773.00)\n\n\n\n\nchapter\n\n\n\n\n\n\n\n\n\n\n    Blood\n0 (NA%)\n4 (0.18%)\n4 (0.18%)\n\n\n\n\n    Circulatory\n0 (NA%)\n745 (34.35%)\n745 (34.35%)\n\n\n\n\n    Congenital\n0 (NA%)\n3 (0.14%)\n3 (0.14%)\n\n\n\n\n    Digestive\n0 (NA%)\n66 (3.04%)\n66 (3.04%)\n\n\n\n\n    Endocrine\n0 (NA%)\n48 (2.21%)\n48 (2.21%)\n\n\n\n\n    External Causes\n0 (NA%)\n66 (3.04%)\n66 (3.04%)\n\n\n\n\n    Genitourinary\n0 (NA%)\n42 (1.94%)\n42 (1.94%)\n\n\n\n\n    Ill Defined\n0 (NA%)\n38 (1.75%)\n38 (1.75%)\n\n\n\n\n    Infectious\n0 (NA%)\n32 (1.48%)\n32 (1.48%)\n\n\n\n\n    Injury and Poisoning\n0 (NA%)\n21 (0.97%)\n21 (0.97%)\n\n\n\n\n    Mental\n0 (NA%)\n144 (6.64%)\n144 (6.64%)\n\n\n\n\n    Musculoskeletal\n0 (NA%)\n14 (0.65%)\n14 (0.65%)\n\n\n\n\n    Neoplasms\n0 (NA%)\n567 (26.14%)\n567 (26.14%)\n\n\n\n\n    Nervous\n0 (NA%)\n130 (5.99%)\n130 (5.99%)\n\n\n\n\n    Respiratory\n0 (NA%)\n245 (11.30%)\n245 (11.30%)\n\n\n\n\n    Skin\n0 (NA%)\n4 (0.18%)\n4 (0.18%)\n\n\n\n\n    Unknown\n5,705\n0\n5,705\n\n\n\n\n\n1 n (%)\n\n\n2 Wilcoxon rank sum test; Pearson’s Chi-squared test; Welch Two Sample t-test; Kruskal-Wallis rank sum test\n\n\n\n\n\n\n\n  #as_gt() %&gt;% \n  #gt::gtsave(filename = \"example.docx\")\n\n\n\nFor stratified tables, we can use tbl_summary inside of the tbl_strata function. Take for example the below, which stratifies by sex.\n\n\nflchain %&gt;%\n  tbl_strata(strata = sex, \n             .tbl_fun = ~.x %&gt;%\n  gtsummary::tbl_summary(by = \"death\",\n                         # test 1\n                         digits = lambda ~ 1,\n                         label = sample.yr ~ \"Sample Year\") %&gt;%\n  add_overall(last = TRUE) %&gt;%\n  italicize_labels() %&gt;%\n  add_p())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nF\nM\n\n\n0, N = 3,1851\n1, N = 1,1651\nOverall, N = 4,3501\np-value2\n0, N = 2,5201\n1, N = 1,0041\nOverall, N = 3,5241\np-value2\n\n\n\n\nage\n60 (54, 68)\n77 (69, 83)\n64 (56, 73)\n&lt;0.001\n59 (54, 65)\n72 (65, 78)\n62 (55, 70)\n&lt;0.001\n\n\nSample Year\n\n\n\n\n\n\n&lt;0.001\n\n\n\n\n\n\n&lt;0.001\n\n\n    1995\n508 (16%)\n196 (17%)\n704 (16%)\n\n\n353 (14%)\n218 (22%)\n571 (16%)\n\n\n\n\n    1996\n1,399 (44%)\n569 (49%)\n1,968 (45%)\n\n\n1,036 (41%)\n487 (49%)\n1,523 (43%)\n\n\n\n\n    1997\n563 (18%)\n228 (20%)\n791 (18%)\n\n\n449 (18%)\n141 (14%)\n590 (17%)\n\n\n\n\n    1998\n281 (8.8%)\n88 (7.6%)\n369 (8.5%)\n\n\n245 (9.7%)\n73 (7.3%)\n318 (9.0%)\n\n\n\n\n    1999\n139 (4.4%)\n38 (3.3%)\n177 (4.1%)\n\n\n144 (5.7%)\n29 (2.9%)\n173 (4.9%)\n\n\n\n\n    2000\n100 (3.1%)\n21 (1.8%)\n121 (2.8%)\n\n\n93 (3.7%)\n31 (3.1%)\n124 (3.5%)\n\n\n\n\n    2001\n66 (2.1%)\n21 (1.8%)\n87 (2.0%)\n\n\n71 (2.8%)\n17 (1.7%)\n88 (2.5%)\n\n\n\n\n    2002\n23 (0.7%)\n0 (0%)\n23 (0.5%)\n\n\n24 (1.0%)\n1 (&lt;0.1%)\n25 (0.7%)\n\n\n\n\n    2003\n106 (3.3%)\n4 (0.3%)\n110 (2.5%)\n\n\n105 (4.2%)\n7 (0.7%)\n112 (3.2%)\n\n\n\n\nkappa\n1.15 (0.87, 1.50)\n1.51 (1.12, 2.05)\n1.22 (0.92, 1.64)\n&lt;0.001\n1.24 (0.96, 1.58)\n1.65 (1.21, 2.24)\n1.33 (1.01, 1.75)\n&lt;0.001\n\n\nlambda\n1.4 (1.1, 1.8)\n1.7 (1.3, 2.3)\n1.5 (1.2, 1.9)\n&lt;0.001\n1.5 (1.2, 1.8)\n1.9 (1.4, 2.5)\n1.6 (1.2, 2.0)\n&lt;0.001\n\n\nflc.grp\n5.00 (2.00, 7.00)\n7.00 (4.00, 9.00)\n5.00 (3.00, 8.00)\n&lt;0.001\n5.00 (3.00, 7.00)\n8.00 (5.00, 10.00)\n6.00 (3.00, 8.00)\n&lt;0.001\n\n\ncreatinine\n0.90 (0.80, 1.00)\n1.00 (0.90, 1.13)\n0.90 (0.90, 1.10)\n&lt;0.001\n1.10 (1.00, 1.30)\n1.20 (1.03, 1.40)\n1.20 (1.00, 1.30)\n&lt;0.001\n\n\n    Unknown\n665\n93\n758\n\n\n478\n114\n592\n\n\n\n\nmgus\n57 (1.8%)\n11 (0.9%)\n68 (1.6%)\n0.047\n42 (1.7%)\n5 (0.5%)\n47 (1.3%)\n0.006\n\n\nfutime\n4,638 (4,082, 4,862)\n2,184 (971, 3,231)\n4,359 (2,871, 4,788)\n&lt;0.001\n4,578 (3,956, 4,844)\n2,159 (921, 3,337)\n4,226 (2,819, 4,739)\n&lt;0.001\n\n\nchapter\n\n\n\n\n\n\n&gt;0.9\n\n\n\n\n\n\n&gt;0.9\n\n\n    Blood\n0 (NA%)\n1 (&lt;0.1%)\n1 (&lt;0.1%)\n\n\n0 (NA%)\n3 (0.3%)\n3 (0.3%)\n\n\n\n\n    Circulatory\n0 (NA%)\n401 (34%)\n401 (34%)\n\n\n0 (NA%)\n344 (34%)\n344 (34%)\n\n\n\n\n    Congenital\n0 (NA%)\n0 (0%)\n0 (0%)\n\n\n0 (NA%)\n3 (0.3%)\n3 (0.3%)\n\n\n\n\n    Digestive\n0 (NA%)\n37 (3.2%)\n37 (3.2%)\n\n\n0 (NA%)\n29 (2.9%)\n29 (2.9%)\n\n\n\n\n    Endocrine\n0 (NA%)\n25 (2.1%)\n25 (2.1%)\n\n\n0 (NA%)\n23 (2.3%)\n23 (2.3%)\n\n\n\n\n    External Causes\n0 (NA%)\n35 (3.0%)\n35 (3.0%)\n\n\n0 (NA%)\n31 (3.1%)\n31 (3.1%)\n\n\n\n\n    Genitourinary\n0 (NA%)\n20 (1.7%)\n20 (1.7%)\n\n\n0 (NA%)\n22 (2.2%)\n22 (2.2%)\n\n\n\n\n    Ill Defined\n0 (NA%)\n25 (2.1%)\n25 (2.1%)\n\n\n0 (NA%)\n13 (1.3%)\n13 (1.3%)\n\n\n\n\n    Infectious\n0 (NA%)\n23 (2.0%)\n23 (2.0%)\n\n\n0 (NA%)\n9 (0.9%)\n9 (0.9%)\n\n\n\n\n    Injury and Poisoning\n0 (NA%)\n12 (1.0%)\n12 (1.0%)\n\n\n0 (NA%)\n9 (0.9%)\n9 (0.9%)\n\n\n\n\n    Mental\n0 (NA%)\n100 (8.6%)\n100 (8.6%)\n\n\n0 (NA%)\n44 (4.4%)\n44 (4.4%)\n\n\n\n\n    Musculoskeletal\n0 (NA%)\n11 (0.9%)\n11 (0.9%)\n\n\n0 (NA%)\n3 (0.3%)\n3 (0.3%)\n\n\n\n\n    Neoplasms\n0 (NA%)\n279 (24%)\n279 (24%)\n\n\n0 (NA%)\n288 (29%)\n288 (29%)\n\n\n\n\n    Nervous\n0 (NA%)\n73 (6.3%)\n73 (6.3%)\n\n\n0 (NA%)\n57 (5.7%)\n57 (5.7%)\n\n\n\n\n    Respiratory\n0 (NA%)\n121 (10%)\n121 (10%)\n\n\n0 (NA%)\n124 (12%)\n124 (12%)\n\n\n\n\n    Skin\n0 (NA%)\n2 (0.2%)\n2 (0.2%)\n\n\n0 (NA%)\n2 (0.2%)\n2 (0.2%)\n\n\n\n\n    Unknown\n3,185\n0\n3,185\n\n\n2,520\n0\n2,520\n\n\n\n\n\n1 Median (IQR); n (%)\n\n\n2 Wilcoxon rank sum test; Pearson’s Chi-squared test; Fisher’s exact test\n\n\n\n\n\n\n\n\n\nComments (pros/cons)\n\nPros\n\nMassive amount of options/variability in output\nFormatting is very nice in html form.\nEasy to add in and specify arguments to testing.\n\nCons\n\nDoes not render in Word document markdown format (can save output to Word document).\nCan be a bit clumsy when specifying some arguments (must be inside list, etc.)\nCan be annoying when working with high dimensional data (selecting large number of columns).\nWhen applying multiple tests within a class of variables (e.g., t-test for one continuous variable, Kruskal-Wallis test for another) it is not obvious which test is applied to which. Only the p-value is returned in each instance."
  },
  {
    "objectID": "posts/table_one/archive/Summary Tables.html#method-2---flextable",
    "href": "posts/table_one/archive/Summary Tables.html#method-2---flextable",
    "title": "Summary Tables",
    "section": "Method 2 - flextable",
    "text": "Method 2 - flextable\n\n#flchain %&gt;%\n#  flextable()\n\n\nComments (pros/cons)\n\nPros\n\nSupports html, Word, pdf, PowerPoint RMarkdown rendering (Word and PowerPoint requires officer package)\nCan also easily render table as Rplots or graphic files (.png, .pdf, .jpeg)\n\nCons\n\nSlow?"
  },
  {
    "objectID": "posts/table_one/archive/Summary Tables.html#method-3---huxtable",
    "href": "posts/table_one/archive/Summary Tables.html#method-3---huxtable",
    "title": "Summary Tables",
    "section": "Method 3 - huxtable",
    "text": "Method 3 - huxtable\n\nCan print the raw dataframe nicely in html format\n\n\nas_hux(head(flchain))\n\n\n\nagesexsample.yrkappalambdaflc.grpcreatininemgusfutimedeathchapter\n\n97F2e+035.7 4.86 101.70851Circulatory\n\n92F2e+030.870.68310.9012811Neoplasms\n\n94F2e+034.363.85 101.40691Circulatory\n\n92F2e+032.422.22 91  01151Circulatory\n\n93F2e+031.321.69 61.1010391Circulatory\n\n90F2e+032.011.86 91  013551Mental\n\n\n\n\n\n\nNot sure if this is correct? If so, this is clunky?\n\n\ncontinuous &lt;- flchain %&gt;%\n  group_by(death) %&gt;%\n  summarise(across(c(age, kappa, lambda, flc.grp, futime),\n                   list(mean = ~mean(.x, na.rm = T),\n                        sd   = ~sd(.x, na.rm = T)))) %&gt;%\n  pivot_longer(cols = -death,\n               names_to = c(\"variable\", \".value\"),\n               names_sep = \"_\",\n               values_to = \"value\") %&gt;%\n  ungroup \n  \n\ncategorical &lt;- flchain %&gt;%\n  select(death, sample.yr, mgus, chapter) %&gt;%\n  mutate(across(everything(), ~as_factor(.))) %&gt;%\n  pivot_longer(cols = -death,\n               names_to = \"variable\",\n               values_to = \"value\") %&gt;%\n  filter(!is.na(value)) %&gt;%\n  group_by(death, variable, value) %&gt;%\n  summarise(n = n())\n\n`summarise()` has grouped output by 'death', 'variable'. You can override using\nthe `.groups` argument.\n\nas_hux(continuous)\n\n\n\ndeathvariablemeansd\n\n0age60.8     8.2     \n\n0kappa1.27    0.645   \n\n0lambda1.54    0.699   \n\n0flc.grp4.98    2.72    \n\n0futime4.23e+03974       \n\n1age73.4     10.3     \n\n1kappa1.85    1.26    \n\n1lambda2.13    1.52    \n\n1flc.grp6.77    2.84    \n\n1futime2.17e+031.38e+03\n\n\n\nas_hux(categorical)\n\n\n\ndeathvariablevaluen\n\n0mgus05606\n\n0mgus199\n\n0sample.yr1995861\n\n0sample.yr19962435\n\n0sample.yr19971012\n\n0sample.yr1998526\n\n0sample.yr1999283\n\n0sample.yr2000193\n\n0sample.yr2001137\n\n0sample.yr200247\n\n0sample.yr2003211\n\n1chapterBlood4\n\n1chapterCirculatory745\n\n1chapterCongenital3\n\n1chapterDigestive66\n\n1chapterEndocrine48\n\n1chapterExternal Causes66\n\n1chapterGenitourinary42\n\n1chapterIll Defined38\n\n1chapterInfectious32\n\n1chapterInjury and Poisoning21\n\n1chapterMental144\n\n1chapterMusculoskeletal14\n\n1chapterNeoplasms567\n\n1chapterNervous130\n\n1chapterRespiratory245\n\n1chapterSkin4\n\n1mgus02153\n\n1mgus116\n\n1sample.yr1995414\n\n1sample.yr19961056\n\n1sample.yr1997369\n\n1sample.yr1998161\n\n1sample.yr199967\n\n1sample.yr200052\n\n1sample.yr200138\n\n1sample.yr20021\n\n1sample.yr200311\n\n\n\n\n\nComments (pros/cons)\n\nPros\n\nNicely renders html and pdf (LaTeX) tables\n\nCons\n\nClunky to specify?\nHave to do calculations yourself?"
  },
  {
    "objectID": "posts/table_one/archive/Summary Tables.html#method-4---tableone",
    "href": "posts/table_one/archive/Summary Tables.html#method-4---tableone",
    "title": "Summary Tables",
    "section": "Method 4 - tableone",
    "text": "Method 4 - tableone\n\ntableone::CreateTableOne(data = flchain,\n                         strata = \"death\",\n                         test = TRUE,\n                         testApprox = chisq.test, # For categorical variables (where Chi-Square approximation is appropriate), apply chi-square test\n                         argsApprox = list(correct = T), # Arguments to above\n                         \n                         testExact = fisher.test, # For categorical variables with small counts, apply Fisher's test\n                         argsExact = list(workspace = 2*10^5), # Arguments to above\n                         \n                         testNormal = oneway.test, # For normally distributed continuous variables, apply one-way test (==t.test if 2 groups)\n                         argsNormal = list(var.equal = T), # Arguments to above\n                         \n                         testNonNormal = kruskal.test, # For non-normally distributed continuous variables, apply Kruskal-Wallis rank-sum test\n                         argsNonNormal = list(NULL), # Arguments to above\n                         \n                         addOverall = T # Add overall column\n                         )\n\n                         Stratified by death\n                          Overall           0                1                \n  n                          7874              5705             2169          \n  age (mean (SD))           64.29 (10.46)     60.84 (8.20)     73.39 (10.32)  \n  sex = M (%)                3524 (44.8)       2520 (44.2)      1004 (46.3)   \n  sample.yr (mean (SD))   1996.79 (1.77)    1996.92 (1.88)   1996.44 (1.35)   \n  kappa (mean (SD))          1.43 (0.90)       1.27 (0.65)      1.85 (1.26)   \n  lambda (mean (SD))         1.70 (1.03)       1.54 (0.70)      2.13 (1.52)   \n  flc.grp (mean (SD))        5.47 (2.86)       4.98 (2.72)      6.77 (2.84)   \n  creatinine (mean (SD))     1.09 (0.42)       1.05 (0.31)      1.19 (0.59)   \n  mgus (mean (SD))           0.01 (0.12)       0.02 (0.13)      0.01 (0.09)   \n  futime (mean (SD))      3661.04 (1432.68) 4226.20 (974.25) 2174.54 (1380.34)\n  death (mean (SD))          0.28 (0.45)       0.00 (0.00)      1.00 (0.00)   \n  chapter (%)                                                                 \n     Blood                      4 ( 0.2)          0 ( NaN)         4 ( 0.2)   \n     Circulatory              745 (34.3)          0 ( NaN)       745 (34.3)   \n     Congenital                 3 ( 0.1)          0 ( NaN)         3 ( 0.1)   \n     Digestive                 66 ( 3.0)          0 ( NaN)        66 ( 3.0)   \n     Endocrine                 48 ( 2.2)          0 ( NaN)        48 ( 2.2)   \n     External Causes           66 ( 3.0)          0 ( NaN)        66 ( 3.0)   \n     Genitourinary             42 ( 1.9)          0 ( NaN)        42 ( 1.9)   \n     Ill Defined               38 ( 1.8)          0 ( NaN)        38 ( 1.8)   \n     Infectious                32 ( 1.5)          0 ( NaN)        32 ( 1.5)   \n     Injury and Poisoning      21 ( 1.0)          0 ( NaN)        21 ( 1.0)   \n     Mental                   144 ( 6.6)          0 ( NaN)       144 ( 6.6)   \n     Musculoskeletal           14 ( 0.6)          0 ( NaN)        14 ( 0.6)   \n     Neoplasms                567 (26.1)          0 ( NaN)       567 (26.1)   \n     Nervous                  130 ( 6.0)          0 ( NaN)       130 ( 6.0)   \n     Respiratory              245 (11.3)          0 ( NaN)       245 (11.3)   \n     Skin                       4 ( 0.2)          0 ( NaN)         4 ( 0.2)   \n                         Stratified by death\n                          p      test\n  n                                  \n  age (mean (SD))         &lt;0.001     \n  sex = M (%)              0.096     \n  sample.yr (mean (SD))   &lt;0.001     \n  kappa (mean (SD))       &lt;0.001     \n  lambda (mean (SD))      &lt;0.001     \n  flc.grp (mean (SD))     &lt;0.001     \n  creatinine (mean (SD))  &lt;0.001     \n  mgus (mean (SD))         0.001     \n  futime (mean (SD))      &lt;0.001     \n  death (mean (SD))       &lt;0.001     \n  chapter (%)                NaN     \n     Blood                           \n     Circulatory                     \n     Congenital                      \n     Digestive                       \n     Endocrine                       \n     External Causes                 \n     Genitourinary                   \n     Ill Defined                     \n     Infectious                      \n     Injury and Poisoning            \n     Mental                          \n     Musculoskeletal                 \n     Neoplasms                       \n     Nervous                         \n     Respiratory                     \n     Skin                            \n\n\n\nComments (pros/cons)\n\nPros\n\nImplicitly tests each variable for appropriateness of test (e.g., normal vs. non-normal etc.)\nSeems like a good “first pass” sort of function to apply in cases where we have a lot of variables. Perhaps less so a “publication ready” function.\n\nCons\n\nRenders nicely to console, not so much to Rmarkdown (relative to above)\nClunky to specify testing\n\nCannot easily tell which test is being applied to which variable (cannot tell whether variable is “normal” vs. “non-normal”)\nDifferent arguments specified for approximate vs. exact categorical tests (chi-square vs. Fisher’s test) and normal vs. non-normal continuous tests (t-test vs. Kruskal-Wallis Rank-Sum test)\n\nOutput is clunky and cannot be tweaked (decimal places, structure of output)"
  },
  {
    "objectID": "posts/table_one/archive/Summary Tables.html#method-5---desctable",
    "href": "posts/table_one/archive/Summary Tables.html#method-5---desctable",
    "title": "Summary Tables",
    "section": "Method 5 - desctable",
    "text": "Method 5 - desctable\n\nflchain %&gt;% \n  group_by(death) %&gt;%\n  desctable::desc_table(\"N\"    = length,\n                        \"Min\"  = min,\n                        \"Q1\"   = ~quantile(., .25),\n                        \"Med\"  = median,\n                        \"Mean\" = mean,\n                        \"Q3\"   = ~quantile(., .75),\n                        \"Max\"  = max,\n                        \"sd\"   = sd,\n                        \"IQR\"  = IQR) %&gt;%\n  #desctable::desc_tests() %&gt;%\n  desctable::desc_output(\"DT\")\n\n\n\n\n\n\nComments (pros/cons)\n\nPros\n\nInteractive table\nCan copy output directly to clipboard, excel (with interactive button)\n\nCons\n\nMust specify grouping variables outside of the desctable environment\nIs slooow.\nSpecification of tests are a little clunky and troublesome with categorical factors.\nSpecification of statistics between categorical and continuous variables are a little clunky (shows IQR for categorical variables, etc.)"
  },
  {
    "objectID": "posts/table_one/archive/Summary Tables.html#method-6---summarytools",
    "href": "posts/table_one/archive/Summary Tables.html#method-6---summarytools",
    "title": "Summary Tables",
    "section": "Method 6 - summarytools",
    "text": "Method 6 - summarytools\nflchain %&gt;%\n  group_by(death) %&gt;%\n  summarytools::dfSummary(round.digits = 2) %&gt;%\n  base::print(.)\nData Frame Summary\nflchain\nGroup: death = 0\nDimensions: 5705 x 11\nDuplicates: 0\n\n\n\n\n\n\n\n\n\n\n\n\nNo\nVariable\nStats / Values\nFreqs (% of Valid)\nGraph\nValid\nMissing\n\n\n\n\n1\nage [numeric]\nMean (sd) : 60.84 (8.2) min &lt; med &lt; max: 50 &lt; 59 &lt; 93 IQR (CV) : 12 (0.13)\n44 distinct values\n: : . : : . : : : : . : : : : : .\n5705 (100.0%)\n0 (0.0%)\n\n\n2\nsex [factor]\n1. F 2. M\n3185 (55.8%) 2520 (44.2%)\nIIIIIIIIIII IIIIIIII\n5705 (100.0%)\n0 (0.0%)\n\n\n3\nsample.yr [numeric]\nMean (sd) : 1996.92 (1.88) min &lt; med &lt; max: 1995 &lt; 1996 &lt; 2003 IQR (CV) : 1 (0)\n1995 : 861 (15.1%) 1996 : 2435 (42.7%) 1997 : 1012 (17.7%) 1998 : 526 ( 9.2%) 1999 : 283 ( 5.0%) 2000 : 193 ( 3.4%) 2001 : 137 ( 2.4%) 2002 : 47 ( 0.8%) 2003 : 211 ( 3.7%)\nIII IIIIIIII III I\n5705 (100.0%)\n0 (0.0%)\n\n\n4\nkappa [numeric]\nMean (sd) : 1.27 (0.65) min &lt; med &lt; max: 0.01 &lt; 1.19 &lt; 20.5 IQR (CV) : 0.62 (0.51)\n765 distinct values\n: : : : : .\n5705 (100.0%)\n0 (0.0%)\n\n\n5\nlambda [numeric]\nMean (sd) : 1.54 (0.7) min &lt; med &lt; max: 0.04 &lt; 1.43 &lt; 17.9 IQR (CV) : 0.62 (0.45)\n626 distinct values\n: : : : . : :\n5705 (100.0%)\n0 (0.0%)\n\n\n6\nflc.grp [numeric]\nMean (sd) : 4.98 (2.72) min &lt; med &lt; max: 1 &lt; 5 &lt; 10 IQR (CV) : 4 (0.55)\n1 : 654 (11.5%) 2 : 690 (12.1%) 3 : 678 (11.9%) 4 : 630 (11.0%) 5 : 637 (11.2%) 6 : 581 (10.2%) 7 : 588 (10.3%) 8 : 482 ( 8.4%) 9 : 484 ( 8.5%) 10 : 281 ( 4.9%)\nII II II II II II II I I\n5705 (100.0%)\n0 (0.0%)\n\n\n7\ncreatinine [numeric]\nMean (sd) : 1.05 (0.31) min &lt; med &lt; max: 0.5 &lt; 1 &lt; 10.8 IQR (CV) : 0.3 (0.29)\n27 distinct values\n: : : : :\n4562 (80.0%)\n1143 (20.0%)\n\n\n8\nmgus [numeric]\nMin : 0 Mean : 0.02 Max : 1\n0 : 5606 (98.3%) 1 : 99 ( 1.7%)\nIIIIIIIIIIIIIIIIIII\n5705 (100.0%)\n0 (0.0%)\n\n\n9\nfutime [integer]\nMean (sd) : 4226.2 (974.25) min &lt; med &lt; max: 1 &lt; 4607 &lt; 5215 IQR (CV) : 819 (0.23)\n1740 distinct values\n: . : : : . : : . . . : : :\n5705 (100.0%)\n0 (0.0%)\n\n\n11\nchapter [factor]\n1. Blood 2. Circulatory 3. Congenital 4. Digestive 5. Endocrine 6. External Causes 7. Genitourinary 8. Ill Defined 9. Infectious 10. Injury and Poisoning 11. Mental 12. Musculoskeletal 13. Neoplasms 14. Nervous 15. Respiratory 16. Skin\nAll NA’s\n\n0 (0.0%)\n5705 (100.0%)\n\n\n\nGroup: death = 1\nDimensions: 2169 x 11\nDuplicates: 0\n\n\n\n\n\n\n\n\n\n\n\n\nNo\nVariable\nStats / Values\nFreqs (% of Valid)\nGraph\nValid\nMissing\n\n\n\n\n1\nage [numeric]\nMean (sd) : 73.39 (10.32) min &lt; med &lt; max: 50 &lt; 74 &lt; 101 IQR (CV) : 14 (0.14)\n51 distinct values\n. : : : : : . : : : : . : : : : : : : : : : : : : : : .\n2169 (100.0%)\n0 (0.0%)\n\n\n2\nsex [factor]\n1. F 2. M\n1165 (53.7%) 1004 (46.3%)\nIIIIIIIIII IIIIIIIII\n2169 (100.0%)\n0 (0.0%)\n\n\n3\nsample.yr [numeric]\nMean (sd) : 1996.44 (1.35) min &lt; med &lt; max: 1995 &lt; 1996 &lt; 2003 IQR (CV) : 1 (0)\n1995 : 414 (19.1%) 1996 : 1056 (48.7%) 1997 : 369 (17.0%) 1998 : 161 ( 7.4%) 1999 : 67 ( 3.1%) 2000 : 52 ( 2.4%) 2001 : 38 ( 1.8%) 2002 : 1 ( 0.0%) 2003 : 11 ( 0.5%)\nIII IIIIIIIII III I\n2169 (100.0%)\n0 (0.0%)\n\n\n4\nkappa [numeric]\nMean (sd) : 1.85 (1.26) min &lt; med &lt; max: 0.12 &lt; 1.57 &lt; 16.4 IQR (CV) : 0.98 (0.68)\n525 distinct values\n: : : : : : : : .\n2169 (100.0%)\n0 (0.0%)\n\n\n5\nlambda [numeric]\nMean (sd) : 2.13 (1.52) min &lt; med &lt; max: 0.05 &lt; 1.77 &lt; 26.6 IQR (CV) : 1 (0.71)\n479 distinct values\n: : : : : :\n2169 (100.0%)\n0 (0.0%)\n\n\n6\nflc.grp [numeric]\nMean (sd) : 6.77 (2.84) min &lt; med &lt; max: 1 &lt; 7 &lt; 10 IQR (CV) : 4 (0.42)\n1 : 115 ( 5.3%) 2 : 121 ( 5.6%) 3 : 142 ( 6.5%) 4 : 156 ( 7.2%) 5 : 154 ( 7.1%) 6 : 210 ( 9.7%) 7 : 218 (10.1%) 8 : 248 (11.4%) 9 : 319 (14.7%) 10 : 486 (22.4%)\nI I I I I I II II II IIII\n2169 (100.0%)\n0 (0.0%)\n\n\n7\ncreatinine [numeric]\nMean (sd) : 1.19 (0.59) min &lt; med &lt; max: 0.4 &lt; 1.1 &lt; 10 IQR (CV) : 0.4 (0.49)\n45 distinct values\n: : : : : :\n1962 (90.5%)\n207 (9.5%)\n\n\n8\nmgus [numeric]\nMin : 0 Mean : 0.01 Max : 1\n0 : 2153 (99.3%) 1 : 16 ( 0.7%)\nIIIIIIIIIIIIIIIIIII\n2169 (100.0%)\n0 (0.0%)\n\n\n9\nfutime [integer]\nMean (sd) : 2174.54 (1380.34) min &lt; med &lt; max: 0 &lt; 2165 &lt; 4998 IQR (CV) : 2337 (0.63)\n1738 distinct values\n: : . : . . . : : : : : : : : : : : : : : : : : : : : : : : : : : : :\n2169 (100.0%)\n0 (0.0%)\n\n\n11\nchapter [factor]\n1. Blood 2. Circulatory 3. Congenital 4. Digestive 5. Endocrine 6. External Causes 7. Genitourinary 8. Ill Defined 9. Infectious 10. Injury and Poisoning [ 6 others ]\n4 ( 0.2%) 745 (34.3%) 3 ( 0.1%) 66 ( 3.0%) 48 ( 2.2%) 66 ( 3.0%) 42 ( 1.9%) 38 ( 1.8%) 32 ( 1.5%) 21 ( 1.0%) 1104 (50.9%)\nIIIIII\n2169 (100.0%)\n0 (0.0%)\n\n\n\n\nComments\n\nPros\n\nHas some pretty cool graphs\n\nCons\n\nA bit fiddly to render nicely in RMarkdown. Requires specification of additional arguments in chunk header (results = asis)\nStratification is clunky in output, and requires manual group_by argument."
  },
  {
    "objectID": "posts/table_one/archive/Summary Tables.html#method-7---janitor",
    "href": "posts/table_one/archive/Summary Tables.html#method-7---janitor",
    "title": "Summary Tables",
    "section": "Method 7 - janitor",
    "text": "Method 7 - janitor\n\n#janitor::tabyl(flchain)\n\n\nComments"
  },
  {
    "objectID": "posts/table_one/archive/Summary Tables.html#method-8---pastecs",
    "href": "posts/table_one/archive/Summary Tables.html#method-8---pastecs",
    "title": "Summary Tables",
    "section": "Method 8 - pastecs",
    "text": "Method 8 - pastecs\n\nflchain %&gt;%\n  pastecs::stat.desc(basic = FALSE,\n                     desc = TRUE)\n\n\n\nagesexsample.yrkappalambdaflc.grpcreatininemgusfutimedeathchapter\n\n63    2e+03       1.27  1.51  5     1      0      4.3e+03 0      \n\n64.3  2e+03       1.43  1.7   5.47  1.09   0.0146 3.66e+030.275  \n\n0.1180.0199  0.01010.01160.03230.005160.0013516.1     0.00503\n\n0.2310.039   0.01980.02280.06320.0101 0.0026531.6     0.00987\n\n109    3.12    0.804 1.06  8.2   0.173  0.0144 2.05e+060.2    \n\n10.5  1.77    0.897 1.03  2.86  0.417  0.12   1.43e+030.447  \n\n0.1630.0008840.627 0.605 0.523 0.381  8.21   0.391   1.62   \n\n\n\n\n\nComments\n\nPros\nCons\n\nDoes not nicely consider missing values for logical/categorical variables."
  },
  {
    "objectID": "posts/table_one/archive/Summary Tables.html#method-9---psych",
    "href": "posts/table_one/archive/Summary Tables.html#method-9---psych",
    "title": "Summary Tables",
    "section": "Method 9 - psych",
    "text": "Method 9 - psych\n\npsych::describe(flchain)\n\n\n\nvarsnmeansdmediantrimmedmadminmaxrangeskewkurtosisse\n\n17.87e+0364.3     10.5     63      63.5     11.9  50   101       51       0.591 -0.44  0.118  \n\n27.87e+031.45    0.497   1      1.43    0    1   2       1       0.211 -1.96  0.0056 \n\n37.87e+032e+03       1.77    2e+03      2e+03       1.48 2e+03   2e+03       8       1.78  3.16  0.0199 \n\n47.87e+031.43    0.897   1.27   1.32    0.5230.0120.5     20.5     5.29  63     0.0101 \n\n57.87e+031.7     1.03    1.51   1.57    0.5190.0426.6     26.6     6.86  98.9   0.0116 \n\n67.87e+035.47    2.86    5      5.46    2.97 1   10       9       0.0208-1.22  0.0323 \n\n76.52e+031.09    0.417   1      1.05    0.1480.4 10.8     10.4     10.4   178     0.00516\n\n87.87e+030.0146  0.12    0      0       0    0   1       1       8.09  63.5   0.00135\n\n97.87e+033.66e+031.43e+034.3e+033.89e+03887    0   5.22e+035.22e+03-1.11  0.046116.1    \n\n107.87e+030.275   0.447   0      0.219   0    0   1       1       1.01  -0.99  0.00503\n\n112.17e+038.38    5.32    11      8.35    5.93 1   16       15       -0.151 -1.75  0.114  \n\n\n\n\n\nComments\n\nSeems like a more comprehensive version of base::summary?\nPros\nCons"
  },
  {
    "objectID": "posts/table_one/archive/Summary Tables.html#method-10---hmisc",
    "href": "posts/table_one/archive/Summary Tables.html#method-10---hmisc",
    "title": "Summary Tables",
    "section": "Method 10 - Hmisc",
    "text": "Method 10 - Hmisc\n\nHmisc::describe(flchain)\n\nflchain \n\n 11  Variables      7874  Observations\n--------------------------------------------------------------------------------\nage \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    7874        0       51    0.999    64.29    11.81       51       52 \n     .25      .50      .75      .90      .95 \n      55       63       72       79       84 \n\nlowest :  50  51  52  53  54, highest:  96  97  99 100 101\n--------------------------------------------------------------------------------\nsex \n       n  missing distinct \n    7874        0        2 \n                      \nValue          F     M\nFrequency   4350  3524\nProportion 0.552 0.448\n--------------------------------------------------------------------------------\nsample.yr \n       n  missing distinct     Info     Mean      Gmd \n    7874        0        9    0.902     1997    1.709 \n                                                                \nValue       1995  1996  1997  1998  1999  2000  2001  2002  2003\nFrequency   1275  3491  1381   687   350   245   175    48   222\nProportion 0.162 0.443 0.175 0.087 0.044 0.031 0.022 0.006 0.028\n\nFor the frequency table, variable is rounded to the nearest 0\n--------------------------------------------------------------------------------\nkappa \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    7874        0      926        1    1.431   0.7854   0.5100   0.6963 \n     .25      .50      .75      .90      .95 \n  0.9600   1.2700   1.6800   2.2470   2.7600 \n\nlowest : 0.01  0.05  0.07  0.1   0.105, highest: 11.9  12.7  13.8  16.4  20.5 \n--------------------------------------------------------------------------------\nlambda \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    7874        0      796        1    1.703   0.8341    0.849    0.990 \n     .25      .50      .75      .90      .95 \n   1.200    1.510    1.920    2.527    3.110 \n\nlowest : 0.04   0.0433 0.047  0.0499 0.0573, highest: 17     17.8   17.9   20.5   26.6  \n--------------------------------------------------------------------------------\nflc.grp \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    7874        0       10     0.99    5.471    3.289        1        2 \n     .25      .50      .75      .90      .95 \n       3        5        8        9       10 \n                                                                      \nValue          1     2     3     4     5     6     7     8     9    10\nFrequency    769   811   820   786   791   791   806   730   803   767\nProportion 0.098 0.103 0.104 0.100 0.100 0.100 0.102 0.093 0.102 0.097\n\nFor the frequency table, variable is rounded to the nearest 0\n--------------------------------------------------------------------------------\ncreatinine \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    6524     1350       50    0.977    1.094   0.2944      0.8      0.8 \n     .25      .50      .75      .90      .95 \n     0.9      1.0      1.2      1.4      1.5 \n\nlowest : 0.4  0.5  0.6  0.7  0.8 , highest: 8.6  8.9  9.6  10   10.8\n--------------------------------------------------------------------------------\nmgus \n       n  missing distinct     Info      Sum     Mean      Gmd \n    7874        0        2    0.043      115  0.01461  0.02879 \n\n--------------------------------------------------------------------------------\nfutime \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    7874        0     2977        1     3661     1511    470.3   1194.3 \n     .25      .50      .75      .90      .95 \n  2852.0   4302.0   4773.0   4929.4   4977.0 \n\nlowest :    0    1    2    3    4, highest: 5166 5171 5177 5187 5215\n--------------------------------------------------------------------------------\ndeath \n       n  missing distinct     Info      Sum     Mean      Gmd \n    7874        0        2    0.599     2169   0.2755   0.3992 \n\n--------------------------------------------------------------------------------\nchapter \n       n  missing distinct \n    2169     5705       16 \n\nlowest : Blood           Circulatory     Congenital      Digestive       Endocrine      \nhighest: Musculoskeletal Neoplasms       Nervous         Respiratory     Skin           \n--------------------------------------------------------------------------------\n\n\n\nComments\n\nPros\nCons\n\nClunky and long output, particularly if there are lots of variables.\nSlow to render.\n\n\n # Conclusion"
  }
]