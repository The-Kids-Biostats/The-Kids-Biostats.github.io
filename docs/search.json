[
  {
    "objectID": "posts/2024_08_09_ai_part_two/ai_part_two.html",
    "href": "posts/2024_08_09_ai_part_two/ai_part_two.html",
    "title": "AI in Biostatistics - Part 2",
    "section": "",
    "text": "If you haven’t read part one of this two-part series on AI, do that first here:\nhttps://the-kids-biostats.github.io/posts/2024_08_08_ai_part_one/ai_part_one.html\nLast time we looked at three cases where our use of AI aided our productivity and gave us the desired result. In this post, we’ll be looking at the other side of the coin, to situations where AI has struggled and where attempts to pursue its use would hinder our productivity and have us produced errors.\nTo demonstrate this, we’ll look through the results of a few published papers and the concepts they introduce. Then, as a wrap of Part 1 and 2, we propose some guidelines that can be used to maximise ones efficiency when using (or thinking about using) AI."
  },
  {
    "objectID": "posts/2024_08_09_ai_part_two/ai_part_two.html#example-1---jagged-frontier",
    "href": "posts/2024_08_09_ai_part_two/ai_part_two.html#example-1---jagged-frontier",
    "title": "AI in Biostatistics - Part 2",
    "section": "Example 1 - Jagged Frontier",
    "text": "Example 1 - Jagged Frontier\nOur first example is drawn from a 2023 paper which looked at the effect of AI use on productivity and quality of work. You can read the paper here:\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=4573321\nIt introduces a very helpful concept - the ‘Jagged Frontier’. Below we use ggplot2 to demonstrate this idea.\n\nWhat is the Jagged Frontier?\nImagine you have 5 tasks to complete today, and you are trying to decide which of these you can delegate to AI (in this case, ChatGPT). You might have in mind that each task has some implicit difficulty associated with it (rated 1- 10), and your perception of current AI technology is that in is capable of tackling tasks up to a difficulty of 5. This is represented by the blue dashed line below.\nYour 5 tasks have various difficulties, represented by the black points. At this point, based on your perception of the difficulty of your tasks and the capability of AI, you would assign tasks 2, 3 and 5 to ChatGPT, since these have a difficulty of &lt;=5.\nUnfortunately, the reality is that our perception of difficulty does not line up well with AI’s capabilities. As we have seen in the last post, it can complete some pretty incredible tasks, such as coding Shiny apps and performing advanced statistical analyses. But as we will also see later, it can struggle with some pretty mundane tasks.\nAnd so the capability of AI cannot be measured by the blue dashed line, but rather the red, jagged line, a seemingly random level of capability which is unrelated to our ideas of difficulty. This is what is referred to as the “Jagged Frontier”.\n\n\n\n\n\n\n\n\n\n\n\nWhy should we care?\nWhen a task can successfully be completed using AI, we refer to that task as being ‘Inside the Frontier’; that is, below the red jagged line above. When AI would struggle or be unable to complete a task, we refer to that as being ‘Outside the Frontier’.\nThe study above set out to investigate the difference in worker productivity when AI is used for tasks considered ‘Inside the Frontier’ compared with those considered ‘Outside the Frontier’. A large study was set up, with the following methodology:\n\nAll 758 participants (experts in the relevant field) complete a baseline project without any AI use. Speed and quality are measured.\nParticipants are randomised into one of four groups (see table below).\n\n\n\n\n\n\nMetric\nInside.Frontier\nOutside.Frontier\n\n\n\n\nChatGPT\n189\n190\n\n\nChatGPT + Overview\n190\n189\n\n\n\n\n\n\n\n\nThose in the “Inside Frontier” group were assigned a project that had been determined beforehand to be within the capabilities of ChatGPT 3.5. In contrast, the “Outside Frontier” group were assigned a project beyond the capabilities of ChatGPT 3.5.\nThe participants were further divided into “ChatGPT” and “ChatGPT + Overview” groups. The latter received some training on prompt engineering (how be to use ChaptGPT) before undertaking the project.\nAll participants completed their project, and their speed and quality was compared to their own individual baseline score.\n\nThe results are quite striking:\n\n\n\n\n\nMetric\nInside.Frontier\n\n\n\n\nChatGPT\n\n\nSpeed\n28%\n\n\nQuality\n38%\n\n\nChatGPT + Overview\n\n\nSpeed\n23%\n\n\nQuality\n43%\n\n\n\n\n\n\n\nUsing ChatGPT for the project ‘Inside the Frontier’ resulted in considerable improvements in both speed and quality of work, regardless of whether participants received training.\nBut perhaps more interestingly, the groups with projects ‘Outside the Frontier’ saw a decrease in quality of work, even though the work was completed faster…\nWe will return to these results in the discussion below."
  },
  {
    "objectID": "posts/2024_08_09_ai_part_two/ai_part_two.html#example-2---biostatistical-questions",
    "href": "posts/2024_08_09_ai_part_two/ai_part_two.html#example-2---biostatistical-questions",
    "title": "AI in Biostatistics - Part 2",
    "section": "Example 2 - Biostatistical Questions",
    "text": "Example 2 - Biostatistical Questions\nAs promised, we are going to investigate some examples of tasks that lie ‘Outside the Frontier’. These examples are drawn from this paper:\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC10646144/\nAs a brief summary, 10 different biostatistics-related problems were posed to ChatGPT 3.5 and subsequently to ChatGPT 4.0. For each problem, the team attempted up to three times to get the correct answer from the AI. The results are below:\n\n\n\n\n\n\n\n\n\nWhile, ChatGPT 4.0 made significant improvements on 3.5, 40% of answers were still incorrect on the first attempt.\nWe attempted to reproduce one of these results, using their Question 2, which was:\n“Suppose the probability of surviving from a particular disease is 0.9 and there are 20 patients. The number surviving will follow a Binomial distribution with p=0.9 and n=20. What is the probability that no more than 1 patient dies?”\nThe expected answer to this question is 39.2%, and there are two ways we could get this answer in R using the pbinom function:\n\nsize &lt;- 20 # Number of trials\nprob &lt;- 0.1 # Probability of *dying*\n\npbinom(1, size, prob)\n\n[1] 0.391747\n\n\n\nprob &lt;- 0.9 # Probability of *surviving*\n\n1 - pbinom(18, size, prob)\n\n[1] 0.391747\n\n\nGraphically, we can highlight the area of the distribution we are interested in calculating - this will be helpful later to compare with ChatGPT’s responses."
  },
  {
    "objectID": "posts/2024_08_09_ai_part_two/ai_part_two.html#first-attempt",
    "href": "posts/2024_08_09_ai_part_two/ai_part_two.html#first-attempt",
    "title": "AI in Biostatistics - Part 2",
    "section": "First Attempt:",
    "text": "First Attempt:\nBelow is our prompt history with ChatGPT 4.0 as we attempted to reproduce the results from the paper - namely, using a maximum of three attempts to get the correct answer.\n\n\n\n\n\n\nNote.\n\n\n\nHighlighted sections and markings were added for emphasis and to assist with the live presentation that these two blog posts have been adapted from.\n\n\n\n\n\n\n\n\n\n\n\nOn ChatGPT’s first attempt, it gives us an answer of 87.8%, a far cry from 39.2%! The blue icon at the end of its answer reveals the Python code it used to generate this answer, however we asked it to provide the equivalent code in R:\n\n\n\n\n\n\n\n\n\nWe can run the code here to verify the answer:\n\nn &lt;- 20 # number of trials\np &lt;- 0.9 # probability of success\n\nprob &lt;- pbinom(19, n, p)\n\nprob\n\n[1] 0.8784233\n\n\nInterestingly, the binom package is not actually used!\nThe AI has seemingly ‘understood’ the question, as shown by its correct inference that “no more than 1 patient dies” is equivalent to “at least 19 patients surviving” in its code documentation. However, the code provided has gotten things the wrong way around.\nAbove, we showed the correct area of the distribution we are interested in. Now we can see this side by side with what the AI code is calculating:"
  },
  {
    "objectID": "posts/2024_08_09_ai_part_two/ai_part_two.html#second-attempt",
    "href": "posts/2024_08_09_ai_part_two/ai_part_two.html#second-attempt",
    "title": "AI in Biostatistics - Part 2",
    "section": "Second Attempt",
    "text": "Second Attempt\nIn the paper, the way in which ChatGPT was prompted to try again at a particular problem was to simply say “I got ”. We do the same here:\n\n\n\n\n\n\n\n\n\nWe see that the AI has doubled down and is suggesting we have misunderstood the question. It describes the problem back to us perfectly, with one exception:\n“The focus should be on the cumulative probability of having up to 19 survivors.”\nWhile this is incorrect, it is the first time anything it has said has matched the calculation it carried out."
  },
  {
    "objectID": "posts/2024_08_09_ai_part_two/ai_part_two.html#third-attempt",
    "href": "posts/2024_08_09_ai_part_two/ai_part_two.html#third-attempt",
    "title": "AI in Biostatistics - Part 2",
    "section": "Third Attempt",
    "text": "Third Attempt\nWith final attempt, it is up for debate whether the response is correct or not:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe code it provides does give the correct answer now, however there is still a ‘belief’ that 0.392 was and is incorrect.\n\nn &lt;- 20 # number of trials\np &lt;- 0.9 # probability of success\n\nprob_no_more_than_1_dying &lt;- pbinom(1, n, 1- p)\n\nprint(prob_no_more_than_1_dying)\n\n[1] 0.391747"
  },
  {
    "objectID": "posts/2024_08_09_ai_part_two/ai_part_two.html#takeaway",
    "href": "posts/2024_08_09_ai_part_two/ai_part_two.html#takeaway",
    "title": "AI in Biostatistics - Part 2",
    "section": "Takeaway",
    "text": "Takeaway\nWithout knowing how pbinom works (and even then, noticing it had been implemented incorrectly!), it is likely that a user would trust the initial incorrect answer. The AI showed signs of understanding the question, the code documentation made sense, and the code itself looked completely plausible…\nHow much more so for more complicated scenarios?"
  },
  {
    "objectID": "posts/2024_08_09_ai_part_two/ai_part_two.html#where-to-from-here-our-recommendations",
    "href": "posts/2024_08_09_ai_part_two/ai_part_two.html#where-to-from-here-our-recommendations",
    "title": "AI in Biostatistics - Part 2",
    "section": "Where to from here? Our recommendations…",
    "text": "Where to from here? Our recommendations…\nThe ‘jagged frontier’ of AI is invisible to us, and hence we often cannot know whether the task we are about to delegate is within its capability or not (and it doesn’t know!). For this reason, we propose the following guidelines to ensure you are using AI effectively and responsibly:\n\nYou should be able to verify the output is true/accurate.\nThe time it would take to verify the AI output should be less than the time it would take to achieve the same output yourself.\n\nLooking back at our examples throughout the two posts:\n\nWeather plot\nIn this example, we could verify the more complex plot by visually comparing it to simple plots we coded ourselves. Additionally, we could understand the R code that AI produced. Whether the use of AI in this case was justified or not depends on how quickly we could have remembered all the ggplot2 commands to do this ourselves!\n\n\nStatistical Analysis\nFor the statistical analysis, we were well-placed to verify the suggestions of ChatGPT. However, this would be a dangerous use-case for someone who is unfamiliar with statistical methodology - advanced or otherwise. We know that the suggested hierarchical mixed models is a good suggestion, but this would not be immediately verifiable to everyone. Therefore we would recommend against this use-case for non-statisticians.\n\n\nShiny App\nThis is probably the best example of AI saving time and increasing quality. The objective is simple - a Shiny App which takes and randomises data. Even as someone who has never coded a Shiny App before, it is easy to verify whether it works by simply using the app.\n\n\nMathematical Questions\nFinally, we have seen that AI can make mistakes when calculating numerical results, and remain confidently incorrect in the face of correction. To verify the answer requires working line by line through each piece of code to ensure it is doing what we asked. Hence it is unlikely to save us time and seemingly likely to be incorrect."
  },
  {
    "objectID": "posts/2024_08_09_ai_part_two/ai_part_two.html#further-reading",
    "href": "posts/2024_08_09_ai_part_two/ai_part_two.html#further-reading",
    "title": "AI in Biostatistics - Part 2",
    "section": "Further reading",
    "text": "Further reading\nA similar paper to Example 2, testing responses to a set of 70 medical questions: https://onlinelibrary.wiley.com/doi/full/10.1111/iej.13985\nDespite the humourous title, a fascinating paper on where Artificial Intelligence is, well, intelligent: https://link.springer.com/article/10.1007/s10676-024-09775-5"
  },
  {
    "objectID": "posts/2024-07-22_pre_post/pre-post.html",
    "href": "posts/2024-07-22_pre_post/pre-post.html",
    "title": "Analysing pre-post data",
    "section": "",
    "text": "Pre-post designs are in use everywhere we look. Before and after treatment, e.g. in a randomised clinical trial, might be the example that most readily comes to mind for analysts in the health sciences area. However, the pre-post concept occurs in many non-randomised settings as well, like observational studies or retrospective policy evaluation studies, and can feature in our daily lives without us even knowing it (like in A/B testing to see if you spend just a little bit longer on social media or buy that chocolate bar at the checkout).\nWhile in principle, the set-up (premise) for such a question seems straightforward, there is extensive literature debating the different approaches to the analysis of this data. The discussions around these choices may confuse the lay or even semi-experienced analyst. Often these discussions are held without a serviceable example to illustrate implementation.\nBelow is an example to help provide some direct links between what we see with raw data and model output, when tackling this question."
  },
  {
    "objectID": "posts/2024-07-22_pre_post/pre-post.html#change-score-analysis",
    "href": "posts/2024-07-22_pre_post/pre-post.html#change-score-analysis",
    "title": "Analysing pre-post data",
    "section": "Change score analysis",
    "text": "Change score analysis\nLet’s reduce the two observations we have per participant into one ‘difference’ (change score: post-value minus pre-value).\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\ncontrol, N = 1971\ntreatment, N = 2041\n\n\n\n\nchange\n20.34 (4.7) [197]\n50.24 (4.5) [204]\n\n\n\n1 Mean (SD)\n[No. obs.]\n\n\n\n\n\n\n\n\nThis makes sense, the mean change of ~20 and ~50 for the control and treatment groups (respectively) aligns with what we asked for in the data generation process.\n\n\nCode\ndat %&gt;% \n  tibble %&gt;% \n  pivot_wider(id_cols = c(id, group), names_from = timepoint, values_from = c(out, time)) %&gt;% \n  mutate(change = out_post - out_pre) %&gt;% \n  select(group, change) %&gt;% \n  ggplot(aes(group, change, colour = group)) +\n  geom_violin(aes(fill = group), alpha = 0.1) +\n  geom_jitter(width = 0.1, height = 0, alpha = 0.3) +\n  theme_clean() +\n  scale_colour_viridis_d(option = \"plasma\", end = 0.85) +\n  scale_fill_viridis_d(option = \"plasma\", end = 0.85) +\n  labs(y = \"Outcome\", x = \"Timepoint\",\n       title = \"Pre-post change score analysis\", \n       subtitle = \"Two-groups, violin plot with jittered points\", colour = \"Group\", fill = \"Group\")\n\n\n\n\n\n\n\n\n\nWith one observation per participant and two groups (and approximately normally distributed data), we can use a t.test to evaluate the difference between groups.\n\n\nRegistered S3 methods overwritten by 'broom':\n  method            from  \n  tidy.glht         jtools\n  tidy.summary.glht jtools\n\n\n# A tibble: 1 × 7\n  estimate statistic   p.value conf.low conf.high method             alternative\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;      \n1    -29.9     -64.9 3.13e-213    -30.8     -29.0 Welch Two Sample … two.sided  \n\n\nThe output suggests:\n\nA difference between groups means of ~30.\n\nThis exact result, with using a change score as the outcome, can also be reached using linear regression (left to the reader)."
  },
  {
    "objectID": "posts/2024-07-22_pre_post/pre-post.html#linear-regression-ancova-framework",
    "href": "posts/2024-07-22_pre_post/pre-post.html#linear-regression-ancova-framework",
    "title": "Analysing pre-post data",
    "section": "Linear regression (ANCOVA framework)",
    "text": "Linear regression (ANCOVA framework)\nHere, the outcome variable is the post-value with the treatment variable as the exposure of interest (RHS of the model).\nlm(out_post ~ out_pre + group, data = dat)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n20.77\n19.66, 21.88\n&lt;0.001\n\n\nout_pre\n0.96\n0.87, 1.05\n&lt;0.001\n\n\ngroup\n\n\n\n\n\n\n\n\n    control\n—\n—\n\n\n\n\n    treatment\n30.36\n29.06, 31.65\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nThis output suggests (in order):\n\nThe intercept (the mean value for a control participant with 0 as a pre-value) is ~21\n\nThis may be curious, we know the post-values for the control group have a mean of ~30, however, the mean pre -alue for the control groups is ~10. Take that 10 away from the ~30, and this is why we see ~20 here for a control participant with 0 as a pre-value)\n\nFor every 1 unit increase in a participants pre-value, their post-value will be 0.97 units higher (… than if their pre-value was 1 unit lower)\n[Perhaps of most interest] The post-values for the treatment group are (on average) ~30 units higher than the control group.\n\n\n\nCode\ndat %&gt;% \n  tibble %&gt;% \n  mutate(timepoint = as.numeric(factor(timepoint, levels = c(\"pre\", \"post\"))) - 1,\n         group = as.numeric(factor(group, levels = c(\"control\", \"treatment\"))) -1) %&gt;% \n  mutate(jittered_pos = jitter(timepoint, amount = 0.05),\n         dodge_pos = group * 0.25 ) %&gt;% #- 0.25/2)\n  mutate(group = factor(group, labels = c(\"control\", \"treatment\"))) %&gt;% \n  ggplot(aes(x = dodge_pos + jittered_pos, y = out, group = id, colour = group)) +\n  geom_point(aes(x = dodge_pos + jittered_pos), \n             position = position_dodge(width = 0.25), alpha = 0.3) +\n  geom_line(aes(x = dodge_pos + jittered_pos), alpha = 0.2) +\n  scale_x_continuous(breaks = 0:1, labels = c(\"pre\", \"post\")) +\n  theme_clean() +\n  scale_colour_viridis_d(option = \"plasma\", end = 0.85) +\n  labs(y = \"Outcome\", x = \"Timepoint\",\n       title = \"Pre-post analysis\", \n       subtitle = \"Two-groups, jitterplot with line connecting pairs of observations\", colour = \"Group\")\n\n\n\n\n\n\n\n\n\nWith lines connecting each pairs’ data points together, we can perhaps more clearly see that those with a higher pre-value also have a higher post-value! (What might the data look like if this wasn’t the case…)\nBut you may be wondering …?\n\n\nCode\ndat_b &lt;- dat %&gt;% \n  mutate(timepoint = factor(timepoint, levels = c(\"pre\", \"post\"))) %&gt;% \n  group_by(group, timepoint) %&gt;% \n  summarise(out_sd = sd(out),\n            out = mean(out)) %&gt;% \n  filter(timepoint == \"post\")\n\ndat %&gt;% \n  mutate(timepoint = factor(timepoint, levels = c(\"pre\", \"post\"))) %&gt;% \n  ggplot(aes(timepoint, out)) +\n  geom_jitter(aes(colour = group, group = group), \n              position = position_jitterdodge(jitter.width = 0.1, dodge.width = 0.35), alpha = 0.3) +\n  geom_smooth(aes(colour = group, group = group), \n              position = position_dodge(width = 0.45),\n              method = \"lm\", formula= y ~ x, alpha = 0.6) +\n  theme_clean() +\n  theme(legend.position = \"bottom\") +\n  scale_colour_viridis_d(option = \"plasma\", end = 0.85) +\n  labs(y = \"Outcome\", x = \"Timepoint\",\n       title = \"Pre-post analysis\", \n       subtitle = \"Two-groups, jitterplot with line connecting observed means\", colour = \"Group\") +\n  stat_brace(data = dat_b, aes(group = timepoint), \n             rotate = 90, width = 0.1, outerstart = 2.2, bending = 1) +\n  scale_y_continuous(breaks = seq(0,100,10)) +\n  geom_text(data = tibble(timepoint = c(2.35),\n                          out = c(50.8), # (71.2-30.4) / 2 + 30.4\n                          label = c(\"A difference of ~40?\\nThe model said ~30?\")),\n            aes(label = label), size = 3.5, hjust = 0) +\n  coord_cartesian(xlim = c(1.25, 2.35)) \n\n\n\n\n\n\n\n\n\n\n\nggbrace()\nIt was a bit of a battle with stat_brace() to get the annotation in there, but it was worth it, Shirley!\nWe know the post-value means are ~40 units apart, yet the model has returned the value of ~30 for the ‘treatment effect’, which (implicitly) has adjusted for the baseline difference between groups of ~10 units."
  },
  {
    "objectID": "posts/2024-07-22_pre_post/pre-post.html#linear-mixed-effects-models",
    "href": "posts/2024-07-22_pre_post/pre-post.html#linear-mixed-effects-models",
    "title": "Analysing pre-post data",
    "section": "Linear mixed effects models",
    "text": "Linear mixed effects models\nHere, the outcome variable includes both the pre- and post-measures with the interaction on the RHS of the model allowing the effect of the treatment variable (as the exposure of interest) to vary according to time.\nlmer(out ~ group*timepoint + (1 | id), data = dat)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\n\n\n\n\n(Intercept)\n10.11\n9.28, 10.94\n\n\ngroup\n\n\n\n\n\n\n    control\n—\n—\n\n\n    treatment\n10.44\n9.27, 11.61\n\n\ntimepoint\n\n\n\n\n\n\n    pre\n—\n—\n\n\n    post\n20.34\n19.69, 20.98\n\n\ngroup * timepoint\n\n\n\n\n\n\n    treatment * post\n29.91\n29.00, 30.81\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nThis output suggests (in order):\n\nThe intercept (mean value for a control participant at the pre-measure) is ~10 (makes sense)\nThe treatment group mean (at the pre-measure) is ~11 units higher than the control group mean (makes sense)\nThe post-values in the control group are (on average) ~20 units higher than the pre-values \n[Perhaps of most interest] Compared to the ~20 unit difference (over time) in the control group, the post-values in the treatment group are (an additional) ~30 units higher than the pre-values (total of 50 units difference between pre and post)\n\nIn looking at, and breaking down, this output, you might think that the linear mixed effects model gives you the same answer but with a much more explicit breakdown of many other ‘things going on’ with your outcome data - and you’d be right."
  },
  {
    "objectID": "posts/2024-07-22_pre_post/pre-post.html#acknowledgements",
    "href": "posts/2024-07-22_pre_post/pre-post.html#acknowledgements",
    "title": "Analysing pre-post data",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Elizabeth McKinnon, Zac Dempsey, and Wesley Billingham for providing feedback on and reviewing this post.\nYou can look forward to seeing posts from these other team members here in the coming weeks and months."
  },
  {
    "objectID": "posts/2024-07-22_pre_post/pre-post.html#reproducibility-information",
    "href": "posts/2024-07-22_pre_post/pre-post.html#reproducibility-information",
    "title": "Analysing pre-post data",
    "section": "Reproducibility Information",
    "text": "Reproducibility Information\nTo access the .qmd (Quarto markdown) files as well as any R scripts or data that was used in this post, please visit our GitHub:\nThe session information can also be seen below.\n\n\nR version 4.4.0 (2024-04-24)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.5\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Australia/Perth\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggbrace_0.1.1     gtsummary_1.7.2   jtools_2.2.2      lme4_1.1-35.4    \n [5] Matrix_1.7-0      data.table_1.15.4 simstudy_0.8.0    lubridate_1.9.3  \n [9] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[13] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1    \n[17] tidyverse_2.0.0  \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.5         xfun_0.46            htmlwidgets_1.6.4   \n [4] lattice_0.22-6       tzdb_0.4.0           vctrs_0.6.5         \n [7] tools_4.4.0          generics_0.1.3       parallel_4.4.0      \n[10] fansi_1.0.6          pkgconfig_2.0.3      uuid_1.2-0          \n[13] gt_0.10.1            lifecycle_1.0.4      farver_2.1.2        \n[16] compiler_4.4.0       munsell_0.5.1        codetools_0.2-20    \n[19] htmltools_0.5.8.1    sass_0.4.9           yaml_2.3.9          \n[22] furrr_0.3.1          pillar_1.9.0         nloptr_2.1.0        \n[25] crayon_1.5.3         MASS_7.3-60.2        broom.helpers_1.15.0\n[28] broom.mixed_0.2.9.5  boot_1.3-30          parallelly_1.37.1   \n[31] nlme_3.1-164         commonmark_1.9.1     tidyselect_1.2.1    \n[34] digest_0.6.36        future_1.33.2        stringi_1.8.4       \n[37] listenv_0.9.1        pander_0.6.5         labeling_0.4.3      \n[40] splines_4.4.0        labelled_2.13.0      fastmap_1.2.0       \n[43] grid_4.4.0           colorspace_2.1-0     cli_3.6.3           \n[46] magrittr_2.0.3       utf8_1.2.4           broom_1.0.6         \n[49] bigmemory.sri_0.1.8  bigmemory_4.6.4      withr_3.0.0         \n[52] scales_1.3.0         backports_1.5.0      timechange_0.3.0    \n[55] rmarkdown_2.27       globals_0.16.3       hms_1.1.3           \n[58] fastglm_0.0.3        evaluate_0.24.0      haven_2.5.4         \n[61] knitr_1.48           viridisLite_0.4.2    mgcv_1.9-1          \n[64] markdown_1.13        rlang_1.1.4          Rcpp_1.0.13         \n[67] glue_1.7.0           xml2_1.3.6           rstudioapi_0.16.0   \n[70] minqa_1.2.7          jsonlite_1.8.8       R6_2.5.1"
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#for-loop",
    "href": "posts/2024-07-01_parallel/parallel.html#for-loop",
    "title": "Parallel Computing in R",
    "section": "For-loop",
    "text": "For-loop\n\nLet’s begin by using a traditional for-loop. For each bootstrap sample, we:\n\nInitialise a bootstrap sample, ind.\nRun our linear regression on the bootstrap sample, results\nExtract the coefficients from results, and append this to an overall coefficient matrix bootstrap_coefs.\n\nSubsequently, we calculate a 95% confidence interval for each of our parameter estimates by taking the 2.5th and 97.5th percentiles from the bootstrap distribution and calling this bootstrap_cis.\n\n\n\nCode\nstart &lt;- proc.time() # Start our timer!\n\n# Initialise a matrix to store the coefficients from each bootstrap sample\nbootstrap_coefs &lt;- matrix(NA, nrow = trials, ncol = 4)\ncolnames(bootstrap_coefs) &lt;- names(coef(lm(mpg ~ hp + wt + am, data = mtcars)))\n\nfor (i in 1:trials){\n  \n  # Take bootstrap sample\n  ind &lt;- mtcars[sample(nrow(mtcars), \n                       replace = TRUE),\n                ]\n  \n  # Construct linear regression\n  result &lt;- lm(mpg ~ hp + wt + as_factor(am), \n               data = ind)\n  \n  # Extract coefficients and store to `bootstrap_coefs`\n  bootstrap_coefs[i, ] &lt;- coef(result)\n}\n\n# Calculate the 2.5th and 97.5th percentile for each variable's coefficient estimates from the bootstrap distribution.\nbootstrap_cis &lt;- apply(bootstrap_coefs, \n                       2, \n                       function(coefs){quantile(coefs, probs = c(0.025, 0.975))})\n\nend &lt;- proc.time() # End our timer!\ntime1 &lt;- end-start\n\n\n\nLet’s visualise the first few lines of the bootstrapped results\n\n\nCode\nhead(bootstrap_coefs)\n\n\n     (Intercept)          hp        wt         am\n[1,]    34.04596 -0.02741585 -3.158154  0.6330600\n[2,]    31.46670 -0.03097903 -2.375727  5.8142235\n[3,]    35.98084 -0.02535775 -3.763464 -0.2485866\n[4,]    33.47330 -0.04410244 -2.343210  2.6890689\n[5,]    32.21798 -0.04138935 -2.222471  1.2289610\n[6,]    32.78747 -0.02758182 -2.989311  1.1744731\n\n\nand associated 95% confidence interval\n\n\nCode\nbootstrap_cis\n\n\n      (Intercept)          hp        wt         am\n2.5%     29.07843 -0.05539954 -5.134682 -0.7627477\n97.5%    40.74463 -0.02161861 -1.057830  4.9428501\n\n\n\nThis had the following run-time (seconds):\n\n\nCode\nprint(time1)\n\n\n   user  system elapsed \n  4.201   0.014   4.309 \n\n\n\n\n\n\n\n\nproc.time components\n\n\n\n\nuser = time the CPU has spent executing the R process.\nsystem = time the CPU has spent on system-level operations that facilitate the R process (e.g., memory management, system calls).\nelapsed = real-world time that has elapsed."
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#do-loop-not-parallel",
    "href": "posts/2024-07-01_parallel/parallel.html#do-loop-not-parallel",
    "title": "Parallel Computing in R",
    "section": "%do% loop (not parallel)",
    "text": "%do% loop (not parallel)\nAs an alternative, let’s also use the %do% operator from the foreach package. Similar to a for-loop, each bootstrap sample is executed sequentially.\n\n\nCode\nstart &lt;- proc.time()\n\nbootstrap_coefs &lt;- foreach::foreach(i = 1:trials, .combine = rbind) %do% {\n  ind &lt;- mtcars[sample(nrow(mtcars), replace = TRUE), ]\n  result &lt;- lm(mpg ~ hp + wt + am, data = ind)\n  coef(result)\n}\n\n# Calculate the 2.5th and 97.5th percentile for each variable's coefficient estimates from the bootstrap distribution.\nbootstrap_cis &lt;- apply(bootstrap_coefs, \n                       2, \n                       function(coefs) {quantile(coefs, probs = c(0.025, 0.975))})\n\nend &lt;- proc.time()\ntime2 &lt;- end-start\n\n\n\nSimilarly, let’s visualise the first few lines of the bootstrapped results\n\n\nCode\nhead(bootstrap_coefs)\n\n\n         (Intercept)          hp        wt       am\nresult.1    32.28476 -0.02807558 -2.989010 2.404865\nresult.2    38.19523 -0.02740136 -4.576854 1.082726\nresult.3    32.87519 -0.06693216 -1.172027 3.879952\nresult.4    29.54068 -0.03806135 -1.468158 1.933494\nresult.5    31.54392 -0.02793433 -2.756060 1.347816\nresult.6    34.42623 -0.03900395 -2.865725 1.167583\n\n\nand associated 95% confidence interval\n\n\nCode\nbootstrap_cis\n\n\n      (Intercept)          hp        wt         am\n2.5%     29.03455 -0.05615444 -5.286909 -0.8228096\n97.5%    41.09610 -0.02124169 -1.055386  4.9293286\n\n\n\nThis had the following run-time (seconds):\n\n\nCode\nprint(time2)\n\n\n   user  system elapsed \n  3.624   0.009   3.672"
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#dopar-parallelisation",
    "href": "posts/2024-07-01_parallel/parallel.html#dopar-parallelisation",
    "title": "Parallel Computing in R",
    "section": "%dopar% parallelisation",
    "text": "%dopar% parallelisation\nNow, let’s run this in parallel across 6 cores. The %dopar% operator defines the for-loop in the parallel environment.\n\n\nCode\ndoParallel::registerDoParallel(cores = 6) # Initialise parallel cluster\n\nstart &lt;- proc.time()\nbootstrap_coefs &lt;- foreach(i = 1:trials, .combine = rbind, .packages = 'stats') %dopar% {\n  ind &lt;- mtcars[sample(nrow(mtcars), replace = TRUE), ]\n  result &lt;- lm(mpg ~ hp + wt + am, data = ind)\n  coef(result)\n}\n\n# Calculate the 2.5th and 97.5th percentile for each variable's coefficient estimates from the bootstrap distribution.\nbootstrap_cis &lt;- apply(bootstrap_coefs, \n                       2, \n                       function(coefs) {quantile(coefs, probs = c(0.025, 0.975))})\n\nend &lt;- proc.time()\ntime3 &lt;- end-start\n\ndoParallel::stopImplicitCluster() # De-register parallel cluster\n\n\nAs expected, the output of the bootstrapped coefficient distribution are identical before\n\n\nCode\nhead(bootstrap_coefs)\n\n\n         (Intercept)          hp        wt         am\nresult.1    40.28800 -0.02495526 -5.378558  1.0705520\nresult.2    35.76160 -0.03754365 -3.259903  2.3125147\nresult.3    34.22544 -0.04007194 -2.705909  2.4514809\nresult.4    33.92515 -0.03215035 -3.096706  1.1716197\nresult.5    35.84923 -0.01510045 -4.301475 -0.4492253\nresult.6    30.77413 -0.04316729 -1.616423  2.6139924\n\n\nas are the associated 95% confidence intervals.\n\n\nCode\nbootstrap_cis\n\n\n      (Intercept)          hp        wt         am\n2.5%     29.09744 -0.05537917 -5.151840 -0.8456313\n97.5%    40.98867 -0.02161646 -1.085433  4.8985324\n\n\nLastly, this had the following run-time (seconds)\n\n\nCode\nprint(time3)\n\n\n   user  system elapsed \n  4.555   0.273   1.318"
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#discussion",
    "href": "posts/2024-07-01_parallel/parallel.html#discussion",
    "title": "Parallel Computing in R",
    "section": "Discussion",
    "text": "Discussion\nImmediately, the syntax of the alternative for-loop structures are more readable and easier to construct than the traditional for-loop. Because the foreach::foreach function easily combines output in a list, we need not define an empty matrix to append output to.\nComputation time in the parallel environment is significantly faster — approximately 69% faster than the traditional for-loop! Across multiple analyses and data sets, these time savings certainly add up!"
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#data",
    "href": "posts/2024-07-01_parallel/parallel.html#data",
    "title": "Parallel Computing in R",
    "section": "Data",
    "text": "Data\nFirst, let’s simulate our data set and set some parameters:\n\n10,000 observations.\nIndependent variables temperature, precipitation and elevation sampled from a random normal distribution and vegetation type (categorical factor) randomly prescribed.\nSpecies presence (dichotomous) outcome variable is randomly prescribed.\n\n\n\nCode\nn &lt;- 10000 # Sample size\n\ndata &lt;- data.frame(temperature = rnorm(n, \n                                       mean = 15, \n                                       sd   = 40),\n                   precipitation = rnorm(n, \n                                         mean = 1000, \n                                         sd   = 300),\n                   elevation = rnorm(n, \n                                     mean = 500, \n                                     sd   = 200),\n                   vegetation_type = as_factor(sample(c(\"forest\", \n                                                        \"grassland\", \n                                                        \"wetland\", \n                                                        \"desert\"), \n                                                      n, \n                                                      replace = T)),\n                   species_presence = as_factor(sample(c(\"present\", \n                                                         \"absent\"), \n                                                       n, \n                                                       replace = T)))\n\n\nLet’s assign 70% of the data to our training set, and the remaining 30% to test data set and initialise a random forest model with 1000 trees.\n\n\nCode\ntrain_index &lt;- sample(1:n, 0.7*n)\n\ntrain_data &lt;- data[train_index, ]\ntest_data &lt;- data[-train_index, ]\n\nnum_trees &lt;- 1000\n\n\n\nInstead of running one random forest model comprising 1000 trees, let’s combine the results of 4 smaller random forest models models each comprising 250 trees. By doing this, we can return more reliable and robust output (smaller random forest models are less prone to overfitting) and better manage working memory (smaller models require less memory to train and store)."
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#for-loop-1",
    "href": "posts/2024-07-01_parallel/parallel.html#for-loop-1",
    "title": "Parallel Computing in R",
    "section": "For-loop",
    "text": "For-loop\n\n\nCode\nstart &lt;- proc.time()\nrf &lt;- list()\nfor (i in 1:(num_trees/250)){\n\n  rf[[i]] &lt;- randomForest::randomForest(species_presence ~ ., \n                                        data = train_data, \n                                        ntree = num_trees/4)\n}\n\ncombined_output &lt;- do.call(randomForest::combine, rf)\n\npredictions &lt;- predict(combined_output, test_data %&gt;% select(-species_presence))\n\nend &lt;- proc.time()\ntime1 &lt;- end - start\n\n\nLet’s print the confusion matrix of this output. Because this data was relatively simply simulated, we don’t expect the predictive power to be too great.\n\n\nCode\ntable(predictions, test_data$species_presence)\n\n\n           \npredictions absent present\n    absent     707     769\n    present    779     745\n\n\nThis has the following run-time (seconds):\n\n\nCode\nprint(time1)\n\n\n   user  system elapsed \n  5.929   0.159   6.203"
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#do-loop-not-parallel-1",
    "href": "posts/2024-07-01_parallel/parallel.html#do-loop-not-parallel-1",
    "title": "Parallel Computing in R",
    "section": "%do% loop (not parallel)",
    "text": "%do% loop (not parallel)\nSimilar to the traditional for-loop, we can sequentially execute this code using the %do% operator.\n\n\nCode\nstart &lt;- proc.time()\nrf &lt;- foreach::foreach(ntree = rep(num_trees/4, 4), \n                       .combine = randomForest::combine, \n                       .packages = 'randomForest') %do%\n  randomForest::randomForest(species_presence ~ ., \n                             data = train_data,\n                             ntree = ntree)\n\npredictions &lt;- predict(rf, test_data %&gt;% select(-species_presence))\n\nend &lt;- proc.time()\ntime2 &lt;- end - start\n\n\nLet’s print the confusion matrix of this output. Because this data was relatively simply simulated, we don’t expect the predictive power to be too great.\n\n\nCode\ntable(predictions, test_data$species_presence)\n\n\n           \npredictions absent present\n    absent     699     777\n    present    787     737\n\n\nThis has the following run-time (seconds):\n\n\nCode\nprint(time2)\n\n\n   user  system elapsed \n  5.768   0.219   6.057"
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#dopar-parallelisation-1",
    "href": "posts/2024-07-01_parallel/parallel.html#dopar-parallelisation-1",
    "title": "Parallel Computing in R",
    "section": "%dopar% parallelisation",
    "text": "%dopar% parallelisation\nFor simplicity, let’s allocate 4 cores to the computation and imagine that one core is responsible for processing one of the four random forest models simultaneously.\n\n\nCode\ndoParallel::registerDoParallel(cores = 4)\n\nstart &lt;- proc.time()\nrf &lt;- foreach::foreach(ntree = rep(num_trees/4, 4), \n                       .combine = randomForest::combine, \n                       .packages = 'randomForest') %dopar%\n  randomForest::randomForest(species_presence ~ ., \n                             data = train_data,\n                             ntree = ntree)\n\npredictions &lt;- predict(rf, test_data %&gt;% select(-species_presence))\n\nend &lt;- proc.time()\ndoParallel::stopImplicitCluster()\n\ntime3 &lt;- end-start\n\n\nLet’s print the confusion matrix of this output. Because this data was relatively simply simulated, we don’t expect the predictive power to be too great.\n\n\nCode\ntable(predictions, test_data$species_presence)\n\n\n           \npredictions absent present\n    absent     705     775\n    present    781     739\n\n\nThis now has the following run-time (seconds):\n\n\nCode\nprint(time3)\n\n\n   user  system elapsed \n  6.237   0.796   3.253"
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#discussion-1",
    "href": "posts/2024-07-01_parallel/parallel.html#discussion-1",
    "title": "Parallel Computing in R",
    "section": "Discussion",
    "text": "Discussion\nAgain, using easily adaptable and readable syntax, we leverage a parallel environment to significantly lessen the computation time of our large model. Relative to a standard for-loop, the parallelised computation is approximately 48% faster."
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#data-1",
    "href": "posts/2024-07-01_parallel/parallel.html#data-1",
    "title": "Parallel Computing in R",
    "section": "Data",
    "text": "Data\n\nLet’s use nycflights13::flights — a set of over 300,000 flight records that departed from all NYC airports in 2013.\nWe would like to explore how arrival delay (as a continuous and dichotomous (delayed = 1, not delayed = 0) outcome variable) may be influenced by a set of independent variables. We would like to stratify this by month.\n\n\n\nCode\nflights &lt;- nycflights13::flights\nflights &lt;- flights %&gt;%\n  select(year, day, dep_delay, arr_delay, air_time, distance) %&gt;%\n  mutate(arr_delay_bin = as.factor(case_when(arr_delay &gt;  15 ~ 1, TRUE ~ 0)))\n\nflights\n\n\n# A tibble: 336,776 × 7\n    year   day dep_delay arr_delay air_time distance arr_delay_bin\n   &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;        \n 1  2013     1         2        11      227     1400 0            \n 2  2013     1         4        20      227     1416 1            \n 3  2013     1         2        33      160     1089 1            \n 4  2013     1        -1       -18      183     1576 0            \n 5  2013     1        -6       -25      116      762 0            \n 6  2013     1        -4        12      150      719 0            \n 7  2013     1        -5        19      158     1065 1            \n 8  2013     1        -3       -14       53      229 0            \n 9  2013     1        -3        -8      140      944 0            \n10  2013     1        -2         8      138      733 0            \n# ℹ 336,766 more rows\n\n\n\nWe would like to specify a set of models which predict overall flight delay, as both continuous (arrival delay time) and dichotomous (delayed yes/no) outcomes.\n\nOutcome variables\n\nArrival delay (continuous)\nArrival delayed (dichotomous)\n\nIndependent variables\n\nFlight distance (distance)\nAir time (air_time)\nDeparture delay (dep_delay)\n\n\n\n\nCode\nindep_vars &lt;- c(\"distance\", \"air_time\", \"dep_delay\")\noutcome_vars &lt;- c(\"arr_delay\", \"arr_delay_bin\")\n\n\nFor each outcome variable, we run a model. If the outcome variable is continuous, we run a simple linear model; otherwise we run a basic logistic regression."
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#for-loop-2",
    "href": "posts/2024-07-01_parallel/parallel.html#for-loop-2",
    "title": "Parallel Computing in R",
    "section": "For-loop",
    "text": "For-loop\n\n\nCode\nstart &lt;- proc.time()\nmodels &lt;- list() # To store our model output\n\nfor (i in outcome_vars){\n  if (i == \"arr_delay\"){\n    model &lt;- lm(as.formula(paste(i,\"~\",paste(indep_vars, collapse = \"+\"))),\n                data = flights)\n    \n  } else if (i == \"arr_delay_bin\"){\n    model &lt;- glm(as.formula(paste(i,\"~\",paste(indep_vars, collapse = \"+\"))),\n                 family = binomial,\n                 data = flights)\n  }\n  models[[i]] &lt;- summary(model)\n}\n\nend &lt;- proc.time()\ntime1 &lt;- end-start\n\n\nThis returns a list with the model output summary for each of our models.\nThe for-loop has the following run-time (seconds):\n\n\nCode\nprint(time1)\n\n\n   user  system elapsed \n  0.495   0.071   0.657"
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#purrrmap",
    "href": "posts/2024-07-01_parallel/parallel.html#purrrmap",
    "title": "Parallel Computing in R",
    "section": "purrr::map",
    "text": "purrr::map\nMap functions apply a function to each element of a list/vector and return an object. In cases relying on multiple computations across different values, they often come in handy.\n\n\nCode\nstart &lt;- proc.time()\n\n\nmodels &lt;- map(outcome_vars, \n      ~ if (.x == \"arr_delay\"){\n        lm(as.formula(paste(.x, \"~\", \n                            paste(indep_vars, collapse = \" + \"))),\n           data = flights) %&gt;%\n          summary()\n        \n        } else if (.x == \"arr_delay_bin\"){\n          glm(as.formula(paste(.x, \"~\",\n                               paste(indep_vars, collapse = \" + \"))),\n              family = binomial,\n              data = flights) %&gt;%\n            summary()\n        }\n      )\nnames(models) &lt;- outcome_vars\n\nend &lt;- proc.time()\ntime2 &lt;- end-start\n\n\nThis has the following run-time (seconds):\n\n\nCode\nprint(time2)\n\n\n   user  system elapsed \n  0.476   0.055   0.544"
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#furrrfuture_map",
    "href": "posts/2024-07-01_parallel/parallel.html#furrrfuture_map",
    "title": "Parallel Computing in R",
    "section": "furrr::future_map",
    "text": "furrr::future_map\nThere is also a parallel implementation of the purrr::map function, offered by the furrr package. The syntax is (nicely) identical to above, but importantly relies on specifying a parallel (multisession) “plan” ahead of executing the code (similar to what we did in Example 1 and 2).\n\n\nCode\nlibrary(furrr)\n\nplan(multisession, workers = 6) # Initialise parallel environment using furrr\n\nstart &lt;- proc.time()\nmodels &lt;- furrr::future_map(outcome_vars, \n      ~ if (.x == \"arr_delay\"){\n        lm(as.formula(paste(.x, \"~\", \n                            paste(indep_vars, collapse = \" + \"))),\n           data = flights) %&gt;%\n          summary()\n        \n        } else if (.x == \"arr_delay_bin\"){\n          glm(as.formula(paste(.x, \"~\",\n                               paste(indep_vars, collapse = \" + \"))),\n              family = binomial,\n              data = flights) %&gt;%\n            summary()\n        }\n      )\nnames(models) &lt;- outcome_vars\n\nend &lt;- proc.time()\ntime3 &lt;- end-start\n\nplan(sequential) # Revert to sequential processing\n\n\nThis has the following run-time (seconds):\n\n\nCode\nprint(time3)\n\n\n   user  system elapsed \n  0.087   0.012   0.888"
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#dopar-parallelisation-2",
    "href": "posts/2024-07-01_parallel/parallel.html#dopar-parallelisation-2",
    "title": "Parallel Computing in R",
    "section": "%dopar% parallelisation",
    "text": "%dopar% parallelisation\nAlternatively, as done earlier, we can turn our non-parallel %do% code into parallel %dopar% code.\n\nWe use the %:% operator from the foreach package to nest a for-loop within a parallel environment.\nThe syntax does not differ too dramatically.\n\n\n\nCode\ndoParallel::registerDoParallel(cores = 6)\n\nstart &lt;- proc.time()\nmodels &lt;- foreach(j = outcome_vars, .combine = \"list\") %dopar% {\n  if (j == \"arr_delay\"){\n    model &lt;- lm(as.formula(paste(j,\"~\",paste(indep_vars, collapse = \"+\"))),\n                data = flights)\n  } else if (j == \"arr_delay_bin\"){\n    model &lt;- glm(as.formula(paste(j,\"~\",paste(indep_vars, collapse = \"+\"))),\n                 family = binomial,\n                 data = flights)\n  }\n}\nnames(models) &lt;- outcome_vars\n\nend &lt;- proc.time()\n\ntime4 &lt;- end - start\ndoParallel::stopImplicitCluster()\n\n\nThis has the following run-time (seconds):\n\n\nCode\nprint(time4)\n\n\n   user  system elapsed \n  0.124   0.136   0.701"
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#discussion-2",
    "href": "posts/2024-07-01_parallel/parallel.html#discussion-2",
    "title": "Parallel Computing in R",
    "section": "Discussion",
    "text": "Discussion\nWe now see that the parallel processing of these tasks takes far longer – about 6% so! The underlying set of operations — running a series of linear models — are already small and relatively fast, so the overhead of managing the task (splitting, computing and combining results) in a parallel environment far exceeds what what can easily be spun up using a for-loop (or purrr::map)."
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#acknowledgements",
    "href": "posts/2024-07-01_parallel/parallel.html#acknowledgements",
    "title": "Parallel Computing in R",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Wesley Billingham, Matt Cooper, and Elizabeth McKinnon for providing feedback on and reviewing this post.\nYou can look forward to seeing posts from these other team members here in the coming weeks and months."
  },
  {
    "objectID": "posts/2024-07-01_parallel/parallel.html#reproducibility-information",
    "href": "posts/2024-07-01_parallel/parallel.html#reproducibility-information",
    "title": "Parallel Computing in R",
    "section": "Reproducibility Information",
    "text": "Reproducibility Information\nTo access the .qmd (Quarto markdown) files as well as any R scripts or data that was used in this post, please visit our GitHub:\nhttps://github.com/The-Kids-Biostats/The-Kids-Biostats.github.io/tree/main/posts/parallel\nThe session information can also be seen below.\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.0 (2024-04-24)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.5\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Australia/Perth\ntzcode source: internal\n\nattached base packages:\n[1] parallel  stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] furrr_0.3.1          future_1.33.2        randomForest_4.7-1.1\n [4] doParallel_1.0.17    iterators_1.0.14     foreach_1.5.2       \n [7] lubridate_1.9.3      forcats_1.0.0        stringr_1.5.1       \n[10] dplyr_1.1.4          purrr_1.0.2          readr_2.1.5         \n[13] tidyr_1.3.1          tibble_3.2.1         ggplot2_3.5.1       \n[16] tidyverse_2.0.0     \n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.4         generics_0.1.3     stringi_1.8.4      listenv_0.9.1     \n [5] hms_1.1.3          digest_0.6.36      magrittr_2.0.3     evaluate_0.24.0   \n [9] grid_4.4.0         timechange_0.3.0   fastmap_1.2.0      jsonlite_1.8.8    \n[13] fansi_1.0.6        scales_1.3.0       codetools_0.2-20   cli_3.6.3         \n[17] rlang_1.1.4        parallelly_1.37.1  munsell_0.5.1      withr_3.0.0       \n[21] yaml_2.3.9         tools_4.4.0        tzdb_0.4.0         colorspace_2.1-0  \n[25] globals_0.16.3     vctrs_0.6.5        R6_2.5.1           lifecycle_1.0.4   \n[29] htmlwidgets_1.6.4  pkgconfig_2.0.3    pillar_1.9.0       gtable_0.3.5      \n[33] glue_1.7.0         xfun_0.46          tidyselect_1.2.1   rstudioapi_0.16.0 \n[37] knitr_1.48         htmltools_0.5.8.1  rmarkdown_2.27     nycflights13_1.0.2\n[41] compiler_4.4.0"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Kids Biostats",
    "section": "",
    "text": "AI in Biostatistics - Part 2\n\n\n\n\n\n\nAI\n\n\nR\n\n\n\n\n\n\n\n\n\nAug 9, 2024\n\n\nWesley Billingham, Dr Matthew Cooper\n\n\n\n\n\n\n\n\n\n\n\n\nAI in Biostatistics\n\n\n\n\n\n\nAI\n\n\nR\n\n\n\n\n\n\n\n\n\nAug 8, 2024\n\n\nWesley Billingham, Dr Matthew Cooper\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysing pre-post data\n\n\n\n\n\n\nPre-post\n\n\nMixed Models\n\n\nR\n\n\n\n\n\n\n\n\n\nJul 22, 2024\n\n\nDr Matthew Cooper\n\n\n\n\n\n\n\n\n\n\n\n\nData Anonymiser\n\n\n\n\n\n\nShiny\n\n\nR\n\n\n\n\n\n\n\n\n\nJul 19, 2024\n\n\nDr Matthew Cooper, Wesley Billingham\n\n\n\n\n\n\n\n\n\n\n\n\nParallel Computing in R\n\n\n\n\n\n\nR\n\n\nParallel\n\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\nZac Dempsey\n\n\n\n\n\n\n\n\n\n\n\n\nHandling Missing Data with LMER\n\n\n\n\n\n\nMissing Data\n\n\nMixed Models\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 7, 2024\n\n\nDr Matthew Cooper\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "We are members of the biostatistics team at Telethon Kids Institute in Perth, Western Australia. Our work involves providing statistical consultation and collaboration to child health researchers at all stages of the scientific pipeline, from design to analysis to publication.\n\nPrimarily R users, we encounter particular coding, methodology, or analysis challenges - often that have recurring or related themes. The idea of this site is to be a centralised repository of our solutions to or narration of these recurrent challenges, for both our reference and yours!\nWe hope you’ll find something useful here, and are always open to feedback and conversation on these topics."
  },
  {
    "objectID": "posts/2024-06-07_lmer-missing/missing_lmer.html",
    "href": "posts/2024-06-07_lmer-missing/missing_lmer.html",
    "title": "Handling Missing Data with LMER",
    "section": "",
    "text": "As consultant statisticians, we are often approached by people who have already carried out some preliminary data analysis and who are now looking to move onto something more complex. As missing data is generally present (or rather not present!) in health-related datasets, we find this is a question that is regularly raised:\n“Can we use mixed models, since they use all available data?”\nWorking through even the basics on this topic will mean one will also have to work through challenging and varied (often cryptic) statistical nomenclature. This post is a worked example and was motivated by reading this paper. It has an interesting example of ‘missing data and mixed models’ that we thought could benefit from some figures and commentary to aid in the understanding of what is achieved."
  },
  {
    "objectID": "posts/2024-06-07_lmer-missing/missing_lmer.html#research-question",
    "href": "posts/2024-06-07_lmer-missing/missing_lmer.html#research-question",
    "title": "Handling Missing Data with LMER",
    "section": "Research question",
    "text": "Research question\nAre there differences in the rate of wage growth between males and females over time (in this dataset)?\nThat question is quite straightforward to answer here, but the motivating commentary is really around how mixed effects model can be beneficial in the presence of systematic missing (follow-up) data - with a focus on parameter estimates and their graphical interpretation.\nMissing follow-up data (lost to follow, attrition, drop out) is often seen in health research datasets. The data might be Missing At Random, it my be Missing Completely At Random, the important nuances of these are largely out of scope for this post."
  },
  {
    "objectID": "posts/2024-06-07_lmer-missing/missing_lmer.html#erroneous-basic-linear-regression-model",
    "href": "posts/2024-06-07_lmer-missing/missing_lmer.html#erroneous-basic-linear-regression-model",
    "title": "Handling Missing Data with LMER",
    "section": "Erroneous basic linear regression model",
    "text": "Erroneous basic linear regression model\nTo address the question of “are there differences between genders in the rate of wage growth”, we are going to fit a gender by time interaction term which will give us an indication of if ‘as time changes’ whether the outcome (logged wage) changes at a different rate for each gender.\n\n\nCode\nmod1 &lt;- lm(lwage ~ gender * t, data = dat)\nexport_summs(mod1, error_format = \"[{conf.low}, {conf.high}]\",\n             error_pos = \"right\", digits = 3,\n             statistics = c(N = \"nobs\"))\n\n\n\n\nModel 1\n\n(Intercept)6.340 ***[6.312, 6.368]\n\ngenderFemale-0.456 ***[-0.540, -0.372]\n\nt0.097 ***[0.091, 0.104]\n\ngenderFemale:t-0.005    [-0.024, 0.014]\n\nN4165             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nWe can see there is an effect of gender present, and an effect of time (the growth overtime we saw in the original plot), but the very small beta coefficient (relative to the scale of data we are working with) and the p-value of 0.6 are suggestive that the rate of wage growth over time does not differ significantly between genders.\nTo view this, we’re not going to use geom_smooth or stat_summary as we might do when graphing on-the-fly, rather we will use the predict() function to create data for our line of best fit.\nTo set a coding framework which we’ll use again later in the post, we’ll create a new dataset and use predictions to draw our (straight) line.\n\n\nCode\nnewdat &lt;- expand.grid(t = 1:7, gender = c(\"Male\", \"Female\")) %&gt;% \n  mutate(gender = factor(gender),\n         gender = relevel(gender, \"Male\"))\n\np_mod1 &lt;- cbind(newdat, \n                lwage = predict(mod1, newdat, interval = \"prediction\"))\n\n\n\n\nCode\ndat %&gt;% \n  ggplot(aes(t, lwage, colour = gender)) +\n  geom_point(alpha = 0.15) +\n  geom_line(aes(group = id), alpha = 0.15) + \n  facet_wrap(~ gender) +\n  geom_line(data = p_mod1, aes(y = lwage.fit), colour = \"red\", linewidth = 1) +\n  scale_colour_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  theme(legend.position=\"none\") +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\") +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9))\n\n\n\n\n\n\n\n\n\nOkay, these red fitted lines look like quite good as a ‘line of best fit’; they pass the eye test of broadly representing the trends of the data well.\nFitted with a prediction confidence interval, we see.\n\n\nCode\np_mod1 %&gt;% \n  ggplot(aes(t, lwage.fit, ymin = lwage.lwr, ymax = lwage.upr, fill = gender)) +\n  geom_ribbon(alpha = 0.5) +\n  geom_line(colour = \"black\", linewidth = 1) + \n  facet_wrap(~ gender) +\n  scale_fill_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  theme(legend.position=\"none\") +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\")  +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9)) -&gt; p1\np1\n\n\n\n\n\n\n\n\n\nThese lines look parallel - suggestive of no difference in growth rates between the genders (in line with the non-significant interaction term we saw).\nOf course, we have not adjusted for the within person correlation present in the data. The model above is inappropriate as one of the main assumptions of the model is violated - the data points are not all independent (we know there are 7 from each individual)."
  },
  {
    "objectID": "posts/2024-06-07_lmer-missing/missing_lmer.html#mixed-effects-model",
    "href": "posts/2024-06-07_lmer-missing/missing_lmer.html#mixed-effects-model",
    "title": "Handling Missing Data with LMER",
    "section": "Mixed effects model",
    "text": "Mixed effects model\nHere we run a fairly basic linear mixed effects model, the model has the same fixed effects terms as above (the interaction term we are curious about) but also includes a random effect, that is, the intercept is allowed to varied for each individual.\n\n\nCode\nmod2 &lt;- lmer(lwage ~ gender * t + (1 | id), data = dat)\nexport_summs(mod2, error_format = \"[{conf.low}, {conf.high}]\",\n             error_pos = \"right\", digits = 3,\n             statistics = c(N = \"nobs\"))\n\n\n\n\nModel 1\n\n(Intercept)6.340 ***[6.307, 6.373]\n\ngenderFemale-0.456 ***[-0.553, -0.358]\n\nt0.097 ***[0.095, 0.100]\n\ngenderFemale:t-0.005    [-0.012, 0.003]\n\nN4165             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nLets also fit the predicted values from this model.\n\n\nCode\nnewdat &lt;- expand.grid(t = 1:7, gender = c(\"Male\", \"Female\")) %&gt;% \n  mutate(gender = factor(gender),\n         gender = relevel(gender, \"Male\"),\n         id = c(rep(1, 7), rep(2, 7)))\n\nstore &lt;- simulate(mod2, seed=1, newdata=newdat, re.form=NA,\n                  allow.new.levels=T, nsim = 500)\n\np_mod2 &lt;- cbind(newdat,\n                store %&gt;% \n                  rowwise() %&gt;% \n                  mutate(fit = mean(c_across(sim_1:sim_500)),\n                         lwr = quantile(c_across(sim_1:sim_500), 0.025),\n                         upr = quantile(c_across(sim_1:sim_500), 0.975)) %&gt;% \n                  select(fit, lwr, upr))\n\n\n\n– Slight segue\nWith standard linear regression model (first used), we used predict to generate a ‘prediction interval’. With linear mixed effects models, we do no have the same function available (that will incorporate the random effects variability into the prediction interval), so rather than calculating these with a formula, we simulate! Some extra content on this can be read here or (somewhat less so) here.\nThis use of simulation is part of the reason behind the confidence intervals not being parallel."
  },
  {
    "objectID": "posts/2024-06-07_lmer-missing/missing_lmer.html#back-to-our-mixed-effects-model",
    "href": "posts/2024-06-07_lmer-missing/missing_lmer.html#back-to-our-mixed-effects-model",
    "title": "Handling Missing Data with LMER",
    "section": "Back to our Mixed effects model",
    "text": "Back to our Mixed effects model\n\n\nCode\np_mod2 %&gt;% \n  ggplot(aes(t, fit, ymin = lwr, ymax = upr, fill = gender)) +\n  geom_ribbon(alpha = 0.5) +\n  geom_line(colour = \"black\", linewidth = 1) + \n  facet_wrap(~ gender) +\n  scale_fill_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  theme(legend.position=\"none\") +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\") +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9)) -&gt; p2\np2\n\n\n\n\n\n\n\n\n\nWhen we compare the output of this model with the earlier (erroneous) model, we see two expected things.\n\n\nCode\nexport_summs(mod1, mod2,\n             error_format = \"[{conf.low}, {conf.high}]\",\n             error_pos = \"right\", digits = 3,\n             statistics = c(N = \"nobs\"),\n             model.names = c(\"Erroneous model\", \"Mixed efects model\"))\n\n\n\n\nErroneous modelMixed efects model\n\n(Intercept)6.340 ***[6.312, 6.368]6.340 ***[6.307, 6.373]\n\ngenderFemale-0.456 ***[-0.540, -0.372]-0.456 ***[-0.553, -0.358]\n\nt0.097 ***[0.091, 0.104]0.097 ***[0.095, 0.100]\n\ngenderFemale:t-0.005    [-0.024, 0.014]-0.005    [-0.012, 0.003]\n\nN4165             4165             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\n\nThe coefficients have the same value in each model as these represent the fixed effect\nThe confidence intervals for those coefficients are slightly narrower in Model 2, this is because some of the variation present [within Model 1] is explained by the random effects [present in Model 2 and not Model 1].\n\nWe can see this when we plot the predicted values side by side.\n\n\nCode\n(p1 + labs(title = \"Erroneous model\")) / (p2 + labs(title = \"Mixed effects model\"))"
  },
  {
    "objectID": "posts/2024-06-07_lmer-missing/missing_lmer.html#missing---erroneous-basic-linear-regression-model",
    "href": "posts/2024-06-07_lmer-missing/missing_lmer.html#missing---erroneous-basic-linear-regression-model",
    "title": "Handling Missing Data with LMER",
    "section": "Missing - Erroneous basic linear regression model",
    "text": "Missing - Erroneous basic linear regression model\n\n\nCode\nmod3 &lt;- lm(mlwage ~ gender * t, data = mdat)\nexport_summs(mod3, error_format = \"[{conf.low}, {conf.high}]\",\n             error_pos = \"right\", digits = 3,\n             statistics = c(N = \"nobs\"))\n\n\n\n\nModel 1\n\n(Intercept)6.379 ***[6.354, 6.405]\n\ngenderFemale-0.462 ***[-0.533, -0.390]\n\nt0.047 ***[0.040, 0.053]\n\ngenderFemale:t0.032 ***[0.016, 0.049]\n\nN3101             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nNow the model is indicating that there is a strong interaction effect for gender by time. The coefficient of the interaction term implies that (logged) wages increase at a faster rate (over time) for females than they do for males.\nLet’s visual this alongside our modified dataset.\n\n\nCode\nnewdat &lt;- expand.grid(t = 1:7, gender = c(\"Male\", \"Female\"), id = 1) %&gt;% \n  mutate(gender = factor(gender),\n         gender = relevel(gender, \"Male\"))\n\np_mod3 &lt;- cbind(newdat, \n                lwage = predict(mod3, newdat, interval = \"prediction\"))\n\n\n\n\nCode\nmdat %&gt;% \n  ggplot(aes(t, mlwage, colour = gender)) +\n  geom_ribbon(data = p_mod3, aes(t, lwage.fit, ymin = lwage.lwr, ymax = lwage.upr, fill = gender), alpha = 0.5) +\n  geom_line(data = p_mod3, aes(t, lwage.fit), colour = \"black\", linewidth = 1) + \n  geom_point(data = mdat %&gt;% filter(is_zero_following == 1),\n             aes(y = lwage, group = id), alpha = 0.1, colour = \"red\") +\n  geom_line(data = mdat %&gt;% filter(is_zero_following == 1),\n            aes(y = lwage, group = id), alpha = 0.15, colour = \"red\") + \n  geom_point(alpha = 0.15) +\n  geom_line(aes(group = id), alpha = 0.15) + \n  facet_wrap(~ gender) +\n  scale_colour_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  theme(legend.position=\"none\",\n        plot.subtitle=element_text(colour = \"red\")) +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\",\n       subtitle = \"Missing data shown in red\")  +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9)) -&gt; p4\np4\n\n\n\n\n\n\n\n\n\nWe can see the predicted line and bands (95% prediction interval) represent the (non-missing) data well, and we can see the difference in slope between genders - the observed significant interaction term."
  },
  {
    "objectID": "posts/2024-06-07_lmer-missing/missing_lmer.html#missing---mixed-effects-model",
    "href": "posts/2024-06-07_lmer-missing/missing_lmer.html#missing---mixed-effects-model",
    "title": "Handling Missing Data with LMER",
    "section": "Missing - Mixed effects model",
    "text": "Missing - Mixed effects model\n\n\nCode\nmod4 &lt;- lmer(mlwage ~ gender * t + (1 | id), data = mdat)\nexport_summs(mod4, error_format = \"[{conf.low}, {conf.high}]\",\n             error_pos = \"right\", digits = 3,\n             statistics = c(N = \"nobs\"))\n\n\n\n\nModel 1\n\n(Intercept)6.341 ***[6.311, 6.372]\n\ngenderFemale-0.457 ***[-0.546, -0.368]\n\nt0.090 ***[0.087, 0.093]\n\ngenderFemale:t0.003    [-0.004, 0.011]\n\nN3101             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nWhen we run the mixed effects model on the dataset with missing data, we (correctly) do not see a significant interaction effect.\nIn fact:\n\n\nCode\nexport_summs(mod4, mod2,\n             error_format = \"[{conf.low}, {conf.high}]\",\n             error_pos = \"right\", digits = 3,\n             model.names = c(\"ME - Missing\", \"ME - Complete\"),\n             statistics = c(N = \"nobs\"))\n\n\n\n\nME - MissingME - Complete\n\n(Intercept)6.341 ***[6.311, 6.372]6.340 ***[6.307, 6.373]\n\ngenderFemale-0.457 ***[-0.546, -0.368]-0.456 ***[-0.553, -0.358]\n\nt0.090 ***[0.087, 0.093]0.097 ***[0.095, 0.100]\n\ngenderFemale:t0.003    [-0.004, 0.011]-0.005    [-0.012, 0.003]\n\nN3101             4165             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nOur mixed effects model on the dataset with a lot of missing data (ME - Missing) generates quite similar estimates to what we know the ‘truth’ to be from the model on the complete data (ME - Complete).\nTo comparatively visualise this.\n\n\nCode\nnewdat &lt;- expand.grid(t = 1:7, gender = c(\"Male\", \"Female\")) %&gt;% \n  mutate(gender = factor(gender),\n         gender = relevel(gender, \"Male\"),\n         id = c(rep(4, 7), rep(8, 7)))\n\nstore &lt;- simulate(mod4, seed=1, newdata=newdat, re.form=NA,\n                  allow.new.levels=T, nsim = 500)\n\np_mod4 &lt;- cbind(newdat,\n                store %&gt;% \n                  rowwise() %&gt;% \n                  mutate(fit = mean(c_across(sim_1:sim_500)),\n                         lwr = quantile(c_across(sim_1:sim_500), 0.025),\n                         upr = quantile(c_across(sim_1:sim_500), 0.975)) %&gt;% \n                  select(fit, lwr, upr))\n\n\n\n\nCode\nmdat %&gt;% \n  ggplot(aes(t, mlwage, colour = gender)) +\n  geom_ribbon(data = p_mod4, aes(t, fit, ymin = lwr, ymax = upr, fill = gender), alpha = 0.5) +\n  geom_line(data = p_mod4, aes(t, fit), colour = \"black\", linewidth = 1) + \n  geom_point(data = mdat %&gt;% filter(is_zero_following == 1),\n             aes(y = lwage, group = id), alpha = 0.1, colour = \"red\") +\n  geom_line(data = mdat %&gt;% filter(is_zero_following == 1),\n            aes(y = lwage, group = id), alpha = 0.15, colour = \"red\") + \n  geom_point(alpha = 0.15) +\n  geom_line(aes(group = id), alpha = 0.15) + \n  facet_wrap(~ gender) +\n  scale_colour_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  theme(legend.position=\"none\",\n        plot.subtitle=element_text(colour = \"red\")) +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\",\n       subtitle = \"Missing data shown in red\")  +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9)) -&gt; p4\np4\n\n\n\n\n\n\n\n\n\nThese lines (predicted lines; by gender) look parallel (as they should). Notably with the Males, we see the predicted line “pulled up” in the direction of the missing data even though that data was not available to the model - this is because the model has leveraged the slope of the data it did have access to, at the individual (person) level, when converging on its estimates.\nIf the erroneous interaction effect (non-parallel lines) was not obvious in the plot separated by gender, here we see the predicted lines on the same plot for each model.\n\n\nCode\np_mod3 %&gt;% \n  ggplot(aes(t, lwage.fit, ymin = lwage.lwr, ymax = lwage.upr, fill = gender)) +\n  geom_ribbon(alpha = 0.5) +\n  geom_line(colour = \"black\", linewidth = 1) + \n  scale_fill_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\",\n       title = \"Erroneous basic linear regression\",\n       subtitle = \"Missing data present\") +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9)) -&gt; p5\n\np_mod4 %&gt;% \n  ggplot(aes(t, fit, ymin = lwr, ymax = upr, fill = gender)) +\n  geom_ribbon(alpha = 0.5) +\n  geom_line(colour = \"black\", linewidth = 1) + \n  scale_fill_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\",\n       title = \"Mixed effects regression\",\n       subtitle = \"Missing data present\")  +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9)) -&gt; p6\n\np5 + p6 + plot_layout(guides = \"collect\") & theme(legend.position='bottom')"
  },
  {
    "objectID": "posts/2024-06-07_lmer-missing/missing_lmer.html#less-aggressive-missingness",
    "href": "posts/2024-06-07_lmer-missing/missing_lmer.html#less-aggressive-missingness",
    "title": "Handling Missing Data with LMER",
    "section": "Less aggressive missingness",
    "text": "Less aggressive missingness\nWhat if we whip through the same process and comparison, in a setting that ‘less aggressively’ has drop out with increasing wage and also some additional random missingness throughout.\n\n\nCode\nmdat &lt;- dat %&gt;% \n  group_by(id) %&gt;% \n  mutate(plwage = c(0, lwage[1:6]),\n         pt = 1 / (1+exp(-7.2 + plwage))) %&gt;% # Less aggressive dropout as a function of age\n  rowwise() %&gt;% \n  mutate(pt = case_when(pt &gt; 0.5 & runif(1) &lt; 0.10 & t &gt; 2 ~ 0, # Adding an underlying random component to dropout\n                        T ~ pt)) %&gt;% \n  group_by(id) %&gt;% \n  mutate(mlwage = case_when(pt &gt; 0.5 ~ lwage,\n                            pt &lt; 0.5 ~ 0),\n         is_zero_following = ifelse(mlwage == 0, 1, 0),\n         is_zero_following = cummax(is_zero_following),\n         mlwage = ifelse(is_zero_following == 1, NA, mlwage)) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nMale, N = 2,8041\nFemale, N = 3521\n\n\n\n\nt\n\n\n\n\n\n\n    1\n528\n67\n\n\n    2\n528\n67\n\n\n    3\n478\n57\n\n\n    4\n396\n49\n\n\n    5\n338\n43\n\n\n    6\n290\n38\n\n\n    7\n246\n31\n\n\n\n1 n\n\n\n\n\n\n\n\n\n\n\nCode\nmdat %&gt;% \n  ggplot(aes(t, mlwage, colour = gender)) +\n  geom_point(data = mdat %&gt;% filter(is_zero_following == 1),\n             aes(y = lwage, group = id), alpha = 0.1, colour = \"red\") +\n  geom_line(data = mdat %&gt;% filter(is_zero_following == 1),\n            aes(y = lwage, group = id), alpha = 0.15, colour = \"red\") + \n  geom_point(alpha = 0.15) +\n  geom_line(aes(group = id), alpha = 0.15) + \n  facet_wrap(~ gender) +\n  scale_colour_viridis_d(option = \"viridis\", end = 0.4) +\n  theme_clean() +\n  theme(legend.position=\"none\") +\n  labs(x = \"Time (years)\", y = \"Wage (logged)\")  +\n  coord_cartesian(xlim = c(0,7),\n                  ylim = c(4.5,9))\n\n\n\n\n\n\n\n\n\n\n\nCode\nb1_mod1 &lt;- lm(mlwage ~ gender * t, data = mdat)\nb1_mod2 &lt;- lmer(mlwage ~ gender * t + (1 | id), data = mdat)\n\nexport_summs(mod2, b1_mod1, b1_mod2,\n             error_format = \"[{conf.low}, {conf.high}]\",\n             error_pos = \"right\", digits = 3,\n             model.names = c(\"ME - Complete\", \"Basic - Missing\", \"ME - Missing\"),\n             statistics = c(N = \"nobs\"))\n\n\n\n\nME - CompleteBasic - MissingME - Missing\n\n(Intercept)6.340 ***[6.307, 6.373]6.380 ***[6.352, 6.407]6.342 ***[6.311, 6.373]\n\ngenderFemale-0.456 ***[-0.553, -0.358]-0.491 ***[-0.573, -0.410]-0.465 ***[-0.557, -0.372]\n\nt0.097 ***[0.095, 0.100]0.072 ***[0.065, 0.079]0.094 ***[0.091, 0.097]\n\ngenderFemale:t-0.005    [-0.012, 0.003]0.024 *  [0.003, 0.044]0.003    [-0.006, 0.012]\n\nN4165             3156             3156             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nWe can see, the (erroneous) basic linear regression still inappropriately suggests a significant interaction effect is present, while the mixed effects models continues to perform well (relative to the model using the complete data)."
  },
  {
    "objectID": "posts/2024-06-07_lmer-missing/missing_lmer.html#completely-random-missingness",
    "href": "posts/2024-06-07_lmer-missing/missing_lmer.html#completely-random-missingness",
    "title": "Handling Missing Data with LMER",
    "section": "Completely random missingness",
    "text": "Completely random missingness\nWhat if the dropout is completely at random?\nNote, this is dropout at random, not sporadic missingness, these are two different things.\n\n\nCode\nmdat &lt;- dat %&gt;% \n  group_by(id) %&gt;% \n  rowwise() %&gt;% \n  mutate(pt = 1,\n         pt = case_when(runif(1) &lt; 0.10 & t &gt; 2 ~ 0, # Implementing completely random dropout\n                        T ~ pt)) %&gt;% \n  group_by(id) %&gt;% \n  mutate(mlwage = case_when(pt &gt; 0.5 ~ lwage,\n                            pt &lt; 0.5 ~ 0),\n         is_zero_following = ifelse(mlwage == 0, 1, 0),\n         is_zero_following = cummax(is_zero_following),\n         mlwage = ifelse(is_zero_following == 1, NA, mlwage)) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nME - CompleteBasic - MissingME - Missing\n\n(Intercept)6.340 ***[6.307, 6.373]6.330 ***[6.300, 6.359]6.333 ***[6.301, 6.366]\n\ngenderFemale-0.456 ***[-0.553, -0.358]-0.456 ***[-0.544, -0.368]-0.456 ***[-0.553, -0.359]\n\nt0.097 ***[0.095, 0.100]0.102 ***[0.095, 0.109]0.099 ***[0.096, 0.102]\n\ngenderFemale:t-0.005    [-0.012, 0.003]-0.007    [-0.029, 0.014]-0.003    [-0.012, 0.005]\n\nN4165             3379             3379             \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nThe basic regression model is no long suggesting there is a significant gender by time interaction effect, and comparatively, all three models give similar estimates."
  },
  {
    "objectID": "posts/2024-06-07_lmer-missing/missing_lmer.html#acknowledgements",
    "href": "posts/2024-06-07_lmer-missing/missing_lmer.html#acknowledgements",
    "title": "Handling Missing Data with LMER",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Elizabeth McKinnon, Zac Dempsey, and Wesley Billingham for providing feedback on and reviewing this post.\nYou can look forward to seeing posts from these other team members here in the coming weeks and months."
  },
  {
    "objectID": "posts/2024-06-07_lmer-missing/missing_lmer.html#reproducibility-information",
    "href": "posts/2024-06-07_lmer-missing/missing_lmer.html#reproducibility-information",
    "title": "Handling Missing Data with LMER",
    "section": "Reproducibility Information",
    "text": "Reproducibility Information\nTo access the .qmd (Quarto markdown) files as well as any R scripts or data that was used in this post, please visit our GitHub:\nhttps://github.com/The-Kids-Biostats/The-Kids-Biostats.github.io/tree/main/posts/lmer-missingx\nThe session information can also be seen below.\n\n\nR version 4.4.0 (2024-04-24)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.5\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Australia/Perth\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n [5] purrr_1.0.2       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n [9] ggplot2_3.5.1     tidyverse_2.0.0   patchwork_1.2.0   parameters_0.22.0\n[13] jtools_2.2.2      gtsummary_1.7.2   AER_1.2-12        survival_3.5-8   \n[17] sandwich_3.1-0    lmtest_0.9-40     zoo_1.8-12        car_3.1-2        \n[21] carData_3.0-5     merTools_0.6.2    arm_1.14-4        MASS_7.3-60.2    \n[25] lme4_1.1-35.4     Matrix_1.7-0     \n\nloaded via a namespace (and not attached):\n [1] rlang_1.1.4          magrittr_2.0.3       furrr_0.3.1         \n [4] compiler_4.4.0       vctrs_0.6.5          pkgconfig_2.0.3     \n [7] crayon_1.5.3         fastmap_1.2.0        backports_1.5.0     \n[10] labeling_0.4.3       pander_0.6.5         utf8_1.2.4          \n[13] promises_1.3.0       rmarkdown_2.27       markdown_1.13       \n[16] tzdb_0.4.0           nloptr_2.1.0         xfun_0.46           \n[19] jsonlite_1.8.8       later_1.3.2          broom_1.0.6         \n[22] parallel_4.4.0       R6_2.5.1             stringi_1.8.4       \n[25] parallelly_1.37.1    boot_1.3-30          estimability_1.5.1  \n[28] assertthat_0.2.1     Rcpp_1.0.13          iterators_1.0.14    \n[31] knitr_1.48           httpuv_1.6.15        splines_4.4.0       \n[34] timechange_0.3.0     tidyselect_1.2.1     rstudioapi_0.16.0   \n[37] abind_1.4-5          yaml_2.3.9           codetools_0.2-20    \n[40] listenv_0.9.1        lattice_0.22-6       shiny_1.8.1.1       \n[43] withr_3.0.0          bayestestR_0.13.2    coda_0.19-4.1       \n[46] evaluate_0.24.0      future_1.33.2        huxtable_5.5.6      \n[49] xml2_1.3.6           pillar_1.9.0         foreach_1.5.2       \n[52] insight_0.20.1       generics_0.1.3       hms_1.1.3           \n[55] munsell_0.5.1        commonmark_1.9.1     scales_1.3.0        \n[58] minqa_1.2.7          globals_0.16.3       xtable_1.8-4        \n[61] glue_1.7.0           emmeans_1.10.2       tools_4.4.0         \n[64] mvtnorm_1.2-5        grid_4.4.0           datawizard_0.11.0   \n[67] colorspace_2.1-0     nlme_3.1-164         Formula_1.2-5       \n[70] cli_3.6.3            fansi_1.0.6          broom.helpers_1.15.0\n[73] viridisLite_0.4.2    gt_0.10.1            gtable_0.3.5        \n[76] broom.mixed_0.2.9.5  sass_0.4.9           digest_0.6.36       \n[79] pbkrtest_0.5.3       htmlwidgets_1.6.4    farver_2.1.2        \n[82] htmltools_0.5.8.1    lifecycle_1.0.4      mime_0.12           \n[85] blme_1.0-5"
  },
  {
    "objectID": "posts/2024-07-19_data_anonymiser/data_anonymiser.html",
    "href": "posts/2024-07-19_data_anonymiser/data_anonymiser.html",
    "title": "Data Anonymiser",
    "section": "",
    "text": "Data access and security is a really important topic and one that can be challenging to navigate. Research ethics and governance approvals typically require detail about where and how data will be stored and analysed - and a researcher typically does not have approval for the data they are working with to be moved to other locations.\nThe rapid emergence of AI tools and online data analytics platforms (e.g. dashboards) may tempt researchers with the promise of short cuts to writing code and/or producing attractive figures with ease. For example, sites like this that will take an uploaded dataset and draw a figure from it following a user prompt – but where did your precious private research data just go?\nBelow, we share a data anonymising (obscuring) shiny app that you can run on your local machine. It will return to you, a “similar” dataset to your original dataset, allowing you to then safely use an online chat bot to assisted you to code up that complex figure or model."
  },
  {
    "objectID": "posts/2024-07-19_data_anonymiser/data_anonymiser.html#the-process",
    "href": "posts/2024-07-19_data_anonymiser/data_anonymiser.html#the-process",
    "title": "Data Anonymiser",
    "section": "The process",
    "text": "The process\nThe app simply does the following:\n\ntakes an uploaded dataset (.csv or .xlsx),\nlets you choose which variables to keep and how many rows you want returned,\nreturns to you a fully obscured dataset (.csv) for download (see example above).\n\nContained on a tab within the app is a brief tabular report that compares your original data to the anonymised data.\n\nGetting started\nTo use the app, you can either download a .zip (or clone the github repository) at the following link:\nhttps://github.com/TelethonKids/data_anonymiser\nThen:\n\nUnzip the folder\nIn your R session, ensure you are in directory of the unzipped files\nIn your R console, type runApp()\n\nYou may need to install.packages() a few packages, though this app only leverages very popular packages that most R users are likely to already have."
  },
  {
    "objectID": "posts/2024-07-19_data_anonymiser/data_anonymiser.html#data-obscuring-process",
    "href": "posts/2024-07-19_data_anonymiser/data_anonymiser.html#data-obscuring-process",
    "title": "Data Anonymiser",
    "section": "Data obscuring process",
    "text": "Data obscuring process\nThis is also explained on the app’s landing page – but the detail of what is happening in the data obscuring process is as follows:\n\nColumn names: These will all be replaced with a generic name of the form “VAR_1”, “VAR_2”, etc.\nContinuous variables\n\n15 or fewer unique values: These will be randomly (evenly) replaced with integers 0 through to the n (where n is the number of unique values within that variable).\nMore than 15 unique values: Will be replaced with random normally distributed data that has the same mean and standard deviation as the original data.\n\nDate variables: The min and max date will be found within the provided data, then uniformly distributed dates between that min and max date will be returned.\nCategorical variables (includes dichotomous variables)\n\n15 or fewer unique categories: These will be randomly (evenly) replaced with letters “A” through “O”\nMore than 15 unique categories: (E.g., studyIDs, names, addresses etc) these will be replaced with a random string of length 6 (may or may not be unique)."
  },
  {
    "objectID": "posts/2024-07-19_data_anonymiser/data_anonymiser.html#acknowledgements",
    "href": "posts/2024-07-19_data_anonymiser/data_anonymiser.html#acknowledgements",
    "title": "Data Anonymiser",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Wesley Billingham and Zac Dempsey for providing feedback on and reviewing this post.\nYou can look forward to seeing posts from these other team members here in the coming weeks and months."
  },
  {
    "objectID": "posts/2024-07-19_data_anonymiser/data_anonymiser.html#reproducibility-information",
    "href": "posts/2024-07-19_data_anonymiser/data_anonymiser.html#reproducibility-information",
    "title": "Data Anonymiser",
    "section": "Reproducibility Information",
    "text": "Reproducibility Information\nSession information:\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.0 (2024-04-24)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.5\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Australia/Perth\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.4.0    fastmap_1.2.0     cli_3.6.3        \n [5] tools_4.4.0       htmltools_0.5.8.1 rstudioapi_0.16.0 yaml_2.3.9       \n [9] rmarkdown_2.27    knitr_1.48        jsonlite_1.8.8    xfun_0.46        \n[13] digest_0.6.36     rlang_1.1.4       evaluate_0.24.0"
  },
  {
    "objectID": "posts/2024_08_08_ai_part_one/ai_part_one.html",
    "href": "posts/2024_08_08_ai_part_one/ai_part_one.html",
    "title": "AI in Biostatistics",
    "section": "",
    "text": "AI is a hot topic in most fields right now, and biostatistics is no exception! We (the Telethon Kids Institute biostats team) were asked to present at the weekly Institute seminar on the use of AI in statistical workflows.\nThe term AI has many meanings depending on the context - in this article we are referring exclusively to (and use the term interchangeably with) large language models (LLMs) such as ChatGPT and Claude. These tools are useful not just for writing sentences and paragraphs of text, but also for writing functioning code!\nSince first investigating the capabilities of ChatGPT in writing R code, our team has been working to utilise it safely and effectively in our everyday workflows.\nThe summary of our message to the Institute staff was that we cannot ignore the massive increase in efficiency that AI can bring if used properly. However, we also need to be very aware of its limitations, and the necessary role of human experts in the process of validating any non-trivial output.\nThis is Part One in a two-part series. This part will look at the capabilities of AI to benefit our workflows and increase efficiency. The next part will look at some limitations of the technology as it currently exists, including some practical recommendations to identify these issues.\n\n\nWe will work through a couple of practical examples of how we might use AI in an everyday workflow:\n\nData Visualisation\nStatistical Analysis\nBonus - Shiny App Development\n\nIn these examples, we will increasingly take our hands off the wheel and allow AI (in these examples, ChatGPT 4.0) to perform more and more of the task."
  },
  {
    "objectID": "posts/2024_08_08_ai_part_one/ai_part_one.html#format-of-this-post",
    "href": "posts/2024_08_08_ai_part_one/ai_part_one.html#format-of-this-post",
    "title": "AI in Biostatistics",
    "section": "",
    "text": "We will work through a couple of practical examples of how we might use AI in an everyday workflow:\n\nData Visualisation\nStatistical Analysis\nBonus - Shiny App Development\n\nIn these examples, we will increasingly take our hands off the wheel and allow AI (in these examples, ChatGPT 4.0) to perform more and more of the task."
  },
  {
    "objectID": "posts/2024_08_08_ai_part_one/ai_part_one.html#providing-the-blueprint",
    "href": "posts/2024_08_08_ai_part_one/ai_part_one.html#providing-the-blueprint",
    "title": "AI in Biostatistics",
    "section": "Providing the Blueprint",
    "text": "Providing the Blueprint\nChatGPT allows the user to provide images as a part of the prompt. With that in mind, the image was uploaded first by itself. ChatGPT was able to successfully identify the key elements of the plot just from the image (!), and so the next step was to give some guidance to make the plot more visually appealing.\nWith just one prompt: “can you facet wrap the min max data”, in addition to the original plot, it got to work. Once the original data was provided, it returned the requested modifcation - complete with Python code for the plot (R cannot be run within ChatGPT, though we certainly could have requested R code to run ourselves instead).\nHere is the entire prompt history, from beginning to end:\n\nOne image\nOne command\nSome (undescribed) data\n\nwas all the context required to understand and return, approximately, the desired output. We say approximately, since some key information such as the counts above certain levels, is absent from this first attempt."
  },
  {
    "objectID": "posts/2024_08_08_ai_part_one/ai_part_one.html#end-result-and-discussion",
    "href": "posts/2024_08_08_ai_part_one/ai_part_one.html#end-result-and-discussion",
    "title": "AI in Biostatistics",
    "section": "End Result and Discussion",
    "text": "End Result and Discussion\nAfter some further refinement and back-and-forth, we arrive at a much-improved plot which more closely captures the spirit of the original, while improving the colour scheme and facetting the data.\n\n\n\n\n\n\n\n\n\nPlots are a great use-case of AI, since the nitty gritty of colours, panes, text, etc can be a time-consuming hassle to navigate manually. AI in our experience does a good job of translating our descriptive visual prompts into code that achieves the described vision. Furthermore, the output is instantly verifiable (keep this point in mind!) and easy to refine."
  },
  {
    "objectID": "posts/2024_08_08_ai_part_one/ai_part_one.html#providing-the-problem",
    "href": "posts/2024_08_08_ai_part_one/ai_part_one.html#providing-the-problem",
    "title": "AI in Biostatistics",
    "section": "Providing the Problem",
    "text": "Providing the Problem\nIn this example, we provided ChatGPT with a very high-level summary of our data and research question, as seen here:\n\n\n\n\n\n\n\n\n\nAgain, we have tried to use prompts that anyone with some familiarity with stats and data analysis would be able to replicate and understand. Here, we simply mention to ChatGPT that there are patients, doctors and hospitals (highlighted above)."
  },
  {
    "objectID": "posts/2024_08_08_ai_part_one/ai_part_one.html#the-response",
    "href": "posts/2024_08_08_ai_part_one/ai_part_one.html#the-response",
    "title": "AI in Biostatistics",
    "section": "The Response",
    "text": "The Response\nAmazingly, especially if you have had limited exposure to AI, these high-level, lay descriptions of our data and research questions were sufficient for ChatGPT to suggest a mixed-effects logistic regression model. It recognised the binary outcome and the hierarchical structure of the data, and proceeded to provide R code to perform the analysis.\n\n\n\n\n\n\n\n\n\nHere is where we encounter a difference from our previous visualisation example, however. For someone unfamiliar with statistics, would they have known if this answer happened to be incorrect? Broadly, this response is on track, but should Doctor be nested within Hospital? (keep this in mind! #2).\nFor now, we do recognise that it has selected an appropriate model to get started. An impressive feat!"
  },
  {
    "objectID": "posts/2024_08_08_ai_part_one/ai_part_one.html#end-result-and-discussion-1",
    "href": "posts/2024_08_08_ai_part_one/ai_part_one.html#end-result-and-discussion-1",
    "title": "AI in Biostatistics",
    "section": "End Result and Discussion",
    "text": "End Result and Discussion\nFor brevity’s sake, we will not include the whole prompting process here. However, following the model specification, ChatGPT was able to successfully guide us through adding a random slope, and then summarising our model using the broom.mixed package (broom for mixed models, as the name suggests).\nIt then gave us code to check the model fit, run model diagnostics, and perform sensitivity analysis. At each stage, it was able to give advice on interpretation and next steps that we could not fault. Here are some highlights:"
  },
  {
    "objectID": "posts/2024_08_08_ai_part_one/ai_part_one.html#why",
    "href": "posts/2024_08_08_ai_part_one/ai_part_one.html#why",
    "title": "AI in Biostatistics",
    "section": "Why?",
    "text": "Why?\nWell yes, you could just use random data between 0 and 1 - but we (and we presume a lot of people) find less mental load involved when the data being worked with resembles the original data (as in, the values are between 140 and 200 for our “blood pressure” histogram, or between 70 and 130 for the “IQ scores” we’re plotting against values between 1.4m-2.1m for “height” etc)."
  },
  {
    "objectID": "posts/2024_08_08_ai_part_one/ai_part_one.html#how-do-i-access-it",
    "href": "posts/2024_08_08_ai_part_one/ai_part_one.html#how-do-i-access-it",
    "title": "AI in Biostatistics",
    "section": "How do I access it?",
    "text": "How do I access it?\nIt would be ironic if we provided this for you on our website, because then you would be sending your data to our servers. At our Institute, we run this on an internal server that is not accessible from outside our internal network.\nYou can download a copy from our github (it is a Shiny app that runs within R), and then run it locally on the desktop (or virtual) machine you are working on - anonymising your data without it leaving your analysis environment!"
  },
  {
    "objectID": "posts/2024_08_08_ai_part_one/ai_part_one.html#anything-else",
    "href": "posts/2024_08_08_ai_part_one/ai_part_one.html#anything-else",
    "title": "AI in Biostatistics",
    "section": "Anything else?",
    "text": "Anything else?\nOutside of adding the instructional blurb and some minor aesthetic tweaks, this entire app was written by ChatGPT. ChatGPT made the code available for download (as a zip file) and worked through a series of prompts (after the initial prompt) to make small tweaks to get the app doing exactly what we wanted.\nWe then thoroughly inspected the code (knowing roughly how it should have been written/structured) and tested it with a range of test datasets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere is a test dataset (.xlsx) available for download here (click downward arrow on right hand size of the page) alongside the app.\nHow was that created? You guessed it!"
  }
]